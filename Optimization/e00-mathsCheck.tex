\input{../latex/shared}

\renewcommand{\course}{Optimization Algorithms}
\renewcommand{\coursepicture}{optim}
\renewcommand{\coursedate}{Winter 2024/25}
\renewcommand{\exnum}{Weekly Exercise 0}

\exercises

\exercisestitle

\excludecomment{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

You do not have to prepare for the first tutorial. You should be able
to solve these exercises directly. Please volunteer to solve these
exercises (or help each other to solve them jointly) at the board.

%% \exsection{Reading}

%% Please read up to page 20 of the ``Maths for Intelligent Systems'' script (linked on ISIS). Bring any questions you have to the tutorial. The exercises below are mostly extracts from that script.

\exsection{Matrix equations}

a) Let $X,A$ be arbitrary matrices, $A$ invertible. Solve for $X$:
\begin{equation}
 X A + A^\T = \Id 
\end{equation}

b) Let $X,A,B$ be arbitrary matrices, $(C-2A^\T)$ invertible. Solve for $X$:
\begin{equation}
 X^\T C = [2 A (X + B)]^\T 
\end{equation}

c) Let $x\in\RRR^n,y\in\RRR^d,A\in\RRR^{d\times n}$. $A$ obviously \emph{not}
invertible, but let $A^\T A$ be invertible. Solve for $x$:
\begin{equation}
 (A x - y)^\T A = \vec 0_n^\T 
\end{equation}

d) As above, additionally $B\in\RRR^{n\times n}$, $B$
positive-definite. Solve for $x$: 
\begin{equation}
 (A x - y)^\T A + x^\T B = \vec 0_n^\T 
\end{equation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exsection{Multivariate Calculus}

%% Given tensors $y\in\RRR^{a\times\ldots\times z}$ and
%% $x\in\RRR^{\alpha\times\ldots\times\omega}$ where $y$ is a function of $x$, the
%% Jacobian tensor $J = \del_x y$ is in $\RRR^{a\times \ldots\times z\times\alpha\times\ldots\times\omega}$ and has coefficients
%% $$
%%   J_{i,j,k,\ldots,l,m,n\ldots} = \frac{\partial}{\partial x_{l,m,n,\ldots}} y_{i,j,k,\ldots}
%% $$

%% (All ``output'' indices come before all ``input'' indices``.)

Compute the following Jacobian tensors
\begin{enumerate}
  \item $\Del x x$, where $x$ is a vector
  \item $\Del x x^\T A x$, where $A$ is a matrix
  %% \item $\Del A y^\T A x$, where $x$ and $y$ are vectors ~ (note, we take the derivative w.r.t.\ $A$)
  %% \item $\Del A A x$
  \item $\Del x f(x)^\T A g(x)$, where $f$ and $g$ are vector-valued functions
  \item $\Del x \frac{Ax}{||Ax||} $, where $A$ is a matrix and $||\cdot||$ is the 2-norm
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\exsection{Minimization}

You can minimize a function $f(\b)$ by computing its gradient $\na_\b f(\b)$ and solving $\na_\b f(\b) = 0$ w.r.t. $\b$.

Given a fixed $y \in \RRR^m,~X \in \RRR^{m \times n}$ and $\lambda \in \RRR^+$, find the value of  $\beta \in \RRR^n$ that minimizes:
 \begin{equation}
 ||y - X \beta ||^2 + \lambda || \beta ||^2
 \end{equation}

 Did you find a local or global optimum? Justify your answer.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\exsection{Projections}

\begin{enumerate}

  \item In $\RRR^n$, a plane (through the origin) is described by
the linear equation
\begin{align}
  c^\T x = 0 ~,
\end{align}
where $c\in\RRR^n$ parameterizes the plane. Provide the matrix that describes the orthogonal projection of a point $x_0 \in \RRR^n$ onto this plane. (Hint: the solution has the form of 'Identity matrix minus a rank-1 matrix').


\item In $\RRR^n$, we have $k$ linearly independent vectors $\{v_i\}_{i=1}^k$, which form the matrix $V = (v_1,..,v_k) \in\RRR^{n\times k}$. 
We want to derive the projection of $x_0 \in \RRR^n$ into the subspace generated by the columns of $V$ as an optimization problem.

Any point in the subspace can be expressed as $V \alpha, ~\alpha \in \RRR^k$. Therefore, the projection of $x_0$ into $V$ is $V \alpha^*$, where
\begin{equation}
  \a^*(x_0) = \argmin_{\a\in\RRR^k} \norm{x_0 - V \a}^2 ~.
\end{equation}
Derive the expression for the optimal $\a^*(x_0)$ from the optimality principles.

% (For information only: Note that $V \a = \sum_{i=1}^k \a_i v_i$ is just the linear combination of $v_i$'s with coefficients $\a$. The projection of a vector $x$ is then $x_{|\!|} = V \a^*(x)$.)


\end{enumerate}


\exerfoot
