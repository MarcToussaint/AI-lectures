\input{../latex/shared}

\renewcommand{\course}{Maths for Intelligent Systems}

\script

\renewcommand{\<}{\langle}
\renewcommand{\>}{\rangle}
\DeclareMathOperator{\vol}{vol}
\newcommand{\ul}{\underline}
\newcommand{\bd}{\boldsymbol}
\newcommand{\ve}[2]{\left[\arr{c}{#1\\#2}\right]}
\newcommand{\ma}[4]{\left[\arr{cc}{#1&#2\\#3&#4}\right]}
\renewcommand{\de}[4]{\left|\arr{cc}{#1&#2\\#3&#4}\right|}
\newcommand{\eig}{\text{eig}}
\renewcommand{\skew}{\text{skew}}

\excludecomment{solution}

%%%%%%%%%%
\newlength\rightmargintoc
\setlength\rightmargintoc{\linewidth}
\addtolength\rightmargintoc{-7em}

\makeatletter
\def\subsubsectocline#1#2#3#4#5{%
\parshape 2 4em \rightmargintoc \dimexpr\parindent+4em\relax \rightmargintoc
\@tempdima#3
\ifdim\lastskip=1sp;\relax\ \else\fi{\footnotesize#4}\hskip1sp%
}
\renewcommand*\l@subsubsection{\subsubsectocline{1}{0em}{2.5em}}
\makeatother

\pretocmd{\chapter}{\addtocontents{toc}{\par}}{}{}
\pretocmd{\section}{\addtocontents{toc}{\par}}{}{}

\AtEndDocument{%
\ifnum\value{subsubsection}>0\relax
  \addtocontents{toc}{\par}
  \fi}
%%%%%%%%%%

  \DefineShortVerb{\@}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Maths for Intelligent Systems}
%\myauthor{Marc Toussaint}
\date{April, 2022}


\begin{document}
\maketitle


This script is primarily based on a lecture I gave 2015-2019 in Stuttgart.
The current form also integrates notes and exercises from the Optimization lecture, and a little material from my Robotics and ML lectures. The first full version was from 2019, since then I occasionally update it.


{\small\tableofcontents}

\section{Speaking Maths}

\subsection{Describing systems}

Systems can be described in many ways. Biologists describe their
systems often using text, and lots and lots of data. Architects
describe buildings using drawings. Physisics describe nature using
differential equations, or optimality principles, or differential
geometry and group theory. The whole point of science is to find
descriptions of systems---in the natural science descriptions that
allow prediction, in the engineering sciences descriptions
that enable the design of good systems, problem-solving systems.

And how should we describe intelligent systems? Robots, perception
systems, machine learning systems? I think there are two main categories:
the imperative way in terms of literal algorithms (code), or the
declarative way in terms of formulating the problem. I prefer the latter.

The point of this lecture is to teach you to \emph{speak maths}, to
use maths to describe systems or problems. I feel that most maths
courses rather teach to consume maths, or solve mathematical problems,
or prove things. Clearly, this is also important. But for the purpose
of intelligent systems research, it is essential to be skilled
in \emph{expressing} problems mathematically, before even thinking
about solving them and deriving algorithms.

If you happen to attend a Machine Learning or Robotics course you'll
see that \emph{every} problem is addressed the same way: You have an
``intuitively formulated'' problem; the first step is to find a
mathematical formulation; the second step to solve it. The second step
is often technical. The first step is really the interesting and
creative part. This is where you have to nail down the problem, i.e.,
nail down what it means to be successful or performant -- and
thereby describe ``intelligence'', or at least a tiny aspect of it.

The ``Maths for Intelligent Systems'' course will recap essentials of
multi-variate functions, linear algebra, optimization, and probabilities. These
fields are essential to formulate problems in intelligent
systems research and hopefully will equip you with the basics of
speaking maths.

\subsection{Should maths be tought with many application examples? Or
abstractly?}

Maybe this is the wrong question and implies a view on maths I don't
agree with. I think (but this is arguable) maths is nothing but
abstractions of real-world things. At least I aim to teach maths as
abstractions of real-world things. It is misleading to think that
there is ``pure maths'' and then ``applications''. Instead
mathematical concepts, such as a vector, are abstractions of
real-world things, such as faces, scenes, images, documents; and
theorems, methods and algorithms that apply on vectors of course also
apply to all the real-world things---subject to the limitations of
this abstraction. So, the goal is not to teach you a lookup table of
which method can be used in which application, but rather to teach
which concepts maths offers to abstract real-world things---so that
you find such abstractions yourself once you'll have to solve a
real-world problem.

But yes, I believe that maths -- in our context -- should ideally be taught with many exercises relating to AI problems. Perhaps the ideal would be:
\begin{itemize}
\item Teach Maths using AI exercises (where AI problems are formulated and treated analytically).
\item Teach AI using coding exercises.
\item Teach coding using maths-implementation exercises.
\end{itemize}
But I haven't yet adopted this myself in my teaching.


\subsection{Notation: Some seeming trivialities}

Equations and mathematical expressions have a syntax. This is hardly
ever made explicit\footnote{Except perhaps by G{\"o}del's incompleteness
theorems and areas like automated theorem proving.} and might seem
trivial. But it is surprising how buggy mathematical statements can be
in scientific papers (and oral exams). I don't want to write much text
about this, just some bullet points:

\begin{itemize}
\item Always declare mathematical objects.
\item Be aware of variable and index scoping. For instance, if you have an
equation, and one side includes a variable $i$, the other side doesn't,
this often is a notational bug. (Unless this equation actually makes
a statement about independence on $i$.)
\item Type checking. Within an equation, be sure to know exactly of
what type each term is: vector? matrix? scalar? tensor? Is the type and dimension of both sides of the equation consistent?
\item Decorations are ok, but really not necessary. It is much more
important to declare all things. E.g., there are all kinds of decorations used for vectors, $\boldsymbol
v, \underline v, \overrightarrow v, |v\rangle$ and matrices. But these are
not necessary. Properly declaring all symbols is much more
important.
\item When declaring sets of indexed elements, I use the notation $\{ x_i \}_{i=1}^n$. Similarly for tuples: $(x_i)_{i=1}^n$,
  $(x_1,..,x_n)$, $x_{1:n}$.
\item When defining sets, we write something like $\{ f(x) : x \in\RRR \}$, or $\{ n \in\NNN : \exists~\{
v_i\}_{i=1}^n~ \text{linearly independent}, v_i \in V\}$
\item I usually use bracket $[a=b] \in \{0,1\}$ for the boolean indicator function of some expression. An alternative notation is $\mathbb I(a=b)$, or the  \Def{Kronecker} symbol $\d_{ab}$.
\item A tuple $(a,b) \in A \times B$ is an element of the product space.
%% \item The \Def{outer product} $x \otimes y \in X \times Y$ is almost the same as
%% $(x,y) \in X \times Y$, only slightly different in standard vector and tensor context);
%% don't confuse with the cross product in 3D!
\item \Def{direct sum} $A \oplus B$ is same as $A \times B$ in
finite-dimensional spaces
\item If $f: X \to Y$, then $\min_x f(x) \in Y$ is minimal function \emph{value} (output); whereas $\argmin_x f(x) \in X$ is the input (``argument'') that minimizes the function. E.g., $\min_x f(x) = f(\argmin_x f(x))$.
\item One should distinguish between the infimum $\inf_x f(x)$ and supremum $\sup_x f(x)$ from the min/max: the inf/sup refer to limits, while the min/max to values actually aquired by the function. I must admit I am sloppy in this regard and usually only write min/max.
\item Never use multiple letters for one thing. E.g.\ $length = 3$
means $l$ times $e$ times $n$ times $g$ times $t$ times $h$ equals
$3$.
\item There is a difference between $\to$ and $\mapsto$:
\begin{equation}
 f:~ \RRR \to \RRR,~ x \mapsto \cos(x)
\end{equation}
\item The dot is used to help defining functions with only some arguments fixed: 
\begin{equation}
f:~ A\times B \to C \comma f(a,\cdot):~ B \to C,~ b\mapsto f(a,b)
\end{equation}
Another example is the typical declaration of an inner product: $\<\cdot,\cdot\>: V \times V \to \RRR$.
\item The \Def{$p$-norm} or \Def{$L^p$-norm} is defined as $\norm{x}_p = [\sum_i x_i^p]^{1/p}$. By default
$p=2$, that is $\norm{x}=\norm{x}_2=|x|$, which is the \Def{$L^2$-norm} or \Def{Euclidean length} of a
vector. Also the \Def{$L^1$-norm} $\norm{x}_1 = \sum_i |x_i|$ (aka \Def{Manhattan distance}) plays an important role.
\item I use $\diag(a_1,..,a_n) = \mat{ccc}{a_1 && 0 \\ & \ddots & \\ 0 &&
a_n}$ for a diagonal matrix. And overload this so that $\diag(A) = (A_{11},..,A_{nn})$ is the diagonal vector of the matrix $A\in\RRR^{n\times n}$.
\item A typical convention is
\begin{equation}
\vec 0_n = (0,..,0)\in\RRR^n
\comma \vec 1_n = (1,..,1) \in \RRR^n
\comma \Id_n = \diag(\vec 1_n)
\end{equation}
Also, $e_i=(0,..,0,1,0,..,0) \in\RRR^n$ often denotes the $i$th column of
the identity matrix, which of course are the coordinates of a basis
vector $e_i\in\VV$ in a basis $(e_i)_{i=1}^n$.
\item The element-wise product of two matricies $A$ and $B$ is also
called \Def{Hadamard product} and notated $A\circ B$ (which has nothing to
do with the concatenation of two operations). If there is need,
perhaps also use this to notate the element-wise product of two vectors.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Functions \& Derivatives}

\subsection{Basics on uni-variate functions}

We super quickly recap the basics of functions $\RRR \to \RRR$, which the reader might already know.

\subsubsection{Continuous, differentiable, \& smooth functions}

\begin{itemize}
\item A function $f:\RRR\to\RRR$ is \Def{continuous} at $x$ if the limit $\lim_{h\to 0} f(x+h) = f(x)$ exists and equals $f(x)$ (from both sides).
\item A function $f:\RRR\to\RRR$ is \Def{diferentiable} at $x$ if $f'(x) = \lim_{h\to 0} \frac{f(x+h) - f(x)}{h}$ exists. Note, differentiable $\To$ continuous.
\item A function is continuous/differentiable if it is continuous/differentiable at any $x\in\RRR$.
\item A function $f$ is an element of $\CC^k$ if it is $k$-fold continuously differentiable, i.e., if its $k$-th derivative $f^{(k)}$ is continuous. For example, $\CC^0$ is the space of continuous functions, and $\CC^2$ the space of twice continuously differentiable functions.
\item A function $f\in\CC^\infty$ is called \Def{smooth} (infinitely often differentiable).
\end{itemize}

\subsubsection{Polynomials, piece-wise, basis functions, splines}

Let's recap some basic functions that often appear in AI research.
\begin{itemize}
\item A \Def{polynomial} of degree $p$ is of the form $f(x) = \sum_{i=0}^p a_i x^i$, which is a weighed sum of \Def{monimials} $1, x, x^2,..$. Note that for multi-variate functions, the number of monimials grows combinatorially with the degree and dimension. E.g., the monimials of degree $2$ in $3$D space are $x_1^2$, $x_1 x_2$, $x_1 x_3$, $x_2^2$, $x_2 x_3$, $x_3^2$. In general, we have $\left(d \atop p\right)$ monimials of degree $p$ in $d$-dimensional space.
\item Polynomials are smooth.
\item I assume it is clear what piece-wise means (we have different polynomials in disjoint intervals covering the input domain $\RRR$).

\item A set of \Def{basis functions} $\{e_1,...,e_n\}$  defines the space $f = \{ \sum_i a_i e_i : a_i \in \RRR \}$ of functions that are linear combinations of the basis functions. More details on bases are discussed below for vector spaces in general. Here we note typical basis functions:
\begin{items}
\item Monimials, which form the basis of polynomials.

\item Trigonometric functions $\sin(2\pi a x), \cos(2\pi a x)$, with integers $a$, which form the Fourier basis of functions.

\item Radial basis functions $\varphi(|x-c_i|)$, where we have $n$ centers $c_i \in\RRR$, $i=1,..,n$, and $\varphi$ is typically some bell-shaped or local support function around zero. A very common one is the squared exponential $\varphi(d) = \exp\{- l d^2\}$ (which you might call a non-normalized Gaussian with variance $1/l$).
\end{items}

\item The above functions are all examples of \Def{parameteric functions}, which means that they can be specified by a finite number of parameters $a_i$. E.g., when we have a finite set of basis functions, the functions can all be described by the finite set of weights in the linear combination.

However, in general a function $f:\RRR \to \RRR$ is an ``infinite-dimensional object'', i.e., it has infinitely many degrees-of-freedom $f(x)$, i.e., values at infinitely many points $x$. In fact, sometimes it is useful to think of $f$ as a ``vector'' of elements $f_x$ with continuous index $x$. Therefore, the space of all possible functions, and also the space of all continuous function $\CC^0$ and smooth functions $\CCC^\infty$, is infinite-dimensional. General functions cannot be specified by a finite number of parameters, and they are called \Def{non-parameteric}.

The core example are functions used for regression or classification in Machine Learning, which are a linear combination of an \emph{infinite} set of basis functions. E.g., an infinite set of radial basis functions $\varphi(|x-c|)$ for \emph{all} centers $c \in \RRR$. This infinite set of basis functions spans a function space called Hilbert space (in the ML context, ``Reproducing Kernel Hilbert Space (RKHS)''), which is an infinite-dimensional vector space. Elements in that space are called non-parameteric.

\item As a final note, splines are parameteric functions that are often used in robotics and engineering in general. Splines usually are piece-wise polynomials that are continuously joined. Namely, a spline of degree $p$ is in $\CC^{p-1}$, i.e., $p-1$-fold continuously differentiable. A spline is not fully smooth, as the $p$-th derivative is discontinuous. E.g., a cubic spline has a piece-wise constant (``bang-bang'') jerk (3rd derivative). Cubic splines and B-splines (which are also piece-wise polynomials) are commonly used in robotics to describe motions. In computational design and graphics, B-splines are equally common to describe surfaces (in the form of Non-uniform rational basis spline, NURBS). Appendix \ref{appSplines} is a brief reference for splines.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Partial vs.\ total derivative, and chain rules}

\subsubsection{Partial Derivative}

A multi-variate function $f: \RRR^n \to \RRR$ can be thought of
as a function of $n$ arguments, $f(x_1,..,x_n)$.

\begin{myDefinition}
The \emph{partial} derivative of a function of multiple arguments
$f(x_1,..,x_n)$ is the standard derivative w.r.t.\ only one of its arguments,
\begin{equation}
\Del{x_i} f(x_1,..,x_n) = \lim_{h\to
0} \frac{f(x_1,..,x_i+h,..,x_n)-f(x)}{h} ~.
\end{equation}
\end{myDefinition}


\subsubsection{Total derivative, computation graphs, forward and backward chain rules}

Let me start with an example: We have three real-valued quantities $x$, $g$ and $f$
which depend on each other. Specifically,
\begin{equation}
f(x,g) = 3x + 2g \quad \text{and}\quad g(x)=2 x ~.
\end{equation}
Question: \emph{What is the ``derivative of $f$ w.r.t.\ $x$''?}

The correct answer is: \emph{Which one do you mean? The partial or total?}

The partial derivative defined above really thinks of $f(x,g)$ as a
function of two arguments, and does not at all care about
whether there might be dependencies of these arguments. It only looks
at $f(x,g)$ alone and takes the partial derivative (=derivative
w.r.t.\ one function argument):
\begin{equation}
\Del{x} f(x,g) = 3 
\end{equation}

However, if you suddenly talk about $h(x) = f(x,g(x))$ as a function of the argument $x$ only, that's a totally different story, and
\begin{equation}
\Del{x} h(x) = \Del{x} 3x+2(2x) = 7 
\end{equation}

Bottom line, the definition of the partial derivative really depends
on what you explicitly defined as the arguments of the function.

To allow for a general treatment of differentiation with dependences we need to define a very useful concept:
\begin{myDefinition}
A \Def{function network} or \Def{computation graph} is a directed acyclic graph (DAG) of $n$ quantities $x_i$ where each quantity is a deterministic function of a set of parents
$\pi(i)\subset\{1,..,n\}$, that is
\begin{equation}
x_i = f_i( x_{\pi(i)} ) 
\end{equation}
where $x_{\pi(i)} = (x_j)_{j \in \pi(i)}$
is the tuple of parent values. This could also be
called a \emph{deterministic Bayes net}.
\end{myDefinition}

In a function network all values can be computed deterministically if
the input values (which do have no parents) are given. Concerning differentiation, we may now ask: Assume
we have a variation $dx$ of some input value, how do all other values
vary? The chain rules give the answer. It turns our \textbf{there are two chain rules} in function networks:

\begin{Identities}[Chain rule (Forward-Version)]
 \begin{equation}
\frac{df}{dx} = \sum_{g\in\pi(f)} \frac{\del f}{\del g}~ \frac{dg}{dx}
  \qquad\qquad \text{(with $\frac{dx}{dx}\equiv 1$, in case $x\in\pi(f)$)}
\end{equation}
%%\begin{equation}
%%\frac{df}{dx} = \frac{\del f}{\del x} + \sum_{g\in\pi(f)\atop
%% g \not= x} \frac{\del f}{\del g}~ \frac{dg}{dx}
%%\end{equation}
\end{Identities}
Read this as follows: ``The change of $f$ with $x$ is the sum of changes that come from its direct dependence on $g\in\pi(f)$, each multiplied the change of $g$ with $x$.''

This rule defines the \Def{total derivative} of $\frac{df}{dx}$ w.r.t.\
$x$. Note how different these two notions of derivatives are by
definition: a partial derivative only looks at a function itself and takes a
limit of differences w.r.t.\ one argument---no notion of further
dependencies. The total derivative asks how, in a function network,
one value changes with a change of another.

\begin{figure}
\show[.4]{chainRule}
\caption{\label{figChainRule}
General Chain Rule. Left: Forward-Version, Right:
Backward-Version. They gray arc denotes the direct dependence
$\frac{\del f}{\del x}$, which appears in the summations via $dx/dx \equiv 1$, $df/df \equiv 1$.}
\end{figure}

The second version of the chain rule is:
\begin{Identities}[Chain rule (Backward-Version)]
\begin{equation}
\frac{df}{dx} = \sum_{g:x\in\pi(g)} \frac{d f}{d g}~ \frac{\del g}{\del x}
  \qquad\qquad \text{(with $\frac{df}{df}\equiv 1$, in case $x\in\pi(f)$)}
\end{equation}
%%\begin{equation}
%%\frac{df}{dx} = \frac{\del f}{\del x} + \sum_{g:x\in\pi(g)\atop
%% g\not= f} \frac{d f}{d g}~ \frac{\del g}{\del x}
%%\end{equation}
\end{Identities}
Read this as follows: ``The change of $f$ with $x$ is the sum of changes that arise from all changes of $g$ which directly depend on $x$.''

Figure \ref{figChainRule} illustrates the fwd and bwd versions of the
chain rule. The bwd version allows you to propagate back, given
gradients $\frac{d f}{d g}$ from top to $g$, one step further down,
from top to $x$. The fwd version allows you to propagate forward,
given gradients $\frac{d g}{d x}$ from $g$ to bottom, one step further
up, from $f$ to bottom. Both versions are recursive equations. If you
would recursively plug in the definition for a given functional network,
both of them would yield the same expression of $\frac{df}{dx}$ in
terms of partial derivatives only.

Let's compare to the chain rule as it is commonly found in other texts (written more precisely):
\begin{equation}
\frac{\del f(g(x))}{\del x} = \frac{\del f(g)}{\del
 g}\bigg|_{g=g(x)}~ \frac{\del g(x)}{\del x}
\end{equation}
Note that we here very explicitly notated that $\frac{\del f(g)}{\del
g}$ considers $f$ to be a function of the argument $g$, which is
evaluted at $g=g(x)$. Written like this, the rule is fine. But the
above discussion and explicitly distinguishing between partial and
total derivative is, when things get complicated, less prone to
confusion.

\subsection{Gradient, Jacobian, Hessian, Taylor Expansion}

\subsubsection{Gradient \& Jacobian}

Let's take the next step and consider functions $f:~ \RRR^n \to \RRR^d$ that map from $n$ numbers to a $d$-dimensional output. In this case, we can take the partial derivative of each output w.r.t.\ each input argument, leading to a matrix of partial derivatives:
\begin{myDefinition}
Given $f:~ \RRR^n \to \RRR^d$, we define the derivative (also
called \Def{Jacobian} matrix) as
\begin{equation}
\newcommand{\jacf}[2]{\frac{\del}{\del x_#2} f_#1(x)}
\Del{x} f(x) =
 \mat{cccc}{
\jacf{1}{1} & \jacf{1}{2} & \dots & \jacf{1}{n} \\
\jacf{2}{1} & \jacf{2}{2} & \dots & \jacf{2}{n} \\
\vdots & & & \vdots \\
\jacf{d}{1} & \jacf{d}{2} & \dots & \jacf{d}{n} } 
\end{equation}
\end{myDefinition}

When the function only as one output dimension, $f:~ \RRR^n \to \RRR^1$, the partial derivative can be written as a vector. Unlike many other texts, I advocate for consistency with the Jacobian matrix (and contra-variance, see below) and define this to be a row vector:
\begin{myDefinition}
We define the derivative of
$f:~ \RRR^n \to \RRR$ as the row vector of partial derivatives
\begin{equation}
 \Del{x} f(x) = (\Del{x_1} f,~ ..,~ \Del{x_n} f) ~. 
\end{equation}
Further, we define the \Def{gradient} as the corresponding column ``vector''
\begin{equation}
 \na f(x) = \[\Del{x} f(x)\]^\T ~. 
\end{equation}
\end{myDefinition}

The ``purpose'' of a derivative is to output a change of function value
when being multiplied to a change of input $\d$. That is, in first
order approximation, we have
\begin{equation}
f(x+\d) - f(x) ~~\dot=~~ \del f(x)~ \d ~,
\end{equation}
where $\dot=$ denotes ``in first order approximation''. This equation holds, no matter if the output space is $\RRR^d$ or
$\RRR$, or the input space and variation is $\d\in\RRR^n$ or $\d\in\RRR$. In the gradient notation we have
\begin{equation}
f(x+\d) - f(x) ~~\dot=~~ \na f(x)^\T \d ~.
\end{equation}

Jumping ahead to our later discussion of linear algebra: The above
two equations are written in coordinates. But note that the equations are
truly independent of the choice of vector space basis and independent
of an optional metric or scalar product in $V$. The transpose should
not be understood as a scalar product between two vectors, but rather
as undoing the transpose in the definition of $\na f$. All this is
consistent to understanding the derivatives as coordinates of a
1-form, as we will introduce it later.

Given a certain direction $d$ (with $|d|=1$) we define the \Def{directional derivative} as $\na f(x)^\T d$, and it holds
\begin{align}
\na f(x)^\T d = \lim_{\e\to 0} \frac{f(x+\e d)-f(x)}{\e} ~.
\end{align}


\subsubsection{Hessian}

\begin{myDefinition}
We define the \Def{Hessian} of a scalar function $f: \RRR^n \to \RRR$ as the
symmetric matrix
\newcommand{\hessf}[2]{\frac{\del^2}{\del x_#1 \del x_#2} f}
\begin{equation}
\he f(x) = \Del x \na f(x) = 
 \mat{cccc}{
\hessf{1}{1} & \hessf{1}{2} & \dots & \hessf{1}{n} \\
\hessf{2}{1} & \hessf{2}{2} & \dots & \hessf{2}{n} \\
\vdots & & & \vdots \\
\hessf{n}{1} & \hessf{n}{2} & \dots & \hessf{n}{n} } 
\end{equation}
\end{myDefinition}

The Hessian can be thought of as the
Jacobian of $\na f$. Using the Hessian, we can express the 2nd order
approximation of $f$ as:
\begin{equation}
f(x+\d) ~~\ddot=~~ f(x) + \del f(x)~ \d + \half \d^\T~ \he f(x)~ \d ~.
\end{equation}

For a uni-variate function $f:\RRR \to \RRR$, the Hessian is just a single number, namely the second derivative $f''(x)$. In this section, let's call this the ``curvature'' of the function (not to be confused with the Rimannian curvature of a manifold). In the uni-variate case, we have the obvious cases:
\begin{itemize}
\item If $f''(x) >0$, the function is locally ``curved upwared'' and  \Def{convex} (see also the formal Definition \ref{secCvx}).
\item If $f''(x) <0$, the function is locally ``curved downward'' and  \Def{concave}.
\end{itemize}

In the multi-variate case $f:\RRR^n \to \RRR$, the Hessian matrix $H$ is symmetric and we can decompose it as $H = \sum_i \l_i
 h_i h_i^\T$ with eigenvalues $\l_i$ and eigenvectors $h_i$ (which we will learn about in detail later). Importantly, all $h_i$ will be orthogonal to each other, forming a nice orthonormal basis.

This insight gives us a very strong intuition on \textbf{how the Hessian $H$ describes the local curvature} of the function $f$:
$\l_i$ gives the \textbf{directional curvature}, i.e., the curvature in the direction of eigenvector $h_i$. If $\l_i>0$, $f$ is curved upward along $h_i$; if
$\l_i<0$, $f$ is curved downward along $h_i$. Therefore, the eigenvalues $\l_i$ tell us whether the function is locally curved upward, downward, or flat in each of the orthogonal directions $h_i$.

This becomes particular intuitive if $\Del{x} f=0$ is zero, i.e., the derivative (slope) of the function is zero in all directions. When the curvatures $\l_i$ are positive in all directions, the function is locally convex (upward parabolic) and $x$ is a local minimum; if the curvatures $\l_i$ are all negative, the function is concave (downward parabolic) and $x$ is a local maximum; if some curvatures are positive and some are negative along different directions $h_i$, then the function curves down in some directions, and up in others, and $x$ is a \Def{saddle point}.

Again jumping ahead, in the coordinate-free notation, the second derivative would be defined as the 2-form
\begin{align}
d^2f\big|_x &:~ V\times V \to G,~ \\
(v,w) &\mapsto \lim_{h\to 0} \frac{df\big|_{x+hw}(v) -f\big|_{x}(v)}{h} \\
 &= \lim_{h,l\to 0} \frac{f(x+hw+lv) - f(x+hw) - f(x+lv) + f(x)}{hl} ~.
\end{align}
The Hessian matrix are the coordinates of this 2-form (which would actually be a row vector of row vectors).

\subsubsection{Taylor expansion}

In 1D, we have
\begin{align}
f(x+v) \approx f(x) + f'(x) v + \frac{1}{2} f''(x) v^2 + \cdots + \frac{1}{k!} f^{(k)}(x) v^k
\end{align}

For $f: \RRR^n\to \RRR$, we have
\begin{align}
f(x+v)&\approx f(x) + \na f(x)^\T v + \half v^\T \he f(x) v + \cdots
\end{align}
which is equivalent to
\begin{align}
f(x+v)
&\approx f(x)
+ \sum_j \Del{x_j} f(x) v_j
+ \frac{1}{2} \sum_{jk} \frac{\del^2}{\del x_j \del x_k} f(x) v_j v_k
%% & \quad + \frac{1}{6} \sum_{jkl} \frac{\del^3}{\del x_j \del x_k \del x_l} f(x) v_j v_k v_l
+ \cdots
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Derivatives with matrices}

The next section will introduce linear algebra from scratch -- here we first want to learn how to practically deal with derivatives in matrix expressions. We think of matrices and vectors simply as arrays of numbers $\in \RRR^{n\times m}$ and $\RRR^n$. As a warmup, try to solve the following exercises:
\begin{enumerate}
\item Let $X,A$ be arbitrary matrices, $A$ invertible. Solve for $X$:
\begin{equation}
 X A + A^\T = \Id 
\end{equation}

\item Let $X,A,B$ be arbitrary matrices, $(C-2A^\T)$ invertible. Solve for $X$:
\begin{equation}
 X^\T C = [2 A (X + B)]^\T 
\end{equation}

\item Let $x\in\RRR^n,y\in\RRR^d,A\in\RRR^{d\times n}$. $A$ obviously \emph{not}
invertible, but let $A^\T A$ be invertible. Solve for $x$:
\begin{equation}
 (A x - y)^\T A = \vec 0_n^\T 
\end{equation}

\item As above, additionally $B\in\RRR^{n\times n}$, $B$
positive-definite. Solve for $x$: 
\begin{equation}
 (A x - y)^\T A + x^\T B = \vec 0_n^\T 
\end{equation}

\item A core problem in Machine Learning: For $\b\in\RRR^d,~ y\in\RRR^n,~ X
\in\RRR^{n\times d}$, compute
\begin{equation}
\argmin_\b \norm{y - X \b}^2 + \l \norm{\b}^2 ~.
\end{equation}

\item A core problem in Robotics: For
$q,q_0\in\RRR^n,~ \phi:~ \RRR^n \to \RRR^d,~ y^*\in\RRR^d$ non-linear but
smooth, compute
\begin{equation}
\argmin_q \norm{\phi(q) - y^*}^2_C + \norm{q-q_0}_W^2 ~.
\end{equation}
Use a local linearization of $\phi$ to solve this.
\end{enumerate}

For problem (v), we want to find a \Def{minimum for a matrix expression}. We find this by setting the derivative equal to zero. Here is the solution, and below details will become clear:
\begin{align}
0 &= \Del{\b} \norm{y - X \b}^2 + \l \norm{\b}^2 \\
&= 2 (y - X\b)^\T (-X) + 2 \l \b^\T \\
0 &= - X^\T (y - X\b) + 2 \l \b \\
0 &= - X^\T y + (X^\T X + 2 \l \Id) \b \\
\b &= - (X^\T X + 2 \l \Id)^\1 X^\T y
\end{align}
Line 2 uses a standard rule for the derivative (see below) and gives a
row vector equation. Line 3 transposes this to become a column vector
equation.


\subsubsection{Derivative Rules}

As 2nd order terms are very common in AI methods, this is a very useful identity to learn:
\begin{Identities}
\begin{align}
\Del x~ f(x)^\T A g(x)
 &= f(x)^\T A \Del x g(x) + g(x)^\T A^\T \Del x f(x)
\end{align}
\end{Identities}
Note that using the 'gradient column' convention this reads
\begin{equation}
\na_x~ f(x)^\T A g(x) = [\Del x  g(x)]^\T A^\T f(x) + [\Del x f(x)]^\T A g(x) 
\end{equation}
which I find impossible to remember, and mixes gradients-in-columns
($\na$) with gradients-in-rows (the Jacobian) notation.

Special cases and variants of this identity are:
\begin{align}
\Del x [\textit{whatever}] x
&= [\textit{whatever}] \comma\text{if \textit{whatever} is indep. of $x$} \\
\Del x a^\T x
&= a^\T \\
\Del x A x
&= A \\
\Del x (A x - b)^\T (C x - d)
&= (A x - b)^\T C + (C x - d )^\T A \\
\Del x x^\T A x
&= x^\T A + x^\T A^\T\\
\Del x \norm{x}
&= \Del x (x^\T x)^\half = \half (x^\T x)^{-\half}~ 2 x^\T = \frac{1}{\norm{x}} x^\T \\
\Hes x (Ax+a)^\T C (Bx+b)
&= A^\T C B + B^\T C^\T A 
\end{align}

Further useful identities are:
\begin{Identities}[Derivative Rules]
\begin{align}
\Del \t |A|
 &= |A|~ \tr(A^\1~ \Del \t A) \\
\Del \t A^\1
 &= - A^\1~ (\Del \t A)~ A^\1 \\
\Del \t \tr(A)
 &= \sum_i \Del \t A_{ii}
\end{align}
\end{Identities}

We can also directly take a derivative of a scalar value w.r.t.\ a
matrix:
\begin{align}
\Del X a^\T X b
 &= ab^\T \\
\Del X (a^\T X^\T C X b)
 & = C^\T X a b^\T + C X b a^\T \\
\Del X \tr(X)
 &= \Id
\end{align}
But if this leads to confusion I would recommend to never take a
derivative w.r.t.\ a matrix. Instead, perhaps take the derivative
w.r.t.\ a matrix element: $\Del{X_{ij}} a^\T X b = a_i b_j$.

For completeness, here are the most important matrix identities (the appendix lists more):
\renewcommand{\bar}{\widehat}
\begin{Identities}[Matrix Identities]
\begin{align}
&(A^\1 + B^\1)^\1 = A~ (A+B)^\1~ B = B~ (A+B)^\1~ A \\
&(A^\1 - B^\1)^\1 = A~ (B-A)^\1~ B \\
&(A+UBV)^\1 = A^\1 - A^\1 U (B^\1 + VA^\1U)^\1 V A^\1 \label{locwood}\\
&(A^\1+B^\1)^\1 = A - A (B + A)^\1 A \\
&(A + J^\T B J)^\1 J^\T B 
= A^\1 J^\T (B^\1 + J A^\1 J^\T)^\1 \label{locwood2}\\
&(A + J^\T B J)^\1 A
= \Id - (A + J^\T B J)^\1 J^\T B J  \label{locnull}
\end{align}
(\ref{locwood})=Woodbury; (\ref{locwood2},\ref{locnull}) holds for pos def $A$
and $B$. See also
the \href{http://www.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf}{matrix
cookbook}.
\end{Identities}


\subsubsection{Example: GP regression}

An example from GP regression: The log-likelihood gradient w.r.t.\ a
kernel hyperparameter:
\begin{align}
\log P(y | X, b)
&= -\half y^\T K^\1 y - \half \log|K| - \frac{n}{2} \log 2\pi \\
& \text{where}\quad K_{ij} = e^{-b(x_i-x_j)^2} + \s^2 \d_{ij} \\
\Del b y^\T K^\1 y
&= y^\T (-K^\1 (\Del b K) K^\1) = y^\T (-K^\1 A K^\1) ~, \feed
&\quad \text{with}~ A_{ij} = - (x_i-x_j)^2~ e^{-b(x_i-x_j)^2} \\
\Del b \log|K|
&= \frac{1}{|K|}~ \Del b |K|
 = \frac{1}{|K|}~ |K|~ \tr(K^\1 \Del b K)
 = \tr(K^\1 A)
\end{align}

\subsubsection{Example: Logistic regression}

An example from logistic regression: We have the loss gradient and
want the Hessian:
\begin{align}
\na_\b L
 &= X^\T (p-x) + 2 \l \Id \b \\
 & \text{where}\quad p_i = \s(x_i^\T \b) \comma \s(z) = \frac{e^z}{1+e^z} \comma \s'(z) =\s(z)~ (1-\s(z)) \\
\he[\b] L
 &= \Del \b \na_\b L = X^\T \Del \b p + 2\l \\
\Del \b p_i
 &= p_i (1-p_i)~ x_i^\T \\
\Del \b p
 &= \diag( [p_i (1-p_i)]_{i=1}^n )~ X = \diag(p\circ(1-p))~
 X\\
\he[\b] L
 &= X^\T \diag(p\circ (1-p) )~ X + 2\l\Id
\end{align}
(Where $\circ$ is the element-wise product.)


\subsection{Check your gradients numerically!}

This is your typical work procedure when implementing a Machine
Learning or AI'ish or Optimization kind of methods:
\begin{itemize}
\item You first mathematically (on paper/LaTeX) formalize the problem
domain, including the objective function.
\item You derive analytically (on paper) the gradients/Hessian of your
objective function.
\item You implement the objective function and these analytic gradient
equations in Matlab/Python/C++, using linear algebra packages.
\item You \textbf{test the implemented gradient equations by comparing
them to a finite difference estimate of the gradients}!
\item Only if that works, you put everything together, interfacing the
objective \& gradient equations with some optimization algorithm
\end{itemize}

\begin{algorithm}
\caption{Finite Difference Jacobian Check}
\begin{algorithmic}[1]
\Require $x\in\RRR^n$, function $f:~ \RRR^n \to \RRR^n$, function
$df:~ \RRR^n \to \RRR^{d\times n}$
\State initialize $\hat J \in \RRR^{d\times m}$, and $\e=10^{-6}$
\For{$i=1:n$}
\State $\hat J_{\cdot i} =  [f(x + \e \vec e_i) - f(x - \e \vec e_i)]/2\e$
\Comment{assigns the $i$th column of $\hat J$}
\EndFor
\State if $\norm{\hat J - df(x)}_\infty < 10^{-4}$ return true; else false
\end{algorithmic}
Here $\vec e_i$ is the $i$th standard basis vector in $\RRR^n$.
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Examples and Exercises}

\exsection{More derivatives}

a) In 3D, note that $a \times b = \skew(a) b = - \skew(b) a$, where
$\skew(v)$ is the skew matrix of $v$. What is the gradient of
$(a \times b)^2$ w.r.t.\ $a$ and $b$?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\input{e06-matlabGradients.tex}
\input{e04-derivatives.tex}
\input{e05-neuralNet.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Linear Algebra}

\subsection{Vector Spaces}

\subsubsection{Why should we care for vector spaces in intelligent systems
research?}

We want to describe intelligent systems. For this we describe systems,
or aspects of systems, as elements of a space:
\begin{items}
\item The input space $X$, output space $Y$ in ML
\item The space of functions (or classifiers) $f: X \to Y $ in
ML
\item The space of world states $S$ and actions $A$ in Reinforcement Learning
\item The space of policies $\pi:~ S \to A$ in RL
\item The space of feedback controllers $\pi:~ x \mapsto u$ in robot control
\item The configuration space $Q$ of a robot
\item The space of paths $x:~ [0,1] \to Q$ in robotics
\item The space of image segmentations $s:~ I \to \{0,1\}$ in computer vision
\end{items}
Actually, some of these spaces are not vector spaces at all. E.g.\ the
configuration space of a robot might have `holes', be a manifold with
complex topology, or not even that (switch dimensionality at some
places). But to do computations in these spaces one always either
introduces (local) parameterizations that make them a vector
space,\footnote{E.g.\ by definition an $n$-dimensional manifold $X$
is locally isomorphic to $\RRR^n$.} or one focusses on local tangent
spaces (local linearizations) of these spaces, which are vector
spaces.

Perhaps the most important computation we want to do in these spaces
is taking derivatives---to set them equal to zero, or do gradient
descent, or Newton steps for optimization. But taking derivatives
essentially requires the input space to (locally) be a vector
space.\footnote{Also when the space is actually a manifold; the
  differential is defined as a 1-form on the local tangent.}  So, we
also need vector spaces because we need derivatives, and Linear
Algebra to deal with the resulting equations.


\subsubsection{What is a vector?}

A \Def{vector} is nothing but an element of a vector space.

It is in general not a column, array, or tuple of numbers. (But tuples of
numbers are a special case of a vector space.)

%% Unprecise/not rigorous: Orientation+length (but then, how do you
%% define 'orientation', if not by a vector). The 'thing' connecting two
%% points (but then, what is a point, and a 'connection' between two
%% points, if you've not defined a vector space first.)


\subsubsection{What is a vector space?}

\begin{myDefinition}[vector space]
A \Def{vector space}\footnote{We only consider vector spaces over $\RRR$.}
$V$ is a space (=set) on which two operations, addition and
multiplication, are defined as follows
\begin{itemize}
\item addition $+:~ V \times V \to V$ is an abelian group, i.e.,
\begin{items}
\item $a,b \in V \To a+b \in V$ ~ (closed under $+$)
\item $a+(b+c) = (a+b)+c$ ~ (association)
\item $a+b=b+a$ ~ (commutation)
\item $\exists \text{~unique~} 0 \in V \st \forall v\in V:~ 0+v = v$
~ (identity)
\item $\forall v\in V: \exists \text{~unique~} -v \st v+(-v) = 0$ ~ (inverse)
\end{items}
\item multiplication $\cdot:~ \RRR \times V \to V$ fulfils, for $\a,\b\in\RRR$, \begin{items}
\item $\a(\b v) = (\a \b) v$ ~ (association)
\item $1 v = v$ ~ (identity)
\item $\a(v+w) = \a v + \a w$ ~ (distribution)
 \end{items}
\end{itemize}
\end{myDefinition}

Roughly, this definition says that a vector space is ``closed under
linear transformations'', meaning that we can add and scale vectors and
they remain vectors.


\subsection{Vectors, dual vectors, coordinates, matrices, tensors}

In this section we explain what might be obvious: that once we have a
basis, we can write vectors as (column) coordinate vectors, 1-forms as
(row) coordinate vectors, and linear transformations as matrices. Only
the last subsection becomes more practical, refers to concrete
exercises, and explains how in practise not to get confused about
basis transforms and coordinate representations in different bases. So
a practically oriented reader might want to skip to the last
subsection.


\subsubsection{A taxonomy of linear functions}

For simplicity we consider only functions involving a single vector
space $V$. But all that is said transfers to the case when multiple
vector spaces $V, W, ...$ were involved.

\begin{myDefinition}
$f:V \to X$ \Def{linear} $\iff f(\a v+\b w) = \a f(v) + \b f(w)$,
where $X$ is any other vector space (e.g.\ $X=\RRR$, or $X=V\times V$).
\end{myDefinition}

\begin{myDefinition}
$f: V \times V \times \cdots \times V \to X$ \Def{multi-linear}
$\iff$ $f$ is linear in each input.
\end{myDefinition}

Many names are used for special linear functions---let's make some explicit:
\begin{items}
\item $f: V \to \RRR$, called linear
functional\footnote{The word 'functional' instead of 'function' is
especially used when $V$ is a space of functions.}, or 1-form, or dual vector.
\item $f: V \to V$, called linear function, or linear transform, or vector-valued 1-form
\item $f: V\times V \to \RRR$, called bilinear functional, or 2-form
\item $f: V\times V\times V  \to \RRR$, called 3-form (or
unspecifically 'multi-linear functional')
\item $f: V\times V \to V$, called vector-valued 2-form (or
unspecifically 'multi-linear map')
\item $f: V\times V \times V \to V \times V$, called bivector-valued 3-form
\item $f: V^k \to V^m$, called $m$-vector-valued $k$-form
\end{items}
This gives us a simple taxonomy of linear functions based on how many
vectors a function \emph{eats}, and how many it \emph{outputs}. To give
examples, consider some space $X$ of systems (examples above), which
might itself not be a vector space. But locally, around a specific
$x\in X$, its tangent $V$ is a vector space. Then
\begin{items}
\item $f: X \to \RRR$ could be a cost function over the system space.
\item The differential $df|_x: V \to \RRR$ is a 1-form, telling us how
$f$ changes when `making a tangent step' $v\in V$.
\item The 2nd derivative $d^2f|_x: V\times V \to \RRR$ is a 2-form,
telling us how $df|_x(v)$ changes when `making a tangent step' $w\in V$.
\item The inner product $\<\cdot , \cdot \>: V\times V \to \RRR$ is a
2-form.
\end{items}
Another example:
\begin{items}
\item $f: \RRR^i \to \RRR^o$ is a neural network that maps $i$ input
signals to $o$ output signals.
\item Its derivative $df|_x: \RRR^i \to \RRR^o$ is a vector-valued
1-form, telling us how each output changes with a step $v\in\RRR^i$ in
the input.
\item Its 2nd derivative $d^2f|_x: \RRR^i \times \RRR^i \to \RRR^o$ is
a vector-valued 2-form.
\end{items}
This is simply to show that vector-valued functions, 1-forms, and 2-forms
are common. Instead of being a neural network, $f$ could also be
a mapping from one parameterization of a system to another, or the
mapping from the joint angles of a robot to its hand position.

\subsubsection{Bases and coordinates}

We need to define some notions. I'm not commenting on these
definitions---train yourself in reading maths...

\begin{myDefinition}
$\Span(\{v_i\}_{i=1}^k ) = \{ \sum_i \a_i v_i : \a_i\in\RRR\}$
\end{myDefinition}

\begin{myDefinition}
$\{v_i\}_{i=1}^n$ \Def{linearly independent} $\iff \[ \sum_i \a_i v_i =
0 \To \forall_i \a_i=0 \]$
\end{myDefinition}

\begin{myDefinition}
$\dim(V) = \max_n \{ n \in\NNN : \exists~\{v_i\}_{i=1}^n~ \text{lin.indep.}, v_i \in V\}$
\end{myDefinition}

\begin{myDefinition}
$B=(e_i)_{i=1}^n$ is a \Def{basis} of $V$ $\iff$ $\Span(B) = V$
and $B$ lin.indep.
\end{myDefinition}

\begin{myDefinition}
The tuple $(v_1,v_2,..,v_n)\in\RRR^n$ is called
\Def{coordinates} of $v\in V$ in the basis
$(e_i)_{i=1}^n$ iff $v=\sum_i v_i e_i$
\end{myDefinition}

Note that $\RRR^n$ is also a vector space, and therefore coordinates
$v_{1:n}\in\RRR^n$ are also vectors, but in $\RRR^n$, not $V$. So
coordinates are vectors, but vectors in general not coordinates.


Given a basis $(e_i)_{i=1}^n$, we can describe every vector $v$ as a
linear combination $v=\sum_i v_i e_i$ of \emph{basic elements}---the
basis vectors $e_i$. This general idea, that ``linear things'' can be
described as linear combinations of ``basic elements'' carries over
also to functions. In fact, to all the types of functions we described
above: 1-forms, 2-forms, bi-vector-valued $k$-forms, whatever. And
if we describe all these als linear combinations of basic elements we
automatically also introduce coordinates for these things. To get
there, we first have to introduce a second type of ``basic elements'':
1-forms.

\subsubsection{The dual vector space -- and its coordinates}

\begin{myDefinition}
Given $V$, its \Def{dual space} is $V^* = \{ f: V\to \RRR~ \text{linear}\}$ (the space of 1-forms). Every $v^*\in V^*$ is
called 1-form or \Def{dual vector} (sometimes also \emph{covector}).
\end{myDefinition}

First, it is easy to see that $V^*$ is also a vector space: We can add
two linear functionals, $f = f_1 + f_2$, and scale them, and it remains
a linear functional.

Second, given a basis $(e_i)_{i=1}^n$ of $V$, we define a corresponding
dual basis $(\bar e_i)_{i=1}^n$ of $V^*$ simply by 
\begin{equation}
\forall_{i,j}:\quad \bar e_i (e_j) = \delta_{ij}
\end{equation}
where $\delta_{ij}= [i=j]$ is the \Def{Kronecker delta}. Note that
\begin{equation}
\forall v\in V:\quad \bar e_i (v) = v_i
\end{equation}
That is, $\bar e_i$ is the 1-form
that simply maps a vector to its $i$th coordinate. It can be shown
that $(\bar e_i)_{i=1}^n$ is in fact a basis of $V^*$. (Omitted.) That
tells us a lot!

$\dim(V^*) = \dim(V)$. That is, the space of 1-forms has
the same dimension as $V$. At this place, geometric intuition should
kick in: indeed, every linear function over $V$ could be envisioned as
a ``plane'' over $V$. Such a plane can be illustrated by its
iso-lines and these can be uniquely determined by their orientation
and distance (same dimensionality as $V$ itself). Also, (assuming we'd
know already what a transpose or scalar product is) every 1-form must be of the form $f(v) = c^\T v$ for some $c\in V$---so
every $f$ is uniquely described by a $c\in V$. Showing that the vector
space $V$ and its dual $V^*$ are really twins.

The dual basis $(\bar e_i)_{i=1}^n$ introduces coordinates in the dual
space: Every 1-form $f$ can be described as a linear combination of basis 1-forms,
\begin{equation}
f = \sum_i f_i \bar e_i
\end{equation}
where the tuple $(f_1,f_2,..,f_n)$ are the
coordinates of $f$. And
\begin{equation}
 \Span(\{\bar e_i\}_{i=1}^n) = V^* ~.
\end{equation}

\subsubsection{Coordinates for every linear thing: tensors}

We now have the \emph{basic elements}: the basis vectors
$(e_i)_{i=1}^n$ of $V$, and basis 1-forms $(\bar e_i)_{i=1}^n$ of
$V^*$. From these, we can describe, for instance,  any bivector-valued 3-form as
a linear combination as follows:
\begin{align}
f&:~ V \times V \times V \to V \times V \\
f&= \sum_{ijklm} f_{ijklm}~ e_i \otimes e_j \otimes \bar e_k \otimes \bar e_l \otimes \bar e_m
\end{align}
The $\otimes$ is called \Def{outer product} (or \Def{tensor product}),
and $v \otimes w \in V \times W$ if $V$ and $W$ are finite
vector spaces. For our purposes, we may think of $v \otimes w = (v,w)$
simply as the tuple of both. Therefore $e_i \otimes e_j \otimes
\bar e_k \otimes \bar e_l \otimes \bar e_m$ is a 5-tuple and we have in total
$n^5$ such basis objects---and $f_{ijklm}$ denotes the corresponding
$n^5$ coordinates. The first two indices are contra-variant, the
last three covariant---these notions are explained in detail later.

\subsubsection{Finally: Matrices}

As a special case of the above, every $f:V \to U$ can be described as a linear combination
\begin{equation}
f = \sum_{ij} f_{ij}~ e_i \otimes \bar e_j ~,
\end{equation}
where $(\bar e_j)_{j=1}^n$ is a basis of $V^*$ and $(e_i)$ a basis of $U$.


Let's see how this fits with some easier view, without all this fuss
about 1-forms. We already understood that the
operator $\bar e_j(v) = v_j$ simply picks the $j$th
coordinate of a vector. Therefore
\begin{equation}
f(v) = \[\sum_{ij} f_{ij}~ e_i \otimes \bar e_j\] (v) = \sum_{ij}
f_{ij}~ e_i v_j ~.
\end{equation}
In case it helps, we can `derive' this more slowly as
\begin{align}
f(v)
&= f(\sum_k v_k e_k)
 = \sum_k v_k f(e_k)
 = \sum_k v_k \[\sum_{ij} f_{ij}~ e_i \otimes \bar e_j\] e_k \\
&= \sum_{ijk} f_{ij}~ v_k~ e_i~ \bar e_j(e_k)
 = \sum_{ijk} f_{ij}~ v_k~ e_i~ \d_{jk}
 = \sum_i \[ \sum_j f_{ij}~ v_j \] e_i ~.
\end{align}
As a result, this tells us that the vector $u=f(v)\in V$ has the
coordinates $u_i=\sum_j f_{ij}~ v_j$. And the vector $f(e_j)\in
V$ has the coordinates $f_{ij}$, that is, $f(e_j) = \sum_i f_{ij}
e_i$.

So there are $n^2$ coordinates $f_{ij}$ for a linear function $f$. The first
index is contra-variant, the second covariant (explained later). As
it so happens, the whole world has agreed on a convention on how to
write such coordinate numbers on sheets of 2-dimensional paper: as
a \Def{matrix}!
\begin{equation}
\mat{cccc}{
f_{11} & f_{12} & \cdots & f_{1n} \\
f_{21} & f_{22} & \cdots & f_{2n} \\
\vdots & & & \vdots \\
f_{n1} & f_{n2} & \cdots & f_{nn}}
\end{equation}
The first (contra-variant) index spans columns; the second
(covariant) spans rows. We call this and the respective definition of
a matrix multiplication as the \Def{matrix convention}.

Note that the identity map $\Id: V \to V$ can be written as
\begin{align}
\Id = \sum_i e_i \otimes \bar e_i \comma \Id_{ij} = \d_{ij} ~.
\end{align}

Equally, the (contra-variant) coordinates of a vector are written as
columns
\begin{equation}
\mat{c}{v_1 \\ v_2 \\ \vdots \\ v_n} 
\end{equation}
and the (covariant) coordinates of a 1-form $h: V \to R$ as a row
\begin{equation}
\mat{cccc}{h_{1} & h_{2} & \cdots & h_{n} }
\end{equation}

$u = f(v)$ is itself a vector, and its coordinates written as a column are
\begin{equation}
\mat{c}{ u_1 \\ u_2 \\ \vdots \\ u_n}
=
\mat{cccc}{
f_{11} & f_{12} & \cdots & f_{1n} \\
f_{21} & f_{22} & \cdots & f_{2n} \\
\vdots & & & \vdots \\
f_{n1} & f_{n2} & \cdots & f_{nn}}~ 
\mat{c}{
v_1 \\
v_2 \\
\vdots \\
v_n}
\end{equation}
where this \Def{matrix multiplication} is defined by $u_i = \sum_j
f_{ij} v_j$, consistent to the above.

\begin{center}
  \begin{tabular}{||c||c||}
\hline
\hline
columns & rows \\
\hline
\multirow{3}{*}{vector} & 1-form  \\
& co-vector \\
& derivative \\
\hline
output space & input space \\
\hline
co-variant & contra-variant \\
\hline
contra-variant coordinates & co-variant coordinates \\
\hline
\hline
\end{tabular}
\end{center}


\subsubsection{Coordinate transformations}\label{coordinate}

The above was rather abstract. The exercises demonstrate representing vectors and transformations with coordinates and matrices
in different input and output bases. We just summarize here:
\begin{itemize}
\item We have two bases $\AA=(a_1,..,a_n)$ and $\BB=(b_1,..,b_n)$, and
  the transformation $T$ that maps each $a_i$ to $b_i$, i.e., $\BB = T
  \AA$.

\item Given a vector $x$ we denote its \textbf{coordinates} in $\AA$
  by $[x]^A$ or briefly as $x^A$. And we denotes its coordinates in
  $\BB$ as $[x]^B$ or $x^B$. E.g., $x^A_i$ is the $i$th coordinate in
  basis $\AA$.

\item $[b_i]^A$ are the coordinates of the new basis vectors in the
  old basis. The \textbf{coordinate transformation matrix} $B$ is
  given with elements $B_{ij} = [b_j]^A_i$. Note that
\begin{equation}
[x]^A = B
  [x]^B~,
\end{equation}
i.e., while the basis transform $T$ carries old basis $a_i$
  vectors to new basis vectors $b_i$, the matrix $B$ carries
  coordinates $[x]^B$ in the new basis to coordinates $[x]^A$ in the
  old basis! This is the origin of understanding that coordinates are
  contra-variant.

\item Given a linear transform $f$ in the vector space, we can
  represent it as a matrix in four ways, using basis $\AA$ or $\BB$ in
  the input and output spaces, respectively. If $[f]^{AA} = F$ is the matrix in
  old coordinates (using $\AA$ for input and output), then $[f]^{BB} = B^\1 F B$
  is its matrix in new coordinates, $[f]^{AB} = F B$ is its matrix using $\BB$
  for the input and $\AA$ for the output space, and $[f]^{BA} = B^\1 F$ is the
  matrix using $\AA$ for input and $\BB$ for output space.

\item $T$ itself is also a linear transform. $[T]^{AA} = B$ is its matrix in old
  coordinates. And the same $[T]^{BB} = B$ is also its matrix in new coordinates! $[T]^{BA} = \Id$ is its matrix when using
  $\AA$ for input and $\BB$ for output space. And $[T]^{AB} = B^2$ is its matrix
  using $\BB$ for input and $\AA$ for output space.
\end{itemize}









\subsection{Scalar product and orthonormal basis}

Please note that so far we have not in any way referred to a scalar
product or a transpose. All the concepts above, dual vector space,
bases, coordinates, matrix-vector multiplication, are fully
independent of the notion of a scalar product or transpose. Columns
and rows naturally appear as coordinates of vectors and 1-forms. But
now we need to introduce scalar products.

\begin{myDefinition}
A \Def{scalar product} (also called inner product) of $V$ is a
symmetric positive definite 2-form
\begin{equation}
\<\cdot,\cdot\>:~ V\times V \to \RRR ~.
\end{equation}
with $\<v,w\> = \<w,v\>$ and $\<v,v\> > 0$ for all $v\not=0 \in V$.
\end{myDefinition}

\begin{myDefinition}
Given a scalar product, we define for every $v\in V$ its \Def{dual}
$v^*\in V^*$ as
\begin{equation}
v^* = \<v, \cdot\> = \sum_i v_i \<e_i,\cdot\> = \sum_i v_i e_i^* ~.
\end{equation}
\end{myDefinition}

Note that $\bar e_i$ and $e_i^*$ are in general different 1-forms! The canonical
dual basis $(\bar e_i)_{i=1}^n$ is independent of an introduction of a
scalar product, they were the basis to introduce coordinates for
linear functions, including matrices. And while such coordinates do depend
on a choice of basis $(e_i)_{i=1}^n$, they do \emph{not} depend on a choice
of scalar product.

The 1-forms $(e_i^*)_{i=1}^n$ also form a basis for $V^*$, but a different
one to the canonical basis, and one that depends on the notion of a
scalar product. You can see this: the coordinates $v_i$ of $v^*$ in
the basis $(e_i^*)_{i=1}^n$ are identical to the coordinates $v_i$ of $v$
in the basis $(e_i)_{i=1}^n$, but different to the coordinates $(v^*)_i$ of
$v^*$ in the basis $(\bar e_i)_{i=1}^n$.

\begin{myDefinition}
Given a scalar product, a set of vectors $\{v_i\}_{i=1}^n$ is
called \Def{orthonormal} iff $\<v_i,v_j\> = \d_{ij} ~.$
\end{myDefinition}

\begin{myDefinition}
Given a scalar product and basis $(e_i)_{i=1}^n$, we define
the \Def{metric tensor} $g_{ij} = \<e_i,e_j\>$, which are the
coordinates of the 2-form $\<\cdot,\cdot\>$, that is
\begin{equation}
\<\cdot,\cdot\> = \sum_{ij} g_{ij}~ \bar e_i \otimes \bar e_j ~.
\end{equation}
\end{myDefinition}

This also implies that
\begin{align}
  \<v,w\> &= \sum_{ij} v_i w_j \<e_i, e_j\> = \sum_{ij} v_i w_j g_{ij}
  = v^\T G w ~.
\end{align}
Although related, do not confuse $g_{ij}$ with the
usual definition of a metric $d(\cdot,\cdot)$ in a metric space.

\subsubsection{Properties of orthonormal bases}

If we have an orthonormal basis $(e_i)_{i=1}^n$, many thing simplify a
lot. Throughout this subsection, we assume  $\{e_i\}$ orthonormal.

\begin{itemize}
\item
The metric tensor $g_{ij} = \<e_i,e_j\> = \d_{ij}$ is the identity
matrix.\footnote{Being picky, a metric is not a matrix but a twice
covariant tensor (a row of rows). That's why it is correctly called
metric tensor.} Such a metric is also called \Def{Euclidean}. The
norm $\norm{\e_i} = 1$. The canonical dual basis $(\bar e_i)_{i=1}^n$ and
the one defined via the scalar product $(e^*_i)_{i=1}^n$ become identical,
$\bar e_i = e^*_i = \<e_i,\cdot\>$. Consequently, $v$ and $v^*$ have
the same coordinates $v_i = (v^*)_i$ w.r.t.\ $(e_i)_{i=1}^n$ and
$(\bar e_i)_{i=1}^n$, respectively.

\item The coordinates of vectors can now easily been computed:
\begin{align}
v = \sum_i v_i e_i \quad\To\quad
\<e_i,v\> = \<e_i, \sum_j v_j e_j\> = \sum_j \<e_i,e_j\> v_j = v_i
\end{align}

\item The coordinates of a linear transform can equally easily been computed:
Given a linear transform $f: V\to U$ an \emph{arbitrary} (e.g.\
non-orthonormal) input basis $(v_i)_{i=1}^n$ of $V$, but an orthonormal
basis $(u_i)_{i=1}^n$, then
\begin{align}
f &= \sum_{ij} f_{ij} u_i \otimes \bar v_j
\quad\To\\
\<u_i, f v_j\>
&= \<u_j, \sum_{kl} f_{kl} u_k \otimes \bar v_l(v_j)\>
 = \sum_{kl} f_{kl}~ \<u_j,u_k\>~ \bar v_l(v_j) \feed
&= \sum_{kl} f_{kl} \d_{jk} \d_{lj} = f_{ij}
\end{align}

\item The projection onto a basis vector is given by $e_i \<e_i,\cdot\>$.

\item The projection onto the span of several basis vectors $(e_1,..,e_k)$
is given by $\sum_{i=1}^k e_i \<e_i,\cdot\>$.

\item The identity mapping $\Id:V \to V$ is given by $\Id = \sum_{i=1}^{\dim(V)} e_i \<e_i,\cdot\>$.

\item The scalar product with an orthonormal basis is
\begin{equation}
\<v,w\> = \sum_{ij} v_i w_j \d_{ij} = \sum_i v_i w_i
\end{equation}
which, using matrix convention, can also be written as
\begin{equation}
\<v,w\>
 = (v_1~ v_2~ ..~ v_n)~
 \mat{c}{w_1 \\ w_2 \\ \vdots \\ w_n}
 = v^\T w = \sum_i v_i w_i ~,
\end{equation}
where for the first time we introduced the \Def{transpose} which, in
the matrix convention, swaps columns to rows and rows to columns.
\end{itemize}

As a general note, a row vector ``eats a vector and outputs a
scalar''. That is $v^\T:~ V \to \RRR$ should be thought of as a
1-form! Due to the matrix conventions, it generally is the case that
``rows eat columns'', that is, every row index should always be
thought of as relating to a 1-form (dual vector), and every column
index as relating to a vector. That is totally consistent to our
definition of coordinates.

For an orthonormal basis we also have
\begin{equation}
 v^*(w) = \<v,w\> = v^\T w ~.
\end{equation}
That is, $v^\T$ is the coordinate representation of the 1-form
$v^*$. (Which also says, that the coordinates of the 1-form $v^*$ in the
special basis $(e_i^*)_{i=1}^n \subset V^*$ coincide with the coordinates of the vector $v$.)

\subsection{The Structure of Transforms \& Singular Value Decomposition}

We focus here on linear transforms (or ``linear maps'') $f: V \to U$
from one vector space to another (or the same). It turns out that
such transforms have a very specific and intuitive structure, which
is captured by the singular value decompositon.

\subsubsection{The Singular Value Decomposition Theorem}

We state the following theorem:

\begin{myTheorem}[Singular Value Decomposition]
Given two vector spaces $\VV$ and $\UU$ with scalar products,
$\dim(\VV)=n$ and $\dim(\UU)=m$, for every linear transform
$f: \VV \to \UU$ there exist a $k\le n,m$ and 
orthonormal vectors $\{v_i\}_{i=1}^k \subset \VV$, orthonormal vectors
$\{u_i\}_{i=1}^k\subset \UU$, and positive scalars $\s_i>0, i=1,..,k$,
such that
\begin{equation}
 f = \sum_{i=1}^k \s_i u_i v_i^* 
\end{equation}
As above, $v_i^*=\<v_i, \cdot\>$ is the basis 1-form that picks the
$i$th coordinate of a vector in the basis $(v_i)_{i=1}^k \subset \VV$.\footnote{Note that $\{v_i\}_{i=1}^k$ may not be a full basis of $\VV$ if $k<n$. But
because $\{v_i\}$ is orthonormal, $\<v_i, \cdot\>$ uniquely picks the
$i$th coordinate no matter how $\{v_i\}_{i=1}^k$ is completed with
further $n-k$ vectors to become a full basis.}
\end{myTheorem}

We first restate this theorem equivalently in coordinates.
\begin{myTheorem}[Singular Value Decomposition]
For every matrix $A\in\RRR^{m \times n}$ there exists a 
$k\le n,m$ and orthonormal vectors $\{v_i\}_{i=1}^k \subset \RRR^n$, orthonormal vectors
$\{u_i\}\subset \RRR^m$, and positive scalars $\s_i>0, i=1,..,k$, such that
\begin{equation}
 A = \sum_{i=1}^k \s_i u_i v_i^\T = U S V^\T
\end{equation}
where $V = (v_1,..,v_k) \in\RRR^{n\times k} \comma U =
(u_1,..,u_k) \in \RRR^{m\times k}$ contain the orthonormal bases
vectors as columns and $S=\diag(\s_1,..,\s_k)$.
\end{myTheorem}

\begin{figure}
\show{SVD}
\caption{A linear transformation $f = \sum_{i=1}^k \s_i u_i v_i^\T$ can
be described as: take the input $x$, project it onto the first input
fundamental vector $v_1$ to yield a scalar, stretch/squeeze it by $\s_1$,
and ``unproject'' this into the first output fundamental vector $u_1$; repeat
this for all $i=1,..,k$, and add up the results.}
\end{figure}

Let me rephrase this in a sentence: \emph{Every matrix $A$ can be expressed as a linear combination of only $k$ rank-1 matrices. Rank-1 matrices are the most minimalistic kinds of matrices and they are always of the form $u v^\T$ for some $u$ and $v$. The rank-1 matrix $u v^\T$ takes an input $x$, projects it on $v$ (measures its alignment with $v$), and ``unprojects'' into $u$ (multiplies $v^\T x$ to the output vector $u$).}

Just to explicitly show the transition from coordinate-free to the
coordinate-based theorem, consider arbitrary orthonormal bases
$\{e_i\}_{i=1}^n \subset \VV$ and $\{\hat
e_i\}_{i=1}^m \subset\UU$. For $x\in\VV$ we have
\begin{align}
f(x)
&= \sum_{i=1}^k \s_i u_i \<v_i,x\>
 = \sum_{i=1}^k \s_i (\sum_l u_{li} \hat e_l) \<\sum_j v_{ji} e_j, \sum_k x_k e_k\> \\
&= \sum_{i=1}^k \s_i (\sum_l u_{li} \hat e_l) \sum_{jk} v_{ji} x_k \d_{jk}
 = \sum_l \[ \sum_{i=1}^k u_{li} \s_i  \sum_j v_{ji} x_j \] \hat e_l \\
&= \sum_l \[ U S V^\T x \]_l \hat e_l
\end{align}
where $v_{ji}$ are the coordinates of $v_i$, $u_{li}$ the coordinates
of $u_i$, $U = (u_1,..,u_k)$ is the matrix containing $\{u_i\}$
as columns, $V = (v_1,..,v_k)$ the matrix containing $\{v_i\}$ as
columns, and $S = \diag(\s_1,..,\s_k)$ the diagonal matrix with
elements $\s_i$.

We add some definitions based on this:
\begin{myDefinition}
The \Def{rank} $\rank(f)=\rank(A)$ of a transform $f$ or its matrix $A$ is the unique minimal $k$.
\end{myDefinition}

\begin{myDefinition}
The \Def{determinant} of a transform $f$ or its matrix $A$ is
\begin{equation}
\det(f) = \det(A) = \det(S) = \begin{cases}
\pm \prod_{i=1}^n \s_i & \text{for}~\rank(f)=n=m \\
0 & \text{otherwise}
\end{cases}~,
\end{equation}
where $\pm$ depends on whether the transform is a
reflection or not.
\end{myDefinition}

The last definition is a bit flaky, as the $\pm$ is not properly
defined. If, alternatively, in the above theorems we would require $V$
and $U$ to be rotations, that is, elements of $SO(n)$ (of
the \emph{special} orthogonal group); then negative $\s$'s would
indicate such a reflection and $\det(A) = \prod_{i=1}^n \s_i$. But
above we required $\s$'s to be strictly positive and $V$ and $U$ only
orthogonal. Fundamental space vectors $v_i$ and $u_i$ could flip
sign. The $\pm$ above indicates how many flip sign.

\begin{myDefinition}
a) The \Def{row space} (also called right or input fundamental
space) of a transform $f$ is
$\Span\{v_i\}_{i=1}^{\rank(f)}$. The \Def{input null space} (or
right null space) $\VV_\perp$ is the subspace orthogonal to the row
space, such that $v\in \VV_\perp \To f(v)=0$.

b) The \Def{column space} or (also called left or output
fundamental space) of a transform $f$ is
$\Span\{u_i\}_{i=1}^{\rank(f)}$. The \Def{output null space} (or
left null space) $\UU_\perp$  the subspace orthogonal to the
column space, such that $u\in \UU_\perp \To \<f(\cdot),u\>=0$.
\end{myDefinition}



\subsection{Point of departure from the coordinate-free notation}

The coordinate-free introduction of vectors and transforms helps a
lot to understand what these fundamentally are. Namely, that coordinate
vectors and matrices are 'just' coordinates and rely on a choice of
basis; what a metric $g_{ij}$ really is; that only for a Euclidean
metric the inner product satisfies $\<v,w\> = v^\T w$. Further, the
coordinate-free view is essential to understand that vector
coordinates behave differently to 1-form coordinates (e.g., ``gradients''!)
under a transformation of the basis. We discuss
contra- versus covariance of gradients at the end of this chapter.

However, we now understood that columns correspond to vectors, rows to
1-forms, and in the Euclidean case the 1-form $\<v,\cdot\>$ directly
corresponds to $v^\T$, in the non-Euclidean to $v^\T G$. In 
applications we typically represent things from start in orthonormal
bases (including perhaps non-Euclidean metrics), there is not much
gain sticking to the coordinate-free notation in most cases. Only when
the matrix notation gets confusing (and this happens, e.g.\ when
trying to compute something like the ``Jacobian of a Jacobian'', or
applying the chain and product rule for a matrix expression $\del_x
f(x)^\T A(x) b(x)$) it is always a save harbour to remind what we
actually talk about.

Therefore, in the rest of the notes we rely on the normal
coordinate-based view. Only in some explanations we
remind at the coordinate-free view when helpful.

\subsection{Filling SVD with life}

In the following we list some statements---all of them relate to the
SVD theorem and together they're meant to give a more intuitive
understanding of the equation $A = \sum_{i=1}^k \s_i u_i v_i^\T = U S
V^\T$.

\subsubsection{Understand \protect$v v^\T$ as a projection}
\begin{itemize}
\item The \Def{projection} of a vector $x\in\VV$ onto a vector $v\in\VV$, is
given by
\begin{equation}
x_\parallel = \frac{1}{v^2}
v \<v,x\> \quad\text{or}\quad \frac{v v^\T}{v^2} x ~.
\end{equation}
Here, the $\frac{1}{v^2}$ is normalizing in case $v$ does not have
length $|v|=1$.

\item The projection-on-$v$-matrix $v v^\T$ is symmetric, semi-pos-def, and has $\rank(v v^\T)=1$.

\item The projection of a vector $x \in \VV$ onto a subvector space
$\Span\{v_i\}_{i=1}^k$ for \emph{orthonormal} $\{v_i\}_{i=1}^k$ is
given by
\begin{equation}
x_\parallel = \sum_i v_i v_i^\T x = V V^T x
\end{equation}
where
$V=(v_1,..,v_k)\in\RRR^{n\times k}$. The projection matrix $V V^\T$
for orthonormal $V$ is symmetric, semi-pos-def, and has $\rank(V V^\T)
= k$.

\item The expression $\sum_i v_i v_i^\T$ is quite related to an SVD. Conversely it shows that the SVD represent every matrix as a liner combination of kind-of-projects, but these kind-of-projects $u v^\T$ first project onto $v$, but then unproject along $u$.
\end{itemize}

\subsubsection{SVD for symmetric matrices}

\begin{itemize}
\item Thm 2 $\To$ Every symmetric matrix $A$ is of the form
\begin{equation}
A = \sum_i \l_i v_i v_i^\T = V \L V^\T
\end{equation}
for orthonormal
$V=(v_1,..,v_k)$. Here $\l_i = \pm \s_i$ and $\L=\diag(\l)$ is the diagonal
matrix of $\l$'s. This describes nothing but a stretching/squeezing
along orthogonal projections.

\item The $\l_i$ and $v_i$ are also the eigenvalues and eigenvectors
of $A$, that is, for all $i=1,..,k$:
\begin{equation}
A v_i = \l_i v_i ~.
\end{equation}
If $A$ has full rank, then the SVD $A = V S
V^\T = V S V^\1$ is therefore also the eigendecomposition of $A$.

\item The pseudo-inverse of a symmetric matrix is
\begin{equation}
A^\dagger = \sum_i \l_i^\1 v_i v_i^\T = V S^\1 V^\T
\end{equation}
which simply
does the reverse stretching/squeezing along the same orthogonal
projections. Note that
\begin{equation}
A A^\dagger = A^\dagger A = V V^\T
\end{equation}
is the projection on
$\{v_i\}_{i=1}^k$. For full $\rank(A)=n$ we have $V V^\T=\Id$ and
$A^\dagger = A^\1$. For $\rank(A)<n$, we have that $A^\dagger y$
minimizes $\min_x \norm{A x - y}^2$, but there are infinitly many
$x$'s that minimize this, spanned by the null space of $A$. $A^\dagger
y$ is the minimizer closest to zero (with smallest norm).

\item Consider a data set $D = \{x_i\}_{i=1}^m$, $x_i\in\RRR^n$. For simplicity assume
it has zero mean, $\sum_{i=1}^m x_i = 0$. The \Def{covariance
matrix} is defined as
\begin{equation}
C = \frac{1}{n}~ \sum_i x_i x_i^\T = \frac{1}{n}~ X^\T X
\end{equation}
where (consistent to ML lecture convention) the data matrix $X$
containes $x_i^\T$ in the $i$th row. Each $x_i x_i^\T$ is a
projection. $C$ is symmetric and semi-pos-dev. Using SVD we can write
\begin{equation}
C = \sum_i \l_i v_i v_i^\T
\end{equation}
and $\l_i$ is the data variance along the eigenvector $v_i$;
$\sqrt{\l_i}$ the standard deviation along $v_i$; and $\sqrt{\l_i} 
v_i$ the principle axis vectors that make the ellipsoid we typically
illustrate convariances with.
\end{itemize}

\subsubsection{SVD for general matrices}

\begin{itemize}
\item For full $\rank(A)=n$, the determinant of a matrix is $\det(A)
= \pm \prod_i \s_i$. We may define the \Def{volume} spanned by any
$\{b_i\}_{i=1}^n$ as
\begin{equation}
\vol(\{b_i\}_{i=1}^n) = \det(B) \comma
B=(b_1,..,b_n)\in\RRR^{n\times n} ~.
\end{equation}
It follows that
\begin{equation}
\vol(\{A b_i\}_{i=1}^n) = \det(A) \det(B)
\end{equation}
that is, the volume is
being multiplied with $\det(A)$, which is consistent with our
intuition of transforms as stretchings/squeezings along
orthonormal projections.

\item The pseudo-inverse of a general matrix is
\begin{equation}
A^\dagger = \sum_i \s_i^\1 v_i u_i^\T = V S^\1 U^\T ~.
\end{equation}

If $k=n$ (full input rank), $\rank(A^\T A) = n$ and
\begin{equation}
(A^\T A)^\1 A^\T = (V S U^\T U S V^\T)^\1 V S U^\T
 = V^{-\T} S^{-2} V^\1 V S U^\T
 = V S^\1 U^\T = A^\dagger
\end{equation}
and $A^\dagger$ is also called left pseudoinverse because $A^\dagger A
 = \Id_n$.

If $k=m$ (full output rank), $\rank(A A^\T) = m$ and
\begin{equation}
A^\T (A A^\T)^\1 = V S U^\T (U S V^\T V S U^\T )^\1 
 = V S U^\T U^{-\T} S^{-2} U^\1
 = V S^\1 U^\T = A^\dagger
\end{equation}
and $A^\dagger$ is also called right pseudoinverse because $A A^\dagger = \Id_m$.

\item Assume $m=n$ (same input/output dimension, or $\VV=\UU$), but
 $k<n$. Then there exist orthogonal $V, U \in \RRR^{n\times n}$ such that
\begin{equation}
A = U D V^\T \comma D = \diag(\s_1,..,\s_k,0,..,0) = \mat{cc}{S & 0
\\ 0 & 0} ~. 
\end{equation}
Here, $V$ and $U$ contain a full orthonormal basis instead of only $k$
orthonormal vectors. But the diagonal matrix $D$ projects all but $k$
of those to zero. Every square matrix $A\in\RRR^{n\times n}$ can be
written like this.
\end{itemize}

\begin{myDefinition}[Rotation]
Given a scalar-product $\<\cdot , \cdot\>$ on V, a linear transform $f:
V\to V$ is called \emph{rotation} iff it preserves the scalar product,
that is,
\begin{align}
\forall v,w\in V:~ \<f(v), f(w)\> = \<v,w\> ~.
\end{align}
\end{myDefinition}

\begin{itemize}
\item Every rotation matrix is orthogonal, i.e., composed
of columns of orthonormal vectors.

\item Every rotation has rank $n$ and $\s_{1,..,n}=1$. (No
stretching/squeezing.)

\item Every square matrix can be written as\\
\cen{rotation${}_U$ $\cdot$ scaling${}_D$ $\cdot$ rotation${}_{V^\1}$}
\end{itemize}


\subsection{Eigendecomposition}

\begin{myDefinition}
The \Def{eigendecomposition} or \Def{diagonalization} of a
square matrix $A\in\RRR^{n\times n}$ is (if it exists!)
\begin{equation}
A=Q \L Q^\1
\end{equation}
where $\L=\diag(\l_1,..,\l_n)$ is a diagonal matrix of
eigenvalues. Each column $q_i$ of $Q$ is an \Def{eigenvector}.
\end{myDefinition}


\begin{itemize}
\item First note that, unlike SVD, this is not a Theorem but just a definition: If such a decomposition exists, it is called eigendecomposition. But it exists for almost any square matrix.
  
\item The set of eigenvalues is the set of roots of the
  \Def{characteristic polynomial} $p_A(\l) = \det(\l I - A)$. Why?
  Because then $A-\l I$ has 'volume' zero (or rank $<n$), showing that
  there exists a vector that is mapped to zero, that is $0=(A-\l
  I)x=Ax-\l x$.

\item Is every square matrix diagonalizable? No, but only an
$n^2-1$-dimensional subset of matrices are not diagonalizable; most
matrices are. A matrix is \emph{not} diagonalizable if an eigenvalue
$\l$ has multiplicity $k$ (more precisely, $\l$ is a root of $p_A(\l)$
with multiplicity $k$), but $n-\rank(A-\l I)$ (the dimensionality of
the span of the eigenvectors of $\l$!) is less than $k$. Therefore,
the eigenvectors of $\l$ are not linearly independent; they do not
span the necessary $k$ dimensions.

So, only very ``special'' matrices are not diagonalizable. Random
matrices are (with prob 1).

\item Symmetric matrices? $\to$ SVD

\item Rotations? Not real. But complex! Think of oscillating
projection onto eigenvector. If $\phi$ is the rotation angle, $e^{\pm
i\phi}$ are eigenvalues.
\end{itemize}

\subsubsection{Power Method}

To find the largest eigenvector of $A$, initialize $x$ randomly and iterate
\begin{equation}
x \gets A x\comma 
x \gets \frac{1}{\norm{x}}~ x
\end{equation}
\begin{items}
\item If this converges, $x$ must be an eigenvector and $\l=x^\T A x$
the eigenvalue.
\item If $A$ is diagonalizable, and $x$ is initially a non-zero linear
combination of all eigen vectors, then it is obvious that $x$ will
converge to the ``largest'' (in \emph{absolute} terms $|\l_i|$)
eigenvector (=eigenvector with largest eigenvalue). Actually, if the
largest (by norm) eigenvector is negative, then it doesn't really
converge but flip sign at every iteration.
\end{items}

\subsubsection{Power Method including the smallest eigenvalue}

A trick, hard to find in the literature, to also compute the smallest
eigenvalue and -vector is the following. We assume all eigenvalues to
be positive. Initialize $x$ and $y$ randomly,
iterate
\begin{equation}
 x \gets A x \comma \l\gets\norm{x} \comma x\gets x/\l \comma y\gets(\l I - A)
 y \comma y\gets y/\norm{y}
\end{equation}
Then $y$ will converge to the smallest eigenvector, and $\l-\norm{y}$
will be its eigenvalue. Note that (in the limit) $A-\l I$
only has negative eigenvalues, therefore $\norm{y}$ should be
positive. Finding smallest eigenvalues is a common problem in model fitting.


\subsubsection{Why should I care about Eigenvalues and Eigenvectors?}

\begin{items}
\item Least squares problems (finding smallest eigenvalue/-vector);
(e.g.\ Camera Calibration, Bundle Adjustment, Plane Fitting)
\item PCA
\item stationary distributiuons of Markov Processes
\item Page Rank
\item Spectral Clustering
\item Spectral Learning (e.g., as approach to training HMMs)
\end{items}

\subsection{Beyond this script: Numerics to compute these things}

We will not go into details of numerics. Nathan's script gives a
really nice explaination of the QR-method. I just mention two things:

(i) The most important forms of matrices for numerics are diagonal
matrices, orthogonal matrices, and upper triangular matrices. One
reason is that all three types can very easily be inverted. A lot of
numerics is about finding \textbf{decompositions} of general matrices
into products of these special-form matrices, e.g.:
\begin{items}
\item QR-decomposition: $A=Q R$ with $Q$ orthogonal and $R$ upper
triangular.
\item LU-decomposition: $A=L U$ with $U$ and $L^\T$ upper triangular.
\item Cholesky decomposition: (symmetric) $A=C^\T C$ with $C$ upper triangular
\item Eigen- \& singular value decompositions
\end{items}
Often, these decompositions are intermediate steps to compute eigenvalue or
singular value decompositions.

(ii) Use linear algebra packages. At the origin of all is LAPACK;
browse through \url{http://www.netlib.org/lapack/lug/} to get an
impression of what really has been one of the most important
algorithms in all technical areas of the last half century. Modern
wrappers are: Matlab (Octave), which originated as just a console
interface to LAPACK; the C++-library Eigen; or the Python NumPy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Derivatives as 1-forms, steepest descent, and the covariant gradient}

\subsubsection{The coordinate-free view: A
derivative takes a change-of-input vector as input, and returns a
change of output}

We previously defined the Jacobian of a function $f: \RRR^n \to \RRR^d$ from vector to vector space. We repeat defining a derivative more generally in
coordinate-free form:
\begin{myDefinition}
Given a function $f: V \to G$ we define the differential 
\begin{equation}
df\big|_x :~ V \to G,~ v \mapsto \lim_{h\to 0} \frac{f(x+hv) -
f(x)}{h}
\end{equation}
\end{myDefinition}
This definition holds whenever $G$ is a continuous
space that allows the definition of this limit and the limit exists
($f$ is differentiable). The notation $df|_x$ reads ``the differential
at location $x$'', i.e., evaluating this derivative at location $x$.

Note that $df|_x$ is a mapping from a ``tangent vector'' $v$ (a
change-of-input vector) to an output-change. Further, by this
definition $df|_x$ is linear. $df|_x(v)$ is the directional derivative we mentioned before. \textbf{Therefore $df|_x$ is a $G$-valued
1-form.} As discussed earlier, we can introduce coordinates for
1-forms; these coordinates are what typically is called the
``gradient'' or ``Jacobian''. But here we explicitly see that we speak
of coordinates of a 1-form.


\subsubsection{Contra- and co-variance}


In Section \ref{coordinate} we summarized the effects of a coorindate transformation. We recap the same here again also for derivatives and scalar products.

We have a vector space $V$, and a function $f: V \to \RRR$. We'll be interested in the change of function value $df|_x (\d)$ for change of input $\d\in V$, as well as the value of the scalar product $\<\d, \d\>$. All these quantities are defined without any reference to coordinates; we'll check now how their coordinate representations change with a change of basis.

As in Section \ref{coordinate} we have two bases $\AA=(a_1,..,a_n)$ and $\BB=(b_1,..,b_n)$, and the transformation $T$ that maps each $a_i$ to $b_i$, i.e., $\BB = T \AA$. Given a vector $\d$ we denote its coordinates in $\AA$  by $[\d]^A$, and its coordinates in $\BB$ by $[\d]^B$. Let $T^{AA} = B$ be the matrix representation of $T$ in the old $A$ coordinates ($B$ contains the new basis vectors $b$ as columns).
\begin{itemize}
\item We previously learned that
 \begin{equation}
[\d]^A = B [\d]^B
\end{equation}
  that is, the matrix $B$ carries new coordinates to old ones. These coordinates are said to be contra-variant: they transform `against' the transformation of the basis vectors.
\item We require that
 \begin{equation}
df|_x (\d) = [\del_x f]^A [\d]^A = [\del_x f]^B [\d]^B
\end{equation}
  must be invariant, i.e., the change of function value for $\d$ should not depend on whether we compute it using $A$ or $B$ coordinates. It follows
  \begin{align}
  &  [\del_x f]^A [\d]^A = [\del_x f]^A B [\d]^B = [\del_x f]^B [\d]^B \\
  &  [\del_x f]^A B = [\del_x f]^B
  \end{align}
  that is, the matrix $B$ carries old 1-form-coordinates to new 1-form-coordinates. Therefore, such 1-form-coordinates are called co-variant: they transform `with' the transformation of basis vectors.
\item What we just wrote for the derivative $df|_x (\d)$ we could equally write and argue for any 1-form $v^*\in V^*$; we always require that the value $v^*(\d)$ is invariant.
\item We also require that the scalar product $\<\d,\d\>$ is invariant. Let
\begin{equation}
\<\d,\d\> = [\d]^A {}^\T [G]^A [\d]^A = [\d]^B {}^\T [G]^B [\d]^B
\end{equation}
  where $[G]^A$ and $[G]^B$ are the 2-form-coordinates (metric tensor) in the old and new basis. It follows
\begin{align}
 & [\d]^A {}^\T [G]^A [\d]^A = [\d]^B {}^\T B^\T [G]^A B [\d]^B \\
 & [G]^B = B^\T [G]^A B
\end{align}
that is, the matrix carries the old 2-form-coordinates to new ones. These coordinates are called twice co-variant.
\end{itemize}    
  
Consider the following example: We have the function
$f: \RRR^2 \to \RRR$, $f(x) = x_1 + x_2$. The function's partial
derivative is of course $\frac{\del f}{\del x} = (1 ~~ 1)$. Now let's
transform the coordinates of the space: we introduce new coordinates
$(z_1, z_2) = (2x_1, x_2)$ or $z = B^\1 x$ with $B=\mat{cc}{\half & 0 \\ 0 &
1}$. The same function, written in the new coordinates, is $f(z)
= \half z_1 + z_2$. The partial derivatives of that same function,
written in these new coordinates, is $\frac{\del f}{\del z} = (\half
~~ 1)$.

%% Considering a metric tensor $\<x,x\> = x^\T A x$, if we
%% require that $\<z,z\> = \<x,x\> = x^\T A_x x$ it follows that the
%% metric tensor in $z$-coordinates is $A_z = B^\mT A_x B^\1$. We summarize:

%% \begin{itemize}
%% \item Let $B$ be a matrix that describes a linear transformation in
%%   coordinates

%% \item A coordinate vector $x$ transforms as $z = B x$

%% \item The gradient vector $\na_x f(x)$ transforms as $\na_z f(z) =
%%   B^\mT \na_x f(x)$

%% \item The metric $A_x$ transforms as $A_z = B^\mT A_x B^\1$

%% \item The steepest descent direction $A_x^\1
%%   \na_x f(x)$ transforms as $A_z^\1 \na_z f(z) = B A_x^\1 \na_x f(x)$,
%%   like a normal coordinate vector.
%% \end{itemize}

Generally, consider we have two kinds of mathematical objects and when we
multiply them together this gives us a scalar. The scalar shouldn't
depend on any choice of coordinate system and is therefore invariant
against coordinate transforms. Then, if one of the objects transforms
in a \Def{covariant} (``transforming \emph{with} the transformation'')
manner, the other object must transform in a \Def{contra-variant}
(``transforming
\emph{contrary} to the transformation'') manner to ensure
that the resulting scalar is invariant. This is a general principle:
whenever two things multiply together to give an invariant thing, one
should transform co- the other contra-variant.

Let's also check Wikipedia:
\begin{items}
\item ``For a vector to be basis-independent, the components [=coordinates] of the
  vector must contra-vary with a change of basis to compensate. That
  is, the matrix that transforms the vector of components must be the
  inverse of the matrix that transforms the basis vectors. The
  components of vectors (as opposed to those of dual vectors) are said
  to be contravariant.
\item For a dual vector (also called a covector) to be
  basis-independent, the components of the dual vector must co-vary
  with a change of basis to remain representing the same
  covector. That is, the components must be transformed by the same
  matrix as the change of basis matrix. The components of dual vectors
  (as opposed to those of vectors) are said to be covariant.''
\end{items}

%% So far so good. This gives a clear explaination for why vector
%% coordinates are called contra- and 1-form coordinates co-variant.

%% Some people though, like me, use the
%% words \emph{co-/contra-variant} also as adjective for a vector or
%% 1-form itself, not their coordinates. That must be confusing. One
%% origin\footnote{A second origin is to use an entirely different
%% convention to define co-variance without reference to coordinates at
%% all: Assume vectors (including, e.g., basis vectors)
%% transform \emph{with} a linear transformation $v \mapsto f
%% v$. Postulate additionally that scalars must be invariant. Then it
%% follows that 1-forms must transform contra-variantly, $g \in
%% V^* \mapsto g \circ f^\1 \in V^*$, such that $g(x) \in \RRR \mapsto
%% g \circ f^\1 (f x) = g(x)$ invariantly. If we now go down to coordinates, if $w^\T$ are
%% the coordinates of the 1-form $g$, and $A$ the coordinates of the
%% transform $f$, then $w \mapsto A^\mT w$ such that $w^\T x \mapsto
%% (A^\mT w)^\T A x = w^\T x$ invariantly.} is in the physicist's
%% convention to use the word co-variance almost synomymously to
%% ``invariant''---and in the above we definitely want the ``vector
%% [itself] to be basis-independent'', so invariant under change of
%% basis. Such invariant vectors are sometimes called co-variant vectors;
%% especially in the context of the gradient vector: \textbf{co-variant
%% gradient} (``basis-independent gradient''). That's confusing because
%% the co-variant gradient (e.g.\ $A^\1 \na f$) has actually
%% contra-variant coordinates, just as (invariant) vectors.

%% \textbf{Warning.} I think by now the concept of co- and contra-variance
%% is clear. There is a slight caveat in the precise use of the words,
%% though. Strictly speaking, vectors are covariant, and therefore,
%% necessarily, coordinate vectors contra-variant. So, when speaking
%% about coordinate representations of things (coordinate-vectors instead
%% of vectors, the partial derivatives instead of the differential
%% 1-form, the metric tensor instead of the metric), co- and
%% contra-variance would actually be swapped. But this leads to a whole
%% lot of confusion. So instead, by convention, when we say that the
%% ``vector $x$ is covariant'' we always think of this as a statement of
%% the abstract vector, independent of whether $x$ happens to be its
%% coordinate-representation; same for gradients, if we say that ``the
%% gradient $\del f(x)/\del x$ is contra-variant'' we think of this as a
%% statement about the abstract 1-form, not the coordinate
%% representation.

\textbf{Ordinary gradient} descent of the form $x \gets x + \a \na f$
adds objects of different types: contra-variant coordinates $x$ with co-variant partial derivatives $\na f$. Clearly, adding two such different
types leads to an object who's transformation under coordinate
transforms is strange---and indeed the ordinary gradient descent is
not invariant under transformations.





%% The gradient $\frac{\del f}{\del x}$ of a function can be defined as
%% an object (a so-called 1-form) that multiplies to a point variation
%% $\d x$ to give the function variation $\d f = \frac{\del f}{\del x} \d
%% x$. Therefore, if coordinate vectors are contra-variant, the gradient
%% must be covariant. Normal (contra-variant) coordinate vectors are
%% written as columns. They multiply with row vectors to give a scalar;
%% row vectors are covariant.\footnote{There is more confusion to add: A
%% vector itself (as a coordinate-free algebraic object) is a covariant
%% object and should be distinguished from its coordinate vector (given a
%% basis), which is a contra-variant object (it must be because
%% coordinate coefficients multiply to basis vectors). A coordinate-free
%% gradient itself (a 1-form) is a contra-variant object and should be
%% distinguished from its coordinate representation (given a basis),
%% which is a covariant object. A metric (or 2-form) is
%% ``double-contra-variant'' since it multiplies to two vectors to give a
%% scalar; its inverse is ``double-covariant''; and again, moving from
%% algebraic objects to coordinate representations this is flipped.}  It
%% therefore makes sense to write a gradient as a row vector. However,
%% note that row/column notations are just a convention anyway---and the
%% wast majority of researchers have the convention to write gradients
%% also as column vector.



\subsubsection{Steepest descent and the covariant gradient vector}

Let's define the steepest descent direction to be the one where, when you
make a step of length 1, you get the largest decrease of $f$ in its
linear (=1st order Taylor) approximation.

\begin{myDefinition}
Given $f:V \to \RRR$ and a norm $\norm{x}^2=\<x,x\>$ (or scalar product)
defined on $V$, we define the \Def{steepest descent} vector
$\d^*\in V$ as the vector:
\begin{align}
\d^* = \argmin_\d df\big|_x(\d) \st \norm{\d}^2=1
\end{align}
\end{myDefinition}
Note that for this definition we need to assume we have a scalar product, otherwise the length=1 constraint is not defined. Also recall that $df|_x(\d) = \del_x f(x) \d = \na f(x) ^\T \d$ are equivalent notations.

Clearly, if we have coordinates in which the norm is Euclidean then
\begin{equation}
\norm{\d}^2 = \d^\T \d \quad\To\quad \d^* \propto - \na f(x)
\end{equation}

However, if we have coordinates in which the metric is non-Euclidean,
we have:
\begin{myTheorem}[Steepest Descent Direction (Covariant gradient)]
For a general scalar product $\<v,w\> = v^\T G w$ (with metric tensor
$G$), the steepest descent direction is
\begin{align}
\d^* \propto - G^\1 \na f(x)
\end{align}
\end{myTheorem}
Proof: Let $G = B^\T B$ (Cholesky decomposition) and $z = B \d$
\begin{align}
\d^*
&= \argmin_\d \na f^\T \d \st \d^\T G \d=1 \\
&= B^\1 \argmin_z \na f^\T B^\1 z \st z^\T z = 1\\
&\propto B^\1 [- B^\mT \na f] = - G^\1 \na f
\end{align}

For a coordinate transformation $B$, recall that new metric becomes $\tilde G = B^\T G B$, and the new gradient $\widetilde{\na f} = B^\T \na f$. Therefore, the new steepest descent is
  \begin{align}
    \widetilde{\d^*}
    &= - [\tilde G]^\1 \widetilde{\na f}
    = - B^\1 G^\1 B^\mT B^\T \na f = -B^\1 G^\1 \na f
    \end{align}
  and therefore transformes like normal contra-variant coordinates of a vector.
  

There is an important special case of this, when $f$ is a function over the space of probability distributions and $G$ is the Fisher metric, which we'll discuss later.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Examples and Exercises}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{e01-bases.tex}
\input{e02-svd.tex}
\input{e03-eigen.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Optimization}

\subsection{Downhill algorithms for unconstrained optimization}

We discuss here algorithms that have one goal: walk downhill as
quickly as possible. These target at efficiently finding local
optima---in constrast to \emph{global optimization} methods, which try
to find the global optimum.

For such downhill walkers, there are two essential things to discuss:
the stepsize and the step direction. When discussing the stepsize
we'll hit on topics like backtracking line search, the Wolfe
conditions and its implications in a basic convergence
proof. The discussion of the step direction will very much circle around
Newton's method and thereby also cover topics like quasi-Newton
methods (BFGS), Gauss-Newton, covariant and conjugate gradients.

\subsubsection{Why you shouldn't trust the magnitude of the gradient}

Consider the following 1D function and naive gradient descent $x \gets
x - \a \na f$ for some fixed and small $\a$.

\show[.6]{gradientOpt}


\begin{itemize}
\item In plateaus we'd make small steps, at steep slopes (here close
to the optimum) we make huge steps, very likely overstepping the
optimum. In fact, for some $\a$ the algorithm might indefinitely loop
a non-sensical sequence of very slowly walking left on the plateau,
then accellerating, eventually overstepping the optimum, then being
thrown back far to the right again because of the huge negative
gradient on the left.

\item Generally, never trust an algorithm that depends on the
scaling---or choice of units---of a function! An optimization
algorithm should be invariant on whether you measure costs in cents or
Euros! Naive gradient descent $x \gets x - \a \na f$ is not!
\end{itemize}

As a conclusion, the gradient $\na f$ gives a reasonable descent
direction, but its magnitude is really arbitrary and no good
indication of a good stepsize. Therefore, it often makes sense to just
compute the step direction
\begin{equation}
\d = - \frac{1}{|\na f(x)|} \na f(x)
\end{equation}
and iterate $x \gets x + \a \step$ for some appropriate stepsize.


\subsubsection{Ensuring monotone and sufficient decrease: Backtracking
line search, Wolfe conditions, \& convergence}

The first idea is simple: If a step would increase the objective
value, reduce the stepsize. We typically use multiplicative stepsize
adaptations: Reduce $\a \gets \adec \a$ with $\adec\approx 0.5$; and
increase $\a \gets \ainc\a$ with $\ainc\approx 1.2$. A simple
monotone gradient descent algorithm reads as follows (the blue part is explained later).
\begin{algorithm}
\caption{\label{alg2} Plain gradient descent with backtracking line search}
\begin{algorithmic}[1]
\Require initial $x\in\RRR^n$, functions $f(x)$ and $\na f(x)$,
tolerance $\t$, parameters (defaults:
$\ainc=1.2, \adec=0.5, \stepmax=\infty, \lsstop=0.01$)
\State initialize stepsize $\a=1$
\Repeat
\State $\step \gets -\frac{\na f(x)}{|\na
f(x)|}$ \Comment{(alternative: $\step=-\na f(x)$)}
\While{$f(x+\a\step) > f(x) {\color{blue}+ \lsstop \na f(x)^\T (\a\step)}$} \Comment{\textbf{backtracking line search}}
\State $\a \gets \adec \a$ \Comment{decrease stepsize}
\EndWhile
\State $x \gets x + \a\step$
\State $\a \gets \min\{\ainc\a,\stepmax\}$ \Comment{increase stepsize}
\Until $|\a\step| <\t$ ~ \Comment{perhaps for 10 iterations in sequence}
\end{algorithmic}
\end{algorithm}
Here, the step vector $\step$ is always normalized and $\a$ is
adapted on the fly; decreasing when $f(x+\a\step)$ is not
sufficiently smaller than $f(x)$.

\begin{figure}
\show[.6]{pics-Optim/backtracking}
\caption{\label{figWolfe} The 1st Wolfe condition: $f(x) + \na f(x)^\T
(\a\step)$ is the tangent, which describes the expected decrease of
$f(x+\a\step)$ if $f$ \emph{was} linear. We cannot expect this to be
the case; so  $f(x+\a\step) > f(x) + \lsstop \na f(x)^\T (\a\step)$
weakens this condition.}
\end{figure}

This \emph{sufficiently smaller} is described by the blue part and is
called the (1st) \Def{Wolfe condition}
\begin{equation}
f(x+\a\step) > f(x) + \lsstop \na f(x)^\T (\a\step) ~.
\end{equation}
Figure \ref{figWolfe} illustrates this. Note that $\na f(x)^\T
(\a\step)$ is a negative value and describes how much the objective
would decrease \emph{if} $f$ was linear. But $f$ is of course not
linear; we cannot expect that a step would really decrease $f$ that
much. Instead we require that it decreases by a fraction of this
expectation. $\lsstop$ describes this fraction and is typically chosen
very moderate, e.g.\ $\lsstop\in[0.01,0.1]$. So, the Wolfe conditions
requires that $f$ descreases by the $\lsstop$-fraction of what it
would decrease if $f$ was linear. Note that for $\a\to 0$ the Wolfe
condition will always be fulfilled for smooth functions, because $f$ ``becomes locally
linear''.

You'll proove the following theorem in the exercises. It is fundamental  to \emph{convex optimization} and proves that Alg \ref{alg2} is efficient for convex objective functions:
\begin{myTheorem}[Exponential convergence on convex functions]
  Let $f:\RRR^\n \to \RRR$ be an objective function where the eigenvalues
  $\l$ of the Hessian $\he f(x)$ are for any $x$. Further, assume that Hessian eigenvalues $\l$ are lower bounded by $m>0$ and upper bounded by $M>m$, with $m,M\in\RRR$, at any location $x\in\RRR^n$. ($f$ is convex.) Then Algorithm \ref{alg2} converges exponentially with convergence rate $(1-2 \frac{m}{M}\lsstop\adec)$ to the optimum.
\end{myTheorem}
But even if your objective function is not globally convex, Alg \ref{alg2} is an efficient downhill walker, and once it reaches a convex region it will efficiently walk into its local minimum.

For completeness, there is a second Wolfe condition,
\begin{equation}
 |\na f (x + \a\step)^\T \step| \le b |\na f(x)^\T \step| ~,
\end{equation}
which states that the gradient magnitude should have decreased
sufficiently. We do not use it much.

\subsubsection{The Newton direction}

We already discussed the \emph{steepest descent direction} $- G^\1 \na
f(x)$ if $G$ is a metric tensor. Let's keep this in mind!

The original Newton method is a method to find the \Def{root} (that
 is, zero point) of a function $f(x)$. In 1D it iterates $x \gets x
 - \frac{f(x)}{f'(x)}$, that is, it uses the gradient $f'$ to estimate
 where the function might cross the $x$-axis. To find an minimum or
 maximum of $f$ we want to find the root of its gradient. For
 $x\in\RRR^n$ the Newton method iterates
\begin{equation}
x \gets x - \he f(x)^\1 \na f(x) ~.
\end{equation}
Note that the Newton step $\step = - \he f(x)^\1 \na f(x)$ is the solution to
\begin{equation}
\min_\d \[f(x) + \na f(x)^\T\d + \half \d^\T \he
f(x) \d\] ~.
\end{equation}
So the Newton method can also be viewed as 1) compute the 2nd-order
Taylor approximation to $f$ at $x$, and 2) jump to the optimum of this
approximation.

Note:
\begin{itemize}
\item If $f$ is just a 2nd-order polynomial, the Newton method will
jump to the optimum in just one step.

\item \emph{Unlike the gradient magnitude $|\na f(x)|$}, the magnitude of the
Newton step $\d$ is very meaningful. It is scale invariant! If you'd
rescale $f$ (trade cents by Euros), $\d$ is unchanged. $|\d|$ is the
distance to the optimum of the 2nd-order Taylor.

\item \emph{Unlike the gradient $\na f(x)$}, the Newton step $\d$ is truely a
vector! The vector itself is invariant under coordinate
transformations; the coordinates of $\d$ transforms contra-variant,
as it is supposed to for vector coordinates.

\item \textbf{The hessian as metric, and the Newton step as steepest descent:} Assume that the hessian $H = \he
f(x)$ is pos-def. Then it fulfils all necessary conditions to define a
scalar product $\<v ,w\> = \sum_{ij} v_i w_j H_{ij}$, where $H$ plays
the role of the metric tensor. If $H$ was the space's metric, then the
steepest descent direction is $- H^\1 \na f(x)$, which is the Newton
direction!

Another way to understand the same: In the 2nd-order Taylor
approximation $f(x+\d) \approx f(x) + \na f(x)^\T\d + \half \d^\T H
\d$ the Hessian plays the role of a metric tensor. Or: we may think
of the function $f$ as being an isometric parabola $f(x+\d) \propto
\<\d,\d\>$, but we've chosen coordinates where $\<v,v\> = v^\T H v$ and
the parabola seems squeezed.

Note that this discussion only holds for pos-dev hessian.
\end{itemize}

A robust Newton method is the core of many solvers, see Algorithm
\ref{algNewton}. We do backtracking line search along the Newton
direction, but with maximal step size $\a=1$ (the full Newton step).

\begin{algorithm}
\caption{\label{algNewton} Newton method}
\begin{algorithmic}[1]
\Require initial $x\in\RRR^n$, functions $f(x), \na f(x), \he f(x)$,
tolerance $\t$, parameters (defaults:
$\ainc=1.2, \adec=0.5, \linc=\ldec=1, \lsstop=0.01$)
\State initialize stepsize $\a=1$
\Repeat
\State choose $\l>-(\text{minimal eigenvalue of $\he f(x)$})$
\State compute $\step$ to solve $(\he f(x) + \l \Id)~ \step = - \na
f(x)$ \label{alg0}
\While{$f(x+\a\step) > f(x) + \lsstop \na f(x)^\T (\a\step)$} \Comment{line search}
\State $\a \gets \adec\a$ \Comment{decrease stepsize}
\State optionally: $\l \gets \linc\l$ and recompute $d$ \Comment{increase damping}
\EndWhile
\State $x \gets x + \a\step$ \Comment{step is accepted}
\State $\a \gets \min\{\ainc\a,1\}$ \Comment{increase stepsize}
\State optionally: $\l \gets \ldec\l$ \Comment{decrease damping}
\Until $\norm{\a\step}_\infty < \t$
\end{algorithmic}
\end{algorithm}

We can additionally add and adapt damping to gain more robustness.
Some notes on the $\l$:
\begin{itemize}
\item In Alg \ref{algNewton}, the first line chooses $\l$ to ensure that $(\he f(x) + \l \Id)$
is indeed pos-dev---and a Newton step actually decreases $f$ instead
of seeking for a maximum. There would be other options:
instead of adding to all eigenvalues we could only set the negative
ones to some $\l>0$.

\item $\step$ solves the problem
\begin{equation}
\min_\step \[ \na f(x)^\T \step + \half \step^\T \he f(x) \step
+ \half \l \step^2)\] ~.
\end{equation}
So, we added a squared potential $\l \step^2$ to the local 2nd-order
Taylor approximation. This is like introducing a squared penalty for
large steps!

\item \Def{Trust region method:} Let's consider a different
mathematical program over the step:
\begin{equation}
\min_\d \na f(x)^\T \step + \half \step^\T \he f(x) \step \st \step^2 \le \b
\end{equation}
This problem wants to find the minimum of the 2nd-order Taylor (like
the Newton step), but constrained to a stepsize no larger than
$\b$. This $\b$ defines the \emph{trust region:} The region in which
we trust the 2nd-order Taylor to be a reasonable enough approximation.

Let's solve this using Lagrange parameters (as we will learn it later): Let's assume the inequality
constraint is active. Then we have
\begin{align}
L(\step, \l)
&= \na f(x)^\T \step + \half \step^\T \he f(x) \step + \l (\step^2 - \b) \\
\na_\step L(\step, \l)
&= \na f(x)^\T + \step^\T (\he f(x) +  2\l\Id)
\end{align}
Setting this to zero gives the step $\step = -(\he f(x) +
 2\l\Id)^\1 \na f(x)$.

Therefore, the $\l$ can be viewed as \Def{dual variable} of a trust
region method. There is no analytic relation between $\b$ and $\l$; we
can't determine $\l$ directly from $\b$. We could use a constrained
optimization method, like primal-dual Newton, or Augmented Lagrangian
approach to solve for $\l$ and $\step$. Alternatively, we can always
increase $\l$ when the computed steps are too large, and decrease if
they are smaller than $\b$---the Augmented Lagrangian update equations
could be used to do such an update of $\l$.

\item The $\l\step^2$ term can be interpreted as penalizing
the \emph{velocity} (stepsizes) of the algorithm. This is in analogy
to a damping, such as ``honey pot damping'', in physical dynamic
systems. The parameter $\l$ is therefore also called \Def{damping}
parameter. Such a damped (or regularized) least-squares method is
also called \Def{Levenberg-Marquardt} method.

\item For $\l\to\infty$ the step direction $\step$ becomes aligned
with the plain gradient direction $-\na f(x)$. This shows that for
$\l\to\infty$ the Hessian (and metric deformation of the space)
becomes less important and instead we'll walk orthogonal to the
iso-lines of the function.

\item The $\l$ term makes the $\step$ non-scale-invariant! $\step$ is
not anymore a proper vector!
\end{itemize}

\begin{algorithm}[p]
\caption{\label{algNewton2} Newton method -- practical version realized in \texttt{rai}}
\begin{algorithmic}[1]
  \Require initial $x\in\RRR^n$, functions $f(x), \na f(x), \he f(x)$
  \Require stopping tolerances $\t_x$, $\t_f$
  \Require parameters $\ainc=1.2, \adec=0.5, \l_0=0.01, \linc=\ldec=1, \lsstop=0.01$
  \Require maximal stepsize $\d_\text{max} \in \RRR$, optionally lower/upper bounds $\underline x,\overline x\in\RRR^n$
  \Statex
  \State initialize stepsize $\a=1$, $\l = \l_0$
  \Repeat
  \State compute smallest eigenvalue $\s_\text{min}$ of $\he f(x) + \l \Id$ \label{line0}
  \If{$\s_\text{min} > 0$ is sufficiently posititive}
  \State compute $\step$ to solve $(\he f(x) + \l \Id)~ \step = - \na f(x)$
  \Else
  \State Option 1: $\l \gets 2\l - \s_\text{min}$ and goto line \ref{line0} \Comment{increase regularization}
  \State Option 2: $\step \gets - \d_\text{max}~ \na f(x) / |\na f(x)|$ \Comment{gradient step of length $\d_\text{max}$}
  \EndIf
  \State if $\norm{\d}_\infty > \d_\text{max}$:~ $\step \gets (\d_\text{max}/\norm{\d}_\infty)~ \step$ \Comment{cap $\step$ length}
  \State $y \gets \textsc{BoundClip}(x+\a\step,\underline x,\overline x)$
  \While{$f(y) > f(x) + \lsstop \na f(x)^\T (y-x)$} \Comment{bound-projected line search}
  \State $\a \gets \adec\a$ \Comment{decrease stepsize}
  \State (unusual option: $\l \gets \linc\l$ and goto line \ref{line0}) \Comment{increase damping}
  \State $y \gets \textsc{BoundClip}(x+\a\step,\underline x,\overline x)$
  \EndWhile
  \State $x_\text{old} \gets x$
  \State $x \gets y$ \Comment{step is accepted}
  \State $\a \gets \min\{\ainc\a,1\}$ \Comment{increase stepsize}
  \State (unusual option: $\l \gets \ldec\l$) \Comment{decrease damping}
  \Until $\norm{x_\text{old}-x}_\infty < \t_x$ repeatedly, or $f(x_\text{old})-f(x) < \t_f$ repeatedly
\Statex
\Procedure {BoundClip}{$x,\underline x,\overline x$}
\State $\forall_i:~$ $x_i \gets \min\{x_i,\overline x_i\}$\comma $x_i \gets \max\{x_i, \underline x_i\}$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\subsubsection{Gauss-Newton: a super important special case}

A special case that appears really a lot in intelligent systems is the
Gauss-Newton case: Consider an objective function of the form
\begin{align}
f(x) = \phi(x)^\T \phi(x) = \sum_i \phi_i(x)^2
\end{align}
where we call $\phi(x)$ the \textbf{cost features}. This is also
called a \Def{sum-of-squares} problem, here a sum of cost feature
squares. We have
\begin{align}
\na f(x)
 &= 2 \Del{x}\phi(x)^\T \phi(x) \\
\he f(x)
 &= 2 \Del{x}\phi(x)^\T \Del{x}\phi(x) +
 2 \phi(x)^\T \Hes{x}\phi(x)
\end{align}

The Gauss-Newton method is the Newton method for         
$f(x) = \phi(x)^\T \phi(x)$ while approximating $\he \phi(x)\approx
0$. That is, it computes approximate Newton steps 
\begin{equation}
\step = - (\Del{x}\phi(x)^\T \Del{x}\phi(x) + \l \Id)^\1 \Del{x}\phi(x)^\T \phi(x) ~.
\end{equation}
Note:
\begin{itemize}
\item The approximate Hessian $2\Del{x}\phi(x)^\T \Del{x}\phi(x)$
is \textbf{always semi-pos-def!} Therefore, no problems arise with
negative hessian eigenvalues.
\item The approximate Hessian only requires the first-order
derivatives of the cost features. There is no need for
computationally expensive hessians of $\phi$.
\item The objective $f(x)$ can be interpreted as just the Euclidean
norm $f(\phi) = \phi^\T \phi$ but pulled back into the $x$-space. More
precisely: Consider a mapping $\phi:~ \RRR^n \to \RRR^m$ and a general
scalar product $\<\cdot,\cdot\>_\phi$ in the output space. In differential geometry
there is the notion of a pull-back of a metric, that is, we define a
scalar product $\<\cdot,\cdot\>_x$ in the input space as
\begin{align}
\<x,y\>_x = \<d\phi(x),d\phi(y)\>_\phi
\end{align}
where $d\phi$ is the differential of $\phi$ (a $\RRR^m$-valued
1-form). Assuming $\phi$-coordinates such that the metric tensor of
$\<\cdot,\cdot\>_\phi$ is Euclidean, we have
\begin{align}
\<x,x\>_x = \<d\phi(x),d\phi(x)\>_\phi = \Del{x}\phi(x)^\T \Del{x}\phi(x)
\end{align}
and therefore, \Def{the approximate Hessian is the
pullback of a Euclidean cost feature metric}, and $\<x,x\>_x$
approximates the 2-order polynomial term of $f(x)$, with the
non-constant (i.e., Riemannian) pull-back metric $\<\cdot,\cdot\>_x$.
\end{itemize}

\subsubsection{Quasi-Newton \& BFGS: approximating the hessian from gradient observations}

To apply full Newton methods we need to be able to compute $f(x), \na
f(x)$, and $\he f(x)$ for any $x$. However, sometimes, computing $\he f(x)$ is not
possible, e.g., because we can't derive an analytic expression for
$\he f(x)$, or it would be to expensive to compute the hessian
exactly, or even to store it in memory---especially in very
high-dimensional spaces. In such cases it makes sense to approximate
$\he f(x)$ or $\he f(x)^\1$ with a low-rank approximation.

Assume we have computed $\na f(x_1)$ and $\na f(x_2)$ at two different
points $x_1,x_2\in\RRR^n$. We define
\begin{equation}
y=\na f(x_2) - \na f(x_1) \comma \d=x_2-x_1 ~.
\end{equation}
From this we may wish to find some approximate Hessian matrix $H$ or  $H^\1$ that
fulfils
\begin{equation}
H~ \d \overset{!}= y \qquad\text{or}\qquad  \d \overset{!}= H^\1 y
\end{equation}
The first equation is called \emph{secant equation}. Here are guesses of  $H$ and  $H^\1$:
\begin{equation}
H = \frac{y y^\T}{y^\T \d} \qquad\text{or}\qquad H^\1 = {\color{blue}\frac{\d \d^\T}{\d^\T y}}
\end{equation}
Convince yourself that these choices fulfil the respective desired relation
above. However, these choices are under-determined. There exist many
alternative $H$ or $H^\1$ that would be consistent with the observed
change in gradient. However, given our understanding of the structure
of matrices it is clear that these choices are the lowest rank
solutions, namely rank 1.

\paragraph{Broyden-Fletcher-Goldfarb-Shanno (BFGS):}
An optimization algorithm computes $\na f(x_k)$ at a series of points
$x_{0:K}$. We incrementally update our guess of $H^\1$ with an update
equation
\begin{equation}
H^\1 \gets {\color{red}\(\Id-\frac{y \d^\T}{\d^\T y}\)^\T H^\1
\(\Id-\frac{y \d^\T}{\d^\T y}\)} + {\color{blue}\frac{\d \d^\T}{\d^\T
    y}} ~,
\end{equation}
which is equivalent to (using the Sherman-Morrison formula)
\begin{equation}
H \gets H - \frac{H \d \d^\T H^\T}{\d^T H \d} + \frac{y y^\T}{y^\T \d}
\end{equation}
Note:
\begin{itemize}
\item If $H^\1$ is initially zero, this update will assign
$H^\1 \gets \frac{\d \d^\T}{\d^\T y}$, which is the minimal rank 1
update we discussed above.

\item If $H^\1$ is previously non-zero, the red part ``deletes certain
  dimensions'' from $H^\1$. More precisely, note that ${\color{red}\(\Id-\frac{y \d^\T}{\d^\T y}\)} y = 0$, that is, this construction deletes $\Span\{y\}$ from its input space. Therefore, the red part gives zero when multiplied with $y$; and it is guaranteed that the resulting $H^\1$ fulfils $H^\1 y = \d$.
\end{itemize}

The \Def{BFGS} algorithms uses this $H^\1$ instead of a precise $\he
f(x)^\1$ to compute the steps in a Newton method. All we said about
line search and Levenberg-Marquardt damping is unchanged.
\footnote{Taken from Peter Blomgren's lecture slides: \url{terminus.sdsu.edu/SDSU/Math693a_f2013/Lectures/18/lecture.pdf}
This is the original Davidon-Fletcher-Powell (DFP) method suggested by
W.C. Davidon in 1959.
The original paper describing this revolutionary idea -- the first
quasi-Newton method -- was not accepted for publication. It later
appeared in 1991 in the first issue the the SIAM Journal on
Optimization.}

In very high-dimensional spaces we do not want to store $H^\1$
densely. Instead we use a compressed storage for low-rank matrices,
e.g., storing vectors $\{v_i\}$ such that $H^\1 = \sum_i v_i
v_i^\T$. \Def{Limited memory BFGS (L-BFGS)} makes this more memory
efficient: it limits the rank of the $H^\1$ and thereby the used memory.
I do not know the details myself, but I assume that with every update
it might  aim to delete the lowest eigenvalue to keep the rank constant.

%% Some thoughts: There are alternative ways to estimate $H^\1$ from the
%% data $\{(x_i, f(x_i), \na f(x_i))\}_{i=1}^k$, e.g.\ using Gaussian
%% Process regression with derivative observations. Not only the
%% derivatives but also the value $f(x_i)$ should give information on
%% $H(x)$ for non-quadratic functions. This is related to model-based
%% optimization (see below)---but I am not sure whether existing methods
%% have exploited gradient observations in this context. Would be
%% interesting to investigate.

\subsubsection{Conjugate Gradient}

The Conjugate Gradient Method is a method for solving large
linear eqn.\ systems $Ax+b=0$. We only mention its extension for
optimizing nonlinear functions $f(x)$.

As above, assume that we evaluted $\na f(x_1)$ and $\na f(x_2)$ at two different
points $x_1,x_2\in\RRR^n$. But now we make one more assumption: The
point $x_2$ is the minimum of a line search from $x_1$ along the
direction $\d_1$. This latter assumption is quite optimistic: it
assumes we did perfect line search. But it gives great information:
The iso-lines of $f(x)$ at $x_2$ are tangential to $\d_1$.

In this setting, convince yourself of the following: Ideally each search direction
should be orthogonal to the previous one---but not orthogonal in the
conventional Euclidean sense, but orthogonal w.r.t.\ the Hessian
$H$. Two vectors $\d$ and $\d'$ are called \Def{conjugate} w.r.t.\
a metric $H$ iff $d'^\T H d = 0$. Therefore, subsequent search
directions to be conjugate to each other.

Conjugate gradient descent does the following:
\begin{algorithm}
\caption{Conjugate gradient descent}
\begin{algorithmic}[1]
\Require initial $x\in\RRR^n$, functions $f(x), \na f(x)$, tolerance $\t$
\Ensure $x$
\State initialize descent direction $d = g = -\na f(x)$
\Repeat
\State $\a \gets \argmin_\a f(x+\a d)$ \Comment{line search}
\State $x \gets x + \a d$
\State $g' \gets g$,~ $g = -\na f(x)$ \Comment{store and compute grad}
\State $\b \gets \max\left\{\frac{g^\T(g - g')}{g'^\T g'},0\right\}$
\State $d \gets g + \b d$ \Comment{conjugate descent direction}
\Until $|\Delta x| <\t$
\end{algorithmic}
\end{algorithm}

\begin{itemize}
\item The equation for $\b$ is by Polak-Ribi{\`e}re: On a quadratic
function $f(x) = x^\T H x$ this leads to \Def{conjugate} search
   directions, $d'^\T H d = 0$.

\item Intuitively, $\b>0$ implies that the new descent direction always adds a bit of the old
   direction. This essentially provides 2nd order information.

\item For arbitrary quadratic functions CG converges in $n$ iterations. But
 this really only works with \Def{perfect line search}.
\end{itemize}

%% \cen{\twocol{.5}{.5}{
%% \show{conjugateGradient}
%% }{
%% \show{conjugateGradient2}
%% }}

\subsubsection{Rprop*}

\begin{algorithm}[t]\caption{Rprop}\label{algRprop}
\begin{algorithmic}[1]
\Require initial $x\in\RRR^n$, function $f(x), \na f(x)$, initial stepsize $\a$, tolerance $\t$
\Ensure $x$
\State initialize $x=x_0$, all $\a_i=\a$, all $g_i=0$
\Repeat
\State $g \gets \na f(x)$
\State $x' \gets x$
\For{$i=1:n$}
\If{$g_i g_i' > 0$} \Comment{same direction as last time}
\State $\a_i \gets 1.2 \a_i$
\State $x_i \gets x_i - \a_i~ \sign(g_i)$
\State $g_i' \gets g_i$
\ElsIf{$g_i g_i' <0$} \Comment{change of direction}
\State $\a_i \gets 0.5 \a_i$
\State $x_i \gets x_i - \a_i~ \sign(g_i)$
\State $g_i' \gets 0$ \Comment{force last case next time}
\Else
\State $x_i \gets x_i - \a_i~ \sign(g_i)$
\State $g_i' \gets g_i$
\EndIf
\State optionally: cap $\a_i \in [\a_{\text{min}}~ x_i, \a_{\text{max}}~ x_i]$
\EndFor
\Until $|x'-x| <\t$ for 10 iterations in sequence
\end{algorithmic}
\end{algorithm}

Read through Algorithm \ref{algRprop}. Notes on this:
\begin{itemize}
\item Stepsize adaptation is done in each coordinate \emph{separately}!
\item The algorithm not only ignores $|\na f|$ but also its exact
direction! Only the gradient signs in each coordinate are
relevant. Therefore, the step directions may differ up to $<90^\circ$ from $-\na f$.
\item It often works surprisingly efficient and robust.
\item If you like, have a look at:

{\small

Christian Igel, Marc Toussaint, W. Weishui (2005): Rprop using the
natural gradient compared to Levenberg-Marquardt optimization. In
Trends and Applications in Constructive Approximation. International
Series of Numerical Mathematics, volume 151, 259-272.

}
\end{itemize}


\subsection{The general optimization problem -- a mathematical program}

\begin{myDefinition}
Let $x\in\RRR^n,~ f:~ \RRR^n \to \RRR,~ g:~ \RRR^n \to \RRR^m,~
h: \RRR^n\to\RRR^l$. An optimization problem, or \emph{mathematical
program}, is
\begin{align*}
\min_x\quad & f(x)\\
\st & g(x)\le 0\comma h(x)=0
\end{align*}
\end{myDefinition}
We typically at least assume $f,g,h$ to be differentiable or smooth.

Get an intuition about this problem formulation by considering the
following examples. Always discuss where is the optimum, and at the
optimum, how the objective $f$ \emph{pulls} at the point, while the
constraints $g$ or $h$ \emph{push} against it.

For the following examples, draw the situation and guess, without much maths, where the optimum is:
\begin{itemize}
\item A 1D example: $x\in\RRR$, $h(x) = \sin(x)$, $g(x) = x^2/4-1$,
$f$ some non-linear function.

\item 2D example: $f(x,y) = x$ (intuition: the objective is constantly
pulling to the left), $h(x,y)=0$ is some non-linear path in the plane
$\to$ the optimum is at the left-most tangent-point of
$h$. Tangent-point means that the tangent of $h$ is vertical. $h$
pushes to the right (always orthogonal to the zero-line of $h$).

\item 2D example: $f(x,y) = x$, $g(x,y) = y^2-x-1$. The zero-line of
$g$ is a parabola towards the right. The objective $f$ pulls into this
parabola; the optimum is in the 'bottom' of the parabola, at $(-1,0)$.

\item 2D example: $f(x,y) = x$, $g(x,y) = x^2 + y^2-1$. The zero-line of
$g$ is a circle. The objective $f$ pulls to the left; the optimum is
at the left tangent-point of the circle, at $(-1,0)$.

\item Figure \ref{figKKTillu}
\end{itemize}

\begin{figure}
\show[.4]{pics-Optim/KKT}
\caption{\label{figKKTillu}
2D example: $f(x,y) = -x$, pulling constantly to the right;
three inequality constraints, two active, one inactive. The
``pull/push'' vectors fulfil the stationarity condition $\na f
+ \l_1 \na g_1 + \l_2 \na g_2 = 0$. }
\end{figure}

\subsection{The KKT conditions}

\begin{myTheorem}[Karush-Kuhn-Tucker conditions]
Given a mathematical program,
\begin{align*}
x \text{~optimal~} \quad\To\quad \exists \l\in\RRR^m, \k\in\RRR^l \st& \nonumber\\
\na f(x) + \sum_{i=1}^m \l_i \na g_i(x) + \sum_{j=1}^l \k_j \na h_j(x)&= 0 && \text{(stationarity)}\\
\forall_j:~ h_j(x)=0 \comma  \forall_i:~ g_i(x) &\le 0 && \text{(primal feasibility)}\\
\forall_i:~ \l_i &\ge 0  && \text{(dual feasibility)}\\
\forall_i:~ \l_i g_i(x) &= 0 && \text{(complementarity)}
\end{align*}
\end{myTheorem}
Note that these are, in general, only necessary conditions. Only in
special cases, e.g.\ convex, these are also sufficient.

These conditions should be intuitive in the previous examples:
\begin{itemize}
\item \emph{The first condition describes the ``force balance'' of the
  objective pulling and the active constraints pushing back.} The
  existance of dual parameters $\l,\k$ could implicitly be expressed
  by stating
\begin{equation}
\na f (x) \in \Span(\{ \na g_{1..m}, \na h_{1..l}\})
\end{equation}
The specific values of $\l$ and $\k$ tell us, how strongly the
  constraints push against the objective, e.g., $\l_i |\na g_i|$ is
  the force excerted by the $i$th inequality.

\item \emph{The fourth condition very elegantly describes the logic of
  inequality constraints being either active ($\l_i>0, g_i=0$) or
  inactive ($\l_i=0, g_i\le 0$).} Intuitively it says: An inequality
  can only push at the boundary, where $g_i=0$, but not inside the feasible
  region, where $g_i<0$.  The trick of using the equation $\l_i g_i=0$ to
  express this logic is beautiful, especially when later we 
  discuss a case which relaxes this strict logic to $\l_i g_i=-\mu$ for
  some small $\mu$---which roughly means that inequalities
  may push a little also inside the feasible region.

\item Special case $m=l=0$ (no constraints). The first condition is
  just the usual $\na f(x)=0$.

\item Discuss the previous examples as special cases; and how the
  force balance is met.
\end{itemize}


\subsection{Unconstrained problems to tackle a constrained problem}

Assume you'd know about basic unconstrained optimization methods (like
standard gradient descent or the Newton method) but nothing about
constrained optimization methods. How would you solve a constrained
problem? Well, I think you'd very quickly have the idea to introduce
extra cost terms for the violation of constraints---a million people have
had this idea and successfully applied it in practice.

In the following we define a new cost function $F(x)$, which includes
the objective $f(x)$ and some extra terms.
\begin{myDefinition}[Log barrier, squared penalty, Lagrangian, Augmented Lagrangian]
\begin{align*}
F_{sp}(x;\nu,\mu) &= f(x) + \nu \sum_j h_j(x)^2 + \mu \sum_i [g_i(x)>0]~ g_i(x)^2&& \text{(sqr.\ penalty)}\\
F_{lb}(x;\mu) &= f(x) - \mu \sum_i \log(-g_i(x)) && \text{(log barrier)}\\
L(x,\l,\k) &= f(x) + \sum_j \k_j h_j(x) + \sum_i \l_i g_i(x) && \text{(Lagrangian)} \\
\hat L(x) &= f(x) + \sum_j \k_j h_j(x) + \sum_i \l_i g_i(x) ~+ \\
&\qquad + \nu\sum_j h_j(x)^2 + \mu \sum_i [g_i(x)>0]~ g_i(x)^2 && \text{(Aug.\ Lag.)}\nonumber
\end{align*}
\end{myDefinition}

\begin{figure}
\show[.4]{pics-Optim/logBarrier}
\caption{\label{figLogBarriers}
The function $-\mu\log(-g)$ (with $g$ on the ``$x$-axis'')
for various $\mu$. This is always undefined (``$\infty$'') for
$g>0$. For $\mu\to 0$ this becomes the hard step function.}
\end{figure}

\begin{itemize}
\item The squared penalty method is straight-forward if we have an
algorithm to minimize $F(x)$. We initialize $\nu=\mu=1$, minimize
$F(x)$, then increase $\nu,\mu$ (multiply with a number $>1$) and
iterate. For $\nu,\mu\to\infty$ we retrieve the optimum.

\item The log barrier method (see Fig.~\ref{figLogBarriers}) does
exactly the same, except that we decrease $\mu$ towards zero (multiply
with a numer $<1$ in each iteration). Note that we need a
feasible initialization $x_0$, because otherwise the barriers are
ill-defined! The whole algorithm will keep the temporary solutions
always \emph{inside} the feasible regions (because the barriers push
away from the constraints). That's why it is also
called \Def{interior point method}.

\item The Lagrangian is a function $L(x,\l,\k)$ which has the gradient
\begin{equation}
\na L(x,\l,\k) = \na f(x) + \sum_i \l_i \na g_i(x) + \sum_j \k_j \na
h_j(x) ~.
\end{equation}
\emph{That is, $\na L(x,\l,\k)=0$ is our first KKT condition!} In that
sense, the additional terms in the Lagrangian \emph{generate the push
forces of the constraints}. If we knew the correct $\l$'s and $\k$'s
beforehand, then we could find the optimal $x$ by the
unconstrained problem $\min_x L(x,\l,\k)$ (if this has a unique solution).

\item The Augmented Lagrangian $\hat L$ is a function that includes
 both, squared penalties, and Lagrangian terms that push proportional
 to $\l,\k$. The Augmented Lagrangian method is an
 iterative algorithm that, while running, figures out how strongly we
 need to push to ensure that the final solution is \emph{exactly} on
 the constraints, where all squared penalties will anyway be zero. It
 does not need to increase $\nu,\mu$ and still converges to the correct
 solution.
\end{itemize}



\subsubsection{Augmented Lagrangian*}

This is not a main-stream algorithm, but I like
it. See \cite{14-toussaint-AugLag}.

In the Augmented Lagrangian $\hat L$, the solver has two types of
knobs to tune: the strenghts of the penalties $\nu,\mu$ and the
strengths of the Lagrangian forces $\l, \k$. The trick is conceptually
easy:
\begin{itemize}
\item Initially we set $\l, \k=0$ and $\nu,\mu = 1$ (or some other
constant). In the first iteration, the unconstrained solver will find $x'
 = \min_x \hat L(x)$; the objective $f$ will typically pull into the
 penalizations.
\item For the second iteration we then choose parameters $\l,\k$ that
 try to avoid that we will be pulled into penalizations the next
 time. Let's update
\begin{equation}
\k_j \gets \k_j + 2 \mu h_j(x')\comma \l_i \gets \max(\l_i + 2 \mu
g_i(x'), 0).
\end{equation}
Note that $2 \mu h_j(x')$ is the force (gradient) of
the equality penalty at $x'$; and $\max(\l_i + 2 \mu g_i(x'), 0)$ is
the force of the inequality constraint at $x'$. What this update
does is: it analyzes the forces excerted by the penalties, and
translates them to forces excerted by the Lagrange terms in the next
iteration. It tries to trade the penalizations for the Lagrange terms.

More rigorously, observe that, if $f,g,h$ are linear and the same
constraints are active in two consecutive iterations, then this update
will guarantee that all penalty terms are zero in the second
iteration, and therefore the solution fulfils the first KKT
condition \citep{14-toussaint-AugLag}. See also the respective exercise.
\end{itemize}

\subsection{The Lagrangian}

\subsubsection{How the Lagrangian relates to the KKT conditions}

The Lagrangian $L(x,\k,\l) = f + \k^\T h + \l^\T g$ has a number of
properties that relates it to the KKT conditions:
\begin{enumerate}
\item Requiring a zero-$x$-gradient, $\na_x L = 0$, implies
the \emph{1st KKT condition}.
\item Requiring a zero-$\k$-gradient, $0 = \na_\k L = h$, implies
primal feasibility (the \emph{2nd KKT condition}) w.r.t.\ the equality
constraints.
\item Requiring that $L$ is maximized w.r.t.\ $\l\ge 0$ is related to
the remaining 2nd and 4th KKT conditions:
\begin{align}\label{eqMaxL}
&\max_{\l\ge 0} L(x,\l)
 = \begin{cases}
   f(x) & \text{if }g(x)\le 0 \\
   \infty & \text{otherwise} \end{cases}\\
&\l=\argmax_{\l\ge 0} L(x,\l)
  \quad\To\quad
\begin{cases}
\l_i=0 & \text{if }g_i(x)<0 \\ 0=\na_{\l_i} L(x,\l) = g_i(x) & \text{otherwise}
\end{cases}
\end{align}
This implies either $(\l_i=0 \wedge g_i(x)<0)$ or $g_i(x)=0$,
which is equivalent to the \emph{complementarity}
and \emph{primal feasibility} for inequalities.
\end{enumerate}

These three facts show how tightly the Lagrangian is related to the
KKT conditions. To simplify the discussion let us assume only
inequality constraints from now on. Fact (i) tells us that if we
$\min_x L(x,\l)$, we reproduce the 1st KKT condition. Fact (iii) tells
us that if we $\max_{\l\ge 0} L(x,\l)$, we reproduce the remaining KKT
conditions. Therefore, the optimal primal-dual solution $(x^*,\l^*)$
can be characterized as a \Def{saddle point of the
Lagrangian}. Finding the saddle point can be written in two ways:
\begin{myDefinition}[primal and dual problem]
\begin{align*}
\min_x~ \max_{\l\ge 0}~ L(x,\l) && \Def{(primal problem)} \\
\max_{\l\ge 0}~ \underbrace{\min_x~
L(x,\l)}_{l(\l) \textbf{\anchor{0,0}{\quad(dual function)}}} && \Def{(dual problem)}
\end{align*}
\end{myDefinition}
Convince yourself, using \refeq{eqMaxL}, that the first expression is
indeed the original primal problem $\[\min_x f(x) \st g(x)\le 0\]$.

\paragraph{What can we learn from this?} The KKT conditions state that, at an optimum, \emph{there
exist some} $\l,\k$. This existance statement is not very helpful to
actually find them. In contrast, the Lagrangian tells us directly how
the dual parameters can be found: by maximizing w.r.t.\ them. This can
be exploited in several ways:

\subsubsection{Solving mathematical programs analytically, on paper.}

Consider the problem
\begin{equation}
\min_{x\in\RRR^2} x^2 \st x_1+x_2 = 1 ~.
\end{equation}
We can
find the solution analytically via the Lagrangian:
\begin{align}
L(x,\k)
 &= x^2 + \k(x_1+x_2-1)\\
0
 &= \na_x L(x,\k)
  = 2 x + \k\mat{c}{1\\1} \quad\To\quad x_1=x_2=-\k/2\\
0
 &= \na_\k L(x,\k)
  = x_1+x_2-1 = -\k/2-\k/2-1 \quad\To\quad \k=-1 \\
\To
 & x_1=x_2=1/2
\end{align}
Here we first formulated the Lagrangian. In this context, $\k$ is
often called \Def{Lagrange multiplier}, but I prefer the term
\emph{dual variable}. Then we find a saddle point of $L$ by requiring
$0 = \na_x L(x,\k)$, $0 = \na_\k L(x,\k)$. If we want to solve a
problem with an inequality constrained, we do the same calculus for
both cases: 1) the constraint is active (handled like an equality
constrained), and 2) the constrained is inactive. Then we check if the
inactive case solution is feasible, or the active case is
dual-feasible ($\l\ge 0$). Note that if we have $m$ inequality
constraints we have to analytically evaluate every combination of
constraints being active/inactive---which are $2^m$ cases. This
already hints at the fact that a real difficulty in solving
mathematical programs is to find out which inequality constraints are
active or inactive. In fact, if we knew this a priori, everything
would reduce to an equality constrained problem, which is much easier
to solve.

\subsubsection{Solving the dual problem, instead of the primal.}

In some cases the dual function $l(\l) = \min_x L(x,\l)$ can
analytically be derived. In this case it makes very much sense to try
solving the dual problem instead of the primal. First, the dual
problem $\max_{\l\ge0} l(\l)$ is guaranteed to be convex even if the
primal is non-convex. (The dual function $l(\l)$ is concave, and the
constraint $\l\ge 0$ convex.) But note that $l(\l)$ is itself defined
as the result of a generally non-convex optimization problem $\min_x
L(x,\l)$. Second, the inequality constraints of the dual problem are
very simple: just $\l\ge 0$. Such inequality constraints are called
\Def{bound constraints} and can be handled with specialized methods.

However, in general $\min_x \max_y f(x,y) \not= \max_y \min_x
f(x,y)$. For example, in discrete domain $x,y\in\{1,2\}$, let
$f(1,1)=1, f(1,2)=3, f(2,1)=4, f(2,2)=2$, and $\min_x f(x,y) = (1, 2)$
and $\max_y f(x,y) = (3, 4)$. Therefore, the dual problem is in
general not equivalent to the primal.

The dual function is, for $\l\ge0$, a lower bound
\begin{equation}
l(\l)= \min_x L(x,\l) ~\le~ \[\min_x f(x) \st g(x)\le 0\] ~.
\end{equation}
And consequently
\begin{equation}
\text{(dual)}\qquad\qquad\max_{\l\ge 0} \min_x L(x,\l) ~\le~ \min_x \max_{\l\ge 0} L(x,\l)\qquad\qquad\text{(primal)}
\end{equation}

We say \Def{strong duality} holds iff
\begin{equation}
\max_{\l\ge 0} \min_x L(x,\l)  =  \min_x \max_{\l\ge 0} L(x,\l)
\end{equation}

If the primal is convex, and there exist an interior point
\begin{equation}
\exists_x:~ \forall_i:~ g_i(x) < 0
\end{equation}
(which is called \Def{Slater
  condition}), then we have \Def{strong duality}.


\subsubsection{Finding the ``saddle point'' directly with a
  primal-dual Newton method.}

In basic unconstrained optimization an efficient way to find an
optimum (minimum or maximum) is to find a point where the gradient is
zero with a Newton method. At saddle points all gradients are also
zero. So, to find a saddle point of the Lagrangian we can equally use
a Newton method that seeks for roots of the gradient. Note that such a
Newton method optimizes in the joint \Def{primal-dual space} of
$(x,\l,\k)$.

In the case of inequalities, the zero-gradients view is over-simplified: While facts (i) and (ii) characterize a saddle point
in terms of zero gradients, fact (iii) makes this more precise to
handle the inequality case. For this reason it is actually easier to
describe the primal-dual Newton method directly in terms of the KKT
conditions: We seek a point $(x,\l,\k)$, with $\l\ge0$, that solves the equation system
\begin{align}
  \na_x f(x) + \l^\T \del_x g + \k^\T \del_x h &= 0 \\
  h(x) &= 0 \\
  \diag(\l) g(x) + \m \one_m &=0
\end{align}
Note that the first equation is the 1st KKT, the 2nd is the 2nd KKT w.r.t.\ equalities, and the third is
the \emph{approximate} 4th KKT with log barrier parameter $\m$ (see below). These
three equations reflect the saddle point properties (facts (i), (ii),
and (iii) above). We define
\begin{align}
r(x,\l,\k)
&= \mat{c}{
  \na f(x) + \l^\T \del g(x) + \k^\T \del h(x) \\
  h(x) \\
  \diag(\l)~ g(x) + \m \one_m}
\end{align}
and use the Newton method
\begin{equation}
\mat{c}{x\\\l\\\k}
 \gets \mat{c}{x\\\l\\\k} - \a~ \del r(x,\l,\k)^\1~ r(x,\l)
\end{equation}
to find the root $r(x,\l,\k)=0$ ($\a$ is the stepsize). We have
\begin{align}
\del r(x,\l,\k)
 &= \mat{ccc}{
  \he f(x) + \Sum_i \l_i \he g_i(x) + \Sum_j \k_j \he h_j(x) & \del g(x)^\T & \del h(x)^\T \\
  \del h(x) & 0 & 0 \\
  \diag(\l)~ \del g(x) & \diag(g(x)) & 0 
}
\end{align}
where $\del r(x,\l,\k) \in \RRR^{(n+m+l)\times(n+m+l)}$.
Note that this method uses the hessians $\he f, \he g$ and $\he h$.

The above formulation allows for a duality gap $\m$. One could choose
$\m=0$, but often that is not robust. The beauty is that we can adapt
$\mu$ on the fly, before each Newton step, so that we do not need a
separate outer loop to adapt $\m$.

Before computing a Newton step, we compute the current duality measure
$\tilde \mu = -\frac{1}{m}~ \sum_{i=1}^m \l_i g_i(x)$. Then we set
$\mu = \half \tilde \mu$ to half of this. In this way, the Newton step
will compute a direction that aims to half the current duality gap. In
practise, this leads to good convercence in a single-loop Newton
method. (See also Boyd sec 11.7.3.)

The dual feasibility $\l_i \ge 0$ needs to be handled explicitly by
the root finder -- the line search can simply clip steps to stay
within the bound constraints.

Typically, the method is called ``\emph{interior} primal-dual
Newton'', in which case also the primal feasibility $g_i \le 0$ has to
be ensured. But I found there are tweaks to make the method also handle
infeasible $x$, including infeasible initializations.


\subsubsection{Log Barriers and the Lagrangian}

Finally, let's revisit the log barrier method. In principle it is very simple: For a given $\mu$, we use an unconstrained solver to find the minimum $x^*(\mu)$ of
\begin{equation}
F(x;\mu) = f(x) - \mu \sum_i \log(-g_i(x)) ~.
\end{equation}
(This process is also called ``centering''.) We then gradually decrease $\mu$ to zero, always calling the inner loop to recenter. The generated path of $x^*(\mu)$ is called central path.

The method is simple and has very insightful relations to the KKT conditions and the dual problem. For given $\mu$, the optimality condition is
\begin{align}
\na F(x;\mu) = 0
 \quad&\To\quad \na f(x) - \sum_i \frac{\mu}{g_i(x)} \na g_i(x) = 0 \\
&\oTo\quad \na f(x) + \sum_i \l_i \na g_i(x) = 0 \comma \l_i g_i(x) = -\mu
\end{align}
where we defined(!) $\l_i = -\mu/g_i(x)$, which guarantees $\l_i \ge 0$ as long as we are in the interior ($g_i\le 0$).

\emph{So, $\na F(x;\mu) = 0$ is equivalent to the \Def{modified
    (=approximate) KKT conditions}}, where the complemenetarity is
relaxed: inequalities may push also inside the feasible region. For
$\mu\to 0$ we converge to the exact KKT conditions with strict
complementarity.

So $\mu$ has the interpretation of a relaxation of complementarity. We
can derive another interpretation of $\mu$ in terms of suboptimality
or the duality gap:

Let $x^*(\mu) = \min_x F(x;\mu)$ be the central path. At each $x^*$
we may define, as above, $\l_i = -\mu/g_i(x^*)$. We note that $\l\ge0$
(dual feasible), as well that $x^*(\mu)$ minimizes the Lagrangian
$L(x,\l)$ w.r.t.\ $x$! This is because,
\begin{align}
0
&= \na F(x,\mu) 
 = \na f(x) + \sum_{i=1}^m \l_i \na g_i(x)
 = \na L(x,\l) ~.
\end{align}
Therefore, $x^*$ is actually the solution to $\min_x L(x,\l)$, which
defines the dual function. We have
\begin{align}
l(\l)
&= \min_x L(x,\l)
 = f(x^*) + \sum_{i=1}^m \l_i g_i(x^*)
 = f(x^*) - m \mu ~.
\end{align}
($m$ is simply the count of inequalities.)
That is, $m \mu$ is the duality gap between the (suboptimal)  $f(x^*)$ and $l(\l)$. Further, given that the
dual function is a lower bound, $l(\l) \le p^*$, where $p^*=\min_x f(x)
\st g(x)\le 0$ is the optimal primal value, we have
\begin{equation}
f(x^*) - p^* \le m \mu ~.
\end{equation}
This gives the interpretation of $\mu$ as an upper bound on the suboptimality of $f(x^*)$.








\subsection{Convex Problems}

We do not put much emphasis on discussing convex problems in this
lecture. The algorithms we discussed so far equally apply on general
non-linear programs as well as on convex problems---of course, only on
convex problems we have convergence guarantees, as we
can see from the convergence rate analysis of Wolfe steps based on the
assumption of positive upper and lower bounds on the Hessian's
eigenvalues.

Nevertheless, we at least define standard LPs, QPs, etc. Perhaps the
most interesting part is the discussion of the Simplex algorithm---not
because the algorithms is nice or particularly efficient, but rather
because one gains a lot of insights in what actually makes
(inequality) constrained problems hard.

\subsubsection{Convex sets, functions, problems}

\begin{myDefinition}[Convex set, convex function]
A set $X\subseteq V$ (a subset of some vector space $V$) is \Def{convex} iff
\begin{align}
\forall x,y\in X, a\in[0,1]:~ ax + (1\!-\!a)y \in X
\end{align}
A function is defined
{\small \begin{align}
\hspace*{-10mm}\Def{convex} &\iff \forall x,y\in\RRR^n, a\in[0,1]:~ f(ax + (1\!-\!a)y) \le a~ f(x) + (1\!-\!a)~ f(y) \\
\hspace*{-10mm}\Def{quasiconvex} &\iff \forall x,y\in\RRR^n, a\in[0,1]:~ f(ax + (1\!-\!a)y) \le \max\{f(x), f(y)\}
\end{align}}
\end{myDefinition}
Note: quasiconvex $\iff$ for any $\a\in\RRR$ the sublevel set $\{ x |
f(x)\le \a\}$ is convex. Further, I call a function \Def{unimodal}
if it has only one local minimum, which is the global minimum.

\begin{myDefinition}[Convex program] ~

\emph{Variant 1:} A mathematical program $\min_x~ f(x) \st g(x)\le 0,~ h(x) =
0$ is convex if $f$ is convex and the feasible set is convex.

\emph{Variant 2:} A mathematical program $\min_x~ f(x) \st g(x)\le 0,~ h(x) =
0$ is convex if $f$ and every $g_i$ are convex and $h$ is linear.
\end{myDefinition}
Variant 2 is the stronger and usual definition. Concerning variant 1,
if the feasible set is convex the zero-level sets of all $g$'s need to
be convex and the zero-level sets of $h$'s needs to be linear. Above
these zero levels the $g$'s and $h$'s could in principle be
abribtrarily non-linear, but these non-linearities are irrelevant for
the mathematical program itself. We could replace such $g$'s and $h$'s
by convex and linear functions and get the same problem.

\subsubsection{Linear and quadratic programs}
\begin{myDefinition}[Linear program (LP), Quadratic program (QP)]
Special case mathematical programs are
\begin{align*}
\hspace*{-10mm}\Def{Linear Program (LP):}\quad & \min_x~ c^\T x \st G x \le h,~ Ax=b \\
\hspace*{-10mm}\Def{LP in standard form:}\quad & \min_x~ c^\T x \st x \ge 0,~ Ax=b \\
\hspace*{-10mm}\Def{Quadratic Program (QP):}\quad & \min_x~ \half x^\T Q x + c^\T x   \st   G x \le h,~ Ax=b~, Q \text{pos-def}
\end{align*}
Rarely, also a \Def{Quadratically Constrained QP (QCQP)} is considered.
\end{myDefinition}

An important example for LP are \Def{relaxations} of integer linear
programs,
 \begin{equation}
\min_x c^\T x \st Ax=b,~ x_i \in\{0,1\} ~,
\end{equation}
which includes Travelling Salesman, MaxSAT or MAP inference problems. Relaxing such
a problem means to instead solve the continuous LP
\begin{equation}
\min_x c^\T x \st Ax=b,~ x_i\in[0,1] ~.
\end{equation}
If one is lucky and the
continuous LP problem converges to a fully integer solution, where all
$x_i \in \{0,1\}$, this is also the solution to the integer
problem. Typically, the solution of the continuous LP will be
partially integer (some values converge to the extreme
$x_i \in \{0,1\}$, while others are inbetween $x_i \in (0,1)$). This
continuous valued solution gives a lower bound on the integer problem,
and provides very efficient heuristics for backtracking or
branch-and-bound search for a fully integer solution.

The standard example for a QP are Support Vector Machines. The primal
problem is
\begin{align}
\min_{\b,\xi}
&~ \norm{\b}^2 + C \sum_{i=1}^n \xi_i \st y_i (x_i^\T \b) \ge
1-\xi_i\comma \xi_i\ge 0
\end{align}
the dual
\begin{align}
l(\a,\m)
&= \min_{\b,\xi} L(\b,\xi,\a,\m)
 = -{\textstyle\frac{1}{4}} \sum_{i=1}^n \sum_{i'=1}^n \a_i \a_{i'}
 y_i y_{i'} \hat x_i^\T \hat x_{i'} + \sum_{i=1}^n \a_i \\
\max_{\a,\m}
&~ l(\a,\m) \st 0 \le \a_i \le C
\end{align}
(See ML lecture 5:13 for a derivation.)

\cen{
\showh[.3]{svm_trenngeraden}
\hspace{1cm}
\showh[.35]{svm_margin}
}

\subsubsection{The Simplex Algorithm}

Consider an LP. We make the following observations:
\begin{itemize}
\item First, in LPs the equality constraints could be resolved simply
by introducing new coordinates along the zero-hyperplane of
$h$. Therefore, for the conceptual discussion we neglect equality constraints.
\item The objective constantly pulls in the direction $-c = -\na
f(x)$.
\item If the solution is bounded there need to be some inequality
constraints that keep the solution from travelling to $\infty$ in the
$-c$ direction.
\item It follows: \textbf{The solution will always be located at a vertex},
that is, an intersection point of several zero-hyperplanes of
inequality constraints.
\item In fact, we should think of the feasible region as
a \Def{polytope} that is defined by all the zero-hyperplanes of the
inequalities. The inside the polytope is the feasible region. The
polytope has edges (intersections of two contraint planes), faces,
etc. A solution will always be located at a vertex of the polytope;
more precisely, there could be a whole set of optimal points (on a
face orthogonal to $c$), but at least one vertex is also optimal.
\item An idea for finding the solution is to \textbf{walk on the edges
of the polytope} until an optimal vertex is found. This is the simplex
algorithm of Georg Dantzig, 1947. In practise this procedure is done
by ``pivoting on the simplex tableaux''---but we fully skip such
details here.
\item The simplex algorithm is often efficient, but in worst case it is
exponential in both, $n$ and $m$! This is hard to make intuitive,
because the effects of high dimensions are not intuitive. But roughly,
consider that in high dimensions there is a combinatorial number of
ways of how constraints may intersect and form edges and vertices.
\end{itemize}

Here is a view that much more relates to our discussion of the log
barrier method: Sitting on an edge/face/vertex is equivalent to
temporarily deciding which constraints are active. If we knew which
constraints are eventually active, the problem would be solved: all
inequalities become equalities or void. (And linear equalities can
directly be solved for.) So, jumping along vertices of the polytope is
equivalent to sequentially making decisions on which constraints might
be active. Note though that there are $2^m$ configurations of
active/non-active constraints. The simplex algorithm therefore walks
through this combinatorial space.

Interior point methods do exactly the opposite: Recall that
the \textbf{4th KKT condition} is $\l_i g_i(x) = 0$. The log barrier
method (for instance) instead \emph{relaxes} this hard logic of
activ/non-active constraints and finds in each iteration a solution to
the relaxed 4th KKT condition $\l_i g_i(x) = -\mu$, which intuitively
means that every constraint may be ``somewhat active''. In fact, every
constraint contributes somewhat to the stationarity condition via the
log barrier's gradients. Thereby interior point methods
\begin{itemize}
\item  post-pone the hard decisions about active/non-active
constraints
\item approach the optimal vertex from the inside of the polytope;
avoiding the polytope surface (and its hard decisions)
\item thereby avoids the need to search through a combinatorial space
of constraint activities and instead continuously converges to a
decision
\item has polynomial worst-case guaranteed
\end{itemize}

Historically, penalty and barrier methods methods were standard before
the Simplex Algorithm. When SA was discovered in the 50ies, it was
quickly considered great. But then, later in the 70-80ies, a lot more
theory was developed for interior point methods, which now again have
become somewhat more popular than the simplex algorithm.


\subsubsection{Sequential Quadratic Programming}

Just for reference, SQP is another standard approach to solving
non-linear mathematical programs. In each iteration we compute all
coefficients of the 2nd order Taylor
$f(x+\d) \approx f(x) + \na f(x)^\T\d + \half \d^\T H \d$
and 1st-order Taylor
$g(x+\d) \approx g(x) + \na g(x)^\T\d$ and then solve the QP
\begin{equation}
\min_\d~ f(x) + \na f(x)^\T \d + \half \d^\T \he f(x) \d \st g(x) + \na
g(x)^\T \d\le 0
\end{equation}
The optimal $\d^*$ of this problem should be seen
analogous to the optimal Newton step: If
$f$ \emph{were} a 2nd-order polynomial and $g$ linear, then $\d^*$
would jump directly to the optimum. However, as this is generally not
the case, $\d^*$ only gives us a very good direction for line
search. In SQP, we need to backtrack until we found a feasible
point \emph{and} $f$ decreases sufficiently.

Solving each QP in the sub-routine requires a constrained solver,
which itself might have two nested loops (e.g.\ using log-barrier or
AugLag). In that case, SQP has three nested loops.



\subsection{Blackbox \& Global Optimization: It's all about learning}

Even if $f,g,h$ are smooth, the solver might not have access to
analytic equations or efficient numeric methods to evaluate the
gradients or hessians of these. Therefore we distinguish (here
neglecting the constraint functions $g$ and $h$):
\begin{myDefinition}
~
\begin{itemize}
\item \emph{Blackbox optimization:} Only $f(x)$ can be evaluated.

\item \emph{1st-order/gradient optimization:} Only $f(x)$ and $\na f(x)$ can be evaluated.

\item \emph{Quasi-Newton optimization:} Only $f(x)$ and $\na f(x)$ can be
evaluated, but the solver does tricks to estimate $\he f(x)$. (So this
is a special case of 1st-order optimization.)

\item \emph{Gauss-Newton type optimization:} $f$ is of the special
form $f(x) = \phi(x)^\T \phi(x)$ and $\Del{x}\phi(x)$ can be evaluated.

\item \emph{2nd order optimization:} $f(x)$, $\na f(x)$ and $\he f(x)$ can be
evaluated.
\end{itemize}
\end{myDefinition}

In this lecture I very briefly want to add comments on \textbf{global}
blackbox optimization. Global means that we now, for the first time,
really aim to find the global optimum (within some pre-specified
bounded range). In essence, to address such a problem we need to
explicitly know what we know about $f$\footnote{Cf.\ the KWIK (knows
what it knows) framework.}, and an obvious way to do this is to use
Bayesian learning.

\subsubsection{A sequential decision problem formulation}

From now on, let's neglect constraints and focus on the mathematical
program
\begin{equation}
\min_x f(x)
\end{equation}
for a blackbox function $f$. The optimization process
can be viewed as a Markov Decision Process that describes the
interaction of the solver (agent) with the function (environment):
\begin{itemize}
\item At step $t$, $D_t=\{ (x_i,y_i) \}_{i=1}^{t\1}$ is the data that
the solver has collected from previous samples. This $D_t$ is the
state of the MDP.
\item At step $t$, the solver may choose a new decision $x_t$ about
where to sample next.
\item Given state $D_t$ and decision $x_t$, the next state is
$D_{t\po} = D \cup \{(x_t,f(x_t))\}$, which is a deterministic
transition \emph{given} the function $f$.
\item A solver policy is a mapping $\pi: D_t \mapsto x_t$ that maps
any state (of knowledge) to a new decision.
\item We may define an \textbf{optimal solver policy} as
\begin{equation}
\pi^* = \argmin_\pi \< y_T \> = \argmin_\pi \int_f
P(f)~ P(D_T|\pi,f)~ y_T
\end{equation}
where $P(D_T|\pi,f)$ is deterministic, and $P(f)$ is a \textbf{prior
over functions}.

This objective function cares only about the \emph{last} value $y_T$
sampled by the solver for a fixed time horizon (budget) $T$. Alternatively, we
may choose objectives $\sum_{t=1}^T y_t$ or $\sum_{t=1}^T \g^t y_t$
for some discounting $\g\in[0,1]$
\end{itemize}

The above defined what is an optimal solver! Something we haven't
touched at all before. The transition dynamics of this MDP is
deterministic, given $f$. However, from the perspective of the solver,
we do not know $f$ apriori. But we can always compute
a \textbf{posterior belief} $P(f|D_t) = P(D_t|f)~ P(f)/P(D_t)$. This
posterior belief defines a \Def{belief MDP} with stochastic transitions
\begin{equation}
P(D_{t\po}) = \int_{D_t} \int_f \int_{x_t} [D_{t\po} =
D \cup \{(x_t,f(x_t))\}]~ \pi(x_t|D_t)~ P(f|D_t)~ P(D_t) ~.
\end{equation}
The belief
MDP's state space is $P(D_t)$ (or equivalently, $P(f|D_t)$, the
current belief over $f$). This belief MDP is something that the solver
can, in principle, forward simulate---it has all information about
it. One can prove that, if the solver could solve its own belief MDP
(find an optimal policy for its belief MDP), then this policy is the
optimal solver policy for the original problem given a prior distribution $P(f)$! So, in
principle we not only defined what is an optimal solver policy, but
can also provide an algorithm to compute it (Dynamic programming in
the belief MDP)! However, this is so expensive to compute that
heuristics need to be used in practise.

One aspect we should learn from this discussion: The solver's
optimal decision is based on its current belief $P(f|D_t)$ over the
function. This belief is the Bayesian representation of everything one
could possibly have learned about $f$ from the data $D_t$ collected so
far. Bayesian Global Optimization methods compute $P(f|D_t)$ in every
step and, based on this, use a heuristic to choose the next decision.

\subsubsection{Acquisition Functions for Bayesian Global Optimization*}

In pratice one typically uses a Gaussian Process representation of
$P(f|D_t)$. This means that in every iteration we have an estimate $\hat
f(x)$ of the function mean and a variance estimate $\hat\s(x)^2$ that
describes our uncertainty about the mean estimate. Based on this we
may define the following acquisition functions
\begin{myDefinition}\qquad
\Def{Probability of Improvement (MPI)}
\begin{equation}
\a_t(x) = \Int_{-\infty}^{y^*} \NN(y|\hat f(x),\hat\s(x))~ dy
\end{equation}
\Def{Expected Improvement (EI)}
\begin{equation}
\a_t(x) = \Int_{-\infty}^{y^*} \NN(y|\hat f(x),\hat\s(x))~ (y^*-y)~ dy
\end{equation}
\Def{Upper Confidence Bound (UCB)}
\begin{equation}
\a_t(x) = - \hat f(x) + \b_t \hat\s(x)
\end{equation}
\Def{Predictive Entropy Search} \cite{NIPS2014_5324}
\begin{align}
\a_t(x)
&= H[p(x^*|D_t)] - \Exp{p(y|D_t;x)}{H[p(x^*|D_t \cup \{(x,y)\})]} \\
&= I(x^*,y|D_t) = H[p(y|D_t,x)] - \Exp{p(x^*|D_t)}{H[p(y|D_t,x,x^*)]} \nonumber
\end{align}
\end{myDefinition}
The last one is special; we'll discuss it below.

These acquisition functions are heuristics that define how valuable it
is to acquire data from the site $x$. The solver then makes the
decision
\begin{equation}
x_t = \argmax_x \a_t(x) ~.
\end{equation}

MPI is hardly being used in practise anymore. EI is classical,
originating way back in the 50ies or earlier; \citet{jones-et-al:98}
gives an overview.  UCB received a lot of attention recently due to
the underlying bandit theory and bounded regret theorems due to the
submodularity. But I think that in practise EI and UCB perform about
equally. As UCB is somewhat easier to implement and intuitive.

In all cases, note that the solver policy $x_t = \argmax_x \a_t(x)$
requires to internally solve another non-linear optimization
problem. However, $\a_t$ is an analytic function for which we can
compute gradients and hessians which ensures every
efficient \emph{local} convergence. But again, $x_t = \argmax_x \a_t(x)$
needs to be solved \emph{globally}---otherwise the solver will also
not solve the original problem properly and globally. As a
consequence, the optimization of the acquisition function needs to be
restarted from many many potential start points close to potential
local minima; typically from grid(!) points over the full domain
range. The number of grid points is exponential in the problem
dimension $n$. Therefore, this inner loop can be very expensive.

And a subjective note: This all sounds great, but be aware that
Gaussian Processes with standard squared-exponential kernels do not
generalize much in high dimensions: one roughly needs exponentially
many data points to fully cover the domain and reduce belief
uncertainty globally, almost as if we were sampling from a grid with
grid size equal to the kernel width. So, the whole approach is not
magic. It just does what is possible given a belief $P(f)$. It would
be interesting to have much more structured (and heteroscedastic)
beliefs specifically for optimization.

The last acquisition function is called \Def{Predictive Entropy
Search}. This formulation is beautiful: We sample at places $x$ where
the (expected) observed value $y$ informs us as much as possible about
the optimum $x^*$ of the function! Formally, this means to maximize
the mutual information between $y$ and $x^*$, in expectation over $y|x$.



\subsubsection{Classical model-based blackbox optimization (non-global)*}

A last method very worth mentioning: Classical model-based blackbox
optimization simply fits a local polynomial model to the recent data
and takes this a basis for search. This is similar to BFGS, but now
for the blackbox case where we not even observe gradients. See
Algorithm \ref{algoModelBasedOpt}.

The local fitting of a polynomial model is again a Machine Learning
method. Whether this gives a function approximation for optimization
depends on the quality of the data $D_t$ used for this
approximation. Classical model-based optimization has interesting
heuristics to evaluate the data quality as well as sample new points
to improve the data quality. Here is a rough algorithm (following
Nodecal et al.'s section on ``Derivative-free optimization''):

\begin{algorithm}
\caption{Classical model-based optimization}\label{algoModelBasedOpt}
\begin{algorithmic}[1]
\State Initialize $D$ with at least $\half (n+1)(n+2)$ data points
\Repeat
\State Compute a regression $\hat f(x) = \phi_2(x)^\T\b$ on $D$
\State Compute $\d = \argmin_\d \hat f(\hat x+\d) \st |\d|<\a$
\If{$f(\hat x+\d) < f(\hat x) - \lsstop [f(\hat x)-\hat f(\hat
x+\d)]$} \Comment{test sufficient decrease}
\State Increase the stepsize $\a$
\State Accept $\hat x \gets \hat x + \d$
\State Add to data, $D \gets D \cup \{(\hat x,f(\hat x))\}$
\Else \Comment{no sufficient decrease}
\If{$\det(D)$ is too small} \Comment{blame the data quality}
\State Compute $x^+ = \argmax_{x'} \det(D\cup\{x'\}) \st |x-x'|<\a$
\State Add to data, $D \gets D \cup \{(x^+,f(x^+))\}$
\Else \Comment{blame the stepsize}
\State Decrease the stepsize $\a$
\EndIf
\EndIf
\State Perhaps prune the data, e.g., remove $\argmax_{x\in\Delta} \det(D\setminus\{x\})$
\Until $x$ converges
\end{algorithmic}
\end{algorithm}

Some notes:
\begin{itemize}
\item Line 4 implement an explicit trust region approach, which hard
bound $\a$ on the step size.
\item Line 5 is like the Wolfe condition. But here, the expected
decrease is $[f(\hat x)-\hat f(\hat x+\d)]$ instead of $-\a\d\na
f(x)$.
\item If there is no sufficient decrease we may blame it on two
reasons: bad data or a too large stepsize.
\item Line 10 uses the \emph{data determinant} as a measure of
quality! This is meant in the sense of linear regression on polynomial
features. Note that, with data matrix $X\in\RRR^{n\times\dim(\b)}$,
$\hat \b^{\text{ls}} = (\vec X^\T \vec X)^\1 \vec X^\T y$ is the
optimal regression. The determinant $\det (\vec X^\T \vec X)$ or
$\det(\vec X)=\det(D)$ is a measure for well the data supports the
regression. If the determinant is zero, the regression problem is
ill-defined. The larger the determinant, the lower the variance of the
regression estimator.

\item Line 11 is an explicit exploration approach: We add a data point
solely for the purpose of increasing the data determinant (increasing
the data spread). Interesting.  Nocedal describes in more detail a
geometry-improving procedure to update $D$.
\end{itemize}

\subsubsection{Evolutionary Algorithms*}

There are interesting and theoretically well-grounded evolutionary
algorithms for optimization, such as Estimation-of-Distribution
Algorithms (EDAs). But generally, don't use them as first choice.

\subsection{Examples and Exercises}

\input{e06-unconstrainedOpt.tex}
\input{e07-lagrangian.tex}
\input{e08-constrainedOpt.tex}
\input{e09-formulatingProblems.tex}
\input{e10-globalOpt.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Probabilities \& Information}

It is beyond the scope of these notes to give a detailed introduction
to probability theory. There are excellent books:
\begin{itemize}
\item Thomas \& Cover
\item Bishop
\item MacKay
\end{itemize}

Instead, we first recap very basics of probability theory, that I
assume the reader has already seen before. The next section will cover
this. Then we focus on specific topics that, in my opinion, deepen the
understanding of the basics, such as the relation between optimization
and probabilities, log-probabilities \& energies, maxEntropy and
maxLikelihood, minimal description length and learning.

\subsection{Basics}

First, in case you wonder about justifications of the use of
(Bayesian) probabilities versus fuzzy sets or alike, here some
pointers to look up: 1) Cox's theorem, which derives from basic
assumptiond about ``rationality and consistency'' the standard
probability axioms; 2) t-norms, which generalize probability and fuzzy
calculus; and 3) read about objective vs.\ subjecte Bayesian
probability.

\subsubsection{Axioms, definitions, Bayes rule}

\begin{myDefinition}[set-theoretic axioms of probabilities] ~\\[-5ex]
\begin{itemize}
\item An experiment can have multiple outcomes; we call the set of
possible outcomes \Def{sample space} or \Def{domain} $S$
\item A mapping $P:~ A\subseteq S \mapsto [0,1]$, that maps any subset
$A\subseteq S$ to a real number, is called \Def{probability measure}
on $S$ iff
\begin{items}
\item $P(A) \ge 0$ for any $A\subseteq S$ (non-negativity)
\item $P(\bigcup_i A_i) = \sum_i P(A_i)$ ~ if $A_i\cap A_j= \emptyset$ (additivity)
\item $P(S) = 1$ (normalization)
\end{items}
\item Implications are:
\begin{items}
\item $0 \le P(A) \le 1$
\item $P(\emptyset) = 0$
\item $A\subseteq B \To P(A)\le P(B)$
\item $P(A\cup B) = P(A)+P(B) - P(A\cap B)$
\item $P(S \setminus A) = 1 - P(A)$
\end{items}
\end{itemize}
\end{myDefinition}

Formally, a \Def{random variable} $X$ is a mapping $X:S\to \O$ from a 
measureable space $S$ (that is, a sample space $S$ that has a
probability measure $P$) to another sample space $\O$, which I
typically call the \Def{domain} $\dom(X)$ of the random
variable. Thereby, the mapping $X: S \to \O$ now also defines a probability
measure over the domain $\O$:
\begin{equation}
P(B \subseteq \O) = P(\{ s : X(s)\in B \})
\end{equation}
In practise we just use the following notations:

\begin{myDefinition}[Random Variable] ~\\[-5ex]
\renewcommand{\=}{\!=\!}
\begin{itemize}
\item  Let $X$ be a random variable with discrete domain $\dom(X)
= \O$
\item $P(X\=x) \in \RRR$ denotes the specific probability that $X=x$
for some $x\in\O$
\item $P(X)$ denotes the \Def{probability distribution} (function over $\O$)
\item $\forall_{x\in\O}:~ 0\le P(X\=x) \le 1$
\item $\sum_{x \in \O} P(X\=x) = 1$
\item We often use the short hand $\sum_X P(X)~ \cdots
= \sum_{x\in\dom(X)} P(X\=x)~ \cdots$ when summing over possible
values of a RV
\end{itemize}
\end{myDefinition}

If we have two or more random variables, we have
\begin{myDefinition}[Joint, marginal, conditional, independence, Bayes' Theorem]~\\[-5ex]
\begin{itemize}
\item We denote the \Def{joint} distribution of two RVs as $P(X,Y)$
\item The \Def{marginal} is defined as $P(X) = \sum_Y P(X,Y)$
\item The \Def{conditional} is defined as $P(X|Y)
= \frac{P(X,Y)}{P(Y)}$, which fulfils\newline $\forall_Y:~ \sum_X P(X|Y) = 1$.
\item $X$ is \Def{independent} of $Y$ iff $P(X,Y) = P(X)~ P(Y)$, or
equivalently,\newline $P(X|Y) = P(X)$.
\item The definition of a conditional implies the \Def{product
rule}
\begin{equation}
P(X,Y) = P(X|Y)~ P(Y) = P(Y|X)~ P(X)
\end{equation}
and \Def{Bayes' Theorem}
\begin{equation}
P(X|Y) = \frac{P(Y|X)~ P(X)}{P(Y)} 
\end{equation}
The individual terms in Bayes' Theorem are typically given names:
\begin{equation}
\text{posterior} ~=~
\frac{\text{likelihood} ~\cdot~ \text{prior}}{\text{normalization}}
\end{equation}
(Sometimes, the normalization is also called \emph{evidence}.)
\item $X$ is \emph{conditionally independent} of $Y$ given $Z$ iff $P(X|Y,Z)
 = P(X|Z)$ or $P(X,Y|Z) = P(X|Z)~ P(Y|Z)$
%% \item Analogously, for 3 random variables $X, Y, Z$ we have
%% $P(X,Z,Y) = P(X|Y,Z)~ P(Y|Z)~ P(Z)$
%% \medskip
%% $P(X|Y,Z) = \frac{P(Y|X,Z)~ P(X|Z)}{P(Y|Z)} $
%% \medskip
%% $P(X,Y|Z) = \frac{P(X,Z|Y)~ P(Y)}{P(Z)} $
\end{itemize}
\end{myDefinition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Standard discrete distributions}

\begin{tabular}{|p{.15\columnwidth}|p{.23\columnwidth}|p{.23\columnwidth}|p{.3\columnwidth}|}
\hline
& RV & parameter & distribution \\
\hline
\hline
Bernoulli
& $x \in \{0,1\}$
& $\mu\in [0,1]$
& $\Bern(x\|\m) = \m^x (1-\m)^{1-x}$ \\
\hline
Beta
& $\mu\in[0,1]$
& $\a,\b \in \RRR^+$
& $\Beta(\m\|a,b) = \frac{1}{B(a,b)}~ \m^{a-1} (1-\m)^{b-1}$ \\
\hline
Multinomial
& $x \in \{1,..,K\}$
& $\m \in [0,1]^K,~ \norm{\mu}_1=1$
& \renewcommand{\=}{\!=\!} $P(x\=k\|\m) = \m_k$ \\
\hline
Dirichlet
& $\m \in [0,1]^K,~ \norm{\m}_1=1$
& $\a_1,..,\a_K \in \RRR^+$
& $\Dir(\m\|\a) \propto~ \Prod_{k=1}^K \m_k^{\a_k-1}$ \\
\hline
\end{tabular}
%=(\m_1,..,\m_K)

Clearly, the Multinomial is a generalization of the Bernoulli, as the
Dirichlet is of the Beta. The mean of the Dirichlet is
$\<\m_i\>=\frac{\a_i}{\sum_j\!\a_j}$, its mode is $\m_i^*
= \frac{\a_i-1}{\sum_j\!\a_j-K}$. The \Def{mode} of a distribution
$p(x)$ is defined as $\argmax_x p(x)$.

\subsubsection{Conjugate distributions}

\begin{myDefinition}[Conjugacy]
Let $p(D|x)$ be a likelihood conditional on a RV $x$. A family $\CC$
of distributions (i.e., $\CC$ is a space of distributions, like the
space of all Beta distributions) is called \Def{conjugate} to the
likelihood function  $p(D|x)$ iff
\begin{equation}
p(x) \in \CC \quad\To\quad p(x|D) = \frac{p(D|x)~ p(x)}{p(D)} \in \CC ~.
\end{equation}
\end{myDefinition}

The standard conjugates you should know:\\
\begin{tabular}{|p{.1\columnwidth}|@{\quad}p{.32\columnwidth}|@{\quad}p{.4\columnwidth}|}
\hline
RV & likelihood & conjugate \\
\hline
$\m$ & Binomial $\Bin(D \| \m)$ & Beta $\Beta(\m \| a,b)$ \\
$\m$ & Multinomial $\Mult(D \| \m)$ & Dirichlet $\Dir(\m \| \a)$ \\
$\m$ & Gauss $\NN(x \| \m,\S)$ & Gauss $\NN(\m \| \m_0, A)$ \\
$\l$ & 1D Gauss $\NN(x \| \m,\l^\1)$ & Gamma ${\rm Gam}(\l \| a,b)$ \\
$\L$ & $n$D Gauss $\NN(x \| \m,\L^\1)$ & Wishart ${\rm Wish}(\L \| W, \n)$ \\
$(\m,\L)$ & $n$D Gauss $\NN(x \| \m,\L^\1)$ & Gauss-Wishart $\NN(\m\|\m_0,
(\b\L)^\1)~ {\rm Wish}(\L \| W, \n)$ \\
\hline
\end{tabular}

\subsubsection{Distributions over continuous domain}

\begin{myDefinition}
Let $x$ be a continuous RV. The \Def{probability density
  function (pdf)} $p(x)\in[0,\infty)$ defines the probability 
\begin{equation}
P(a\le x \le b) = \int_a^b p(x)~ dx ~ \in[0,1]
\end{equation}
The \Def{cumulative probability distribution} $F(y) = P(x \le y) = \int_{-\infty}^y dx~
p(x)\in[0,1]$ is the cumulative integral with $\lim_{y\to\infty} F(y) = 1$

However, I and most others say \Def{probability distribution} to refer
  to probability density function.
\end{myDefinition}

One comment about integrals. If $p(x)$ is a probability density function
and $f(x)$ some arbitrary function, typically one writes
\begin{equation}
\int_x f(x)~ p(x)~ dx ~,
\end{equation}
where $dx$ denotes the (Borel) measure we
integrate over. However, some authors (correctly) think of a
distribution $p(x)$ as being a measure over the space $\dom(x)$
(instead of just a function). So the above notation is actually
``double'' w.r.t.\ the measures. So they might (also correctly) write
\begin{equation}
\int_x p(x)~ f(x) ~,
\end{equation}
and take care that there is exactly one
measure to the right of the integral.


\subsubsection{Gaussian}

\begin{myDefinition} We define an  $n$-dim Gaussian in \emph{normal
  form} as
\begin{align}
\NN(x \| \m, \S)
 &= \frac{1}{\|2\pi \S\|^{1/2}}~ \exp\{-\half (x-\m)^\T~ \S^\1~ (x-\m)\}
\end{align}
with \Def{mean} $\m$ and \Def{covariance} matrix
$\S$. In \emph{canonical form} we define
\begin{align}
\NN[x \| a, A]
 = \frac{\exp\{-\half a^\T A^\1a\}}{\|2\pi A^\1\|^{1/2}}~
   \exp\{-\half x^\T~ A~ x + x^\T a\}
\end{align}
with \Def{precision} matrix $A = \S^\1$ and coefficient $a=\S^\1 \m$
(and mean $\m = A^\1 a$).
\end{myDefinition}

Gaussians are used all over---below we explain in what sense they are
the probabilistic analogue to a parabola (or a 2nd-order Taylor
expansions). The most important properties are:
\begin{itemize}
\providecommand{\+}{\myplus}
\renewcommand{\-}{\,\text{-}\,}

\item Symmetry: \qquad $\NN(x\|a,A) = \NN(a\|x,A) = \NN(x-a\|0,A)$
\item Product:\\
$\NN(x \| a,A)~ \NN(x \| b,B) 
 = \NN[x \| A^\1 a+B^\1 b, A^\1 + B^\1]~ \NN(a\|b,A+B)$\\
$\NN[x \| a,A]~ \NN[x \| b,B]
 = \NN[x \| a+b,A+B]~ \NN(A^\1 a \| B^\1 b, A^\1+B^\1)$
\item ``Propagation'':\\
$\int_y \NN(x \| a + Fy, A)~ \NN(y \| b, B)~ dy
 = \NN(x \| a + Fb, A+FBF^\T)$
\item Transformation:\\
$ \NN(F x + f \| a,A)
 = \frac{1}{\|F\|}~ \NN(x \| ~ F^\1 (a-f),~ F^\1 AF^\mT) $
\item Marginal \& conditional:\\
$\NN\bigg( \arr{c}{x\\ y} \bigg| \arr{c}{a\\ b} ,~ 
         \arr{cc}{A & C\\ C^\T & B} \bigg)
 = \NN(x \| a,A) \cdot \NN(y \| b+C^\T A^\1(x\-a),~ B - C^\T A^\1 C)$
\end{itemize}

More Gaussian identities are found at {\tiny\url{http://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gaussians.pdf}}

\begin{myExample}[ML estimator of the mean of a Gaussian] Assume we have data $D
= \{x_1,..,x_n\}$, each $x_i\in\RRR^n$, with likelihood
\begin{align}
P(D\|\m,\S)
&= \Prod_i \NN(x_i\|\m,\S) \\
\argmax_\m~ P(D\|\m,\S)
&= \frac{1}{n}\sum_{i=1}^n x_i \\
\argmax_\S~ P(D\|\m,\S)
&= \frac{1}{n} \sum_{i=1}^n (x_i-\m)(x_i-\m)^\T
\end{align}
 Assume we are initially uncertain about $\m$ (but know $\S$). We
can express this uncertainty using again a Gaussian $\NN[\m \|
a,A]$. Given data we have
\begin{align}
P(\m\|D)
&\propto P(D\|\m,\S)~ P(\m)
 = \Prod_i \NN(x_i\| \m,\S)~ \NN[\m \| a, A] \\
&= \Prod_i \NN[\m\| \S^\1 x_i,\S^\1]~ \NN[\m \| a, A]
 \propto \NN[\m \| \S^\1 \sum_i x_i,~ n\S^\1 + A]
\end{align}
Note: in the limit $A\to 0$ (uninformative prior) this becomes
\begin{align}
P(\m\|D)
 &= \NN(\m \| \frac{1}{n}\sum_i x_i, \frac{1}{n}\S)
\end{align}
which is consistent with the Maximum Likelihood estimator
\end{myExample}

\subsubsection{``Particle distribution''}

Usually, ``particles'' are not listed as standard continuous
distribution. However I think they should be. They're heavily used in
several contexts, especially as approximating other distributions in
Monte Carlo methods and particle filters.

\begin{myDefinition}[Dirac or $\d$-distribution] In \emph{distribution
theory} it is proper to define a distribution $\d(x)$ that is the
derivative of the Heavyside step function $H(x)$,
\begin{equation}
\d(x) = \Del{x} H(x) \comma H(x) = [x\ge0] ~.
\end{equation}
It is akward to think of $\d(x)$ as a normal function, as it'd be
``infinite'' at zero. But at least we understand that is has the
properties
\begin{equation}
\d(x) = 0 \text{~everywhere except at~} x=0 \comma \int\d(x)~dx =
1 ~.
\end{equation}
\end{myDefinition}

I sometimes call the Dirac distribution also a \Def{point
particle}: it has all its unit ``mass'' concentrated at zero.

\begin{myDefinition}[Particle Distribution]
We define a \Def{particle distribution} $q(x)$ as
a \Def{mixture} of Diracs,
\begin{equation}
q(x) := \sum_{i=1}^N w_i~ \d(x-x_i) ~,
\end{equation}
which is parameterized by the number $N$, the locations $\{
x_i \}_{i=1}^N$, $x_i \in\RRR^n$, and the normalized weights $\{
w_i \}_{i=1}^N$, $w_i \in\RRR$, $\norm{w}_1 = 1$ of the $N$ particles.
\end{myDefinition}

We say that a particle distribution $q(x)$ approximates another
distribution $p(x)$ iff for any (smooth) $f$
\begin{equation}
\<f(x)\>_p = \int_x f(x) p(x) dx ~\approx~ \Sum_{i=1}^N w_i f(x_i)
\end{equation}
Note the generality of this statement! $f$ could be anything, it could
be any features of the variable $x$, like coordinates of $x$, or
squares, or anything. So basically this statement says, whatever you
might like to estimate about $p$, you can approximate it based on the
particles $q$.

Computing particle approximations of comples (non-analytical,
non-tracktable) distributions $p$ is a core challenge in many
fields. The true $p$ could for instance be a distributions over games
(action sequences). The approximation $q$ could for instance be
samples generated with Monte Carlo Tree Search (MCTS). The
tutorial \emph{An Introduction to MCMC for Machine
Learning} \url{www.cs.ubc.ca/~nando/papers/mlintro.pdf} gives an
excellent introduction. Here are some illustrations of what it means
to approximate some $p$ by particles $q$, taken from this
tutorial. The black line is $p$, histograms illustrate the particles
$q$ by showing how many of (uniformly weighted) particles fall into a
bin:

\show{particleRepresentation}
(from de Freitas et al.)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
\key{Utilities and Decision Theory}
\slide{Utilities \& Decision Theory}{

~

\item Given a space of events $\O$ (e.g., outcomes of a trial, a game,
etc) the utility is a function
\begin{equation}
 U:~ \O \to \RRR 
\end{equation}

\item The utility represents preferences as a single scalar -- which
is not always obvious ~ (cf.\ multi-objective optimization)

\item \emph{Decision Theory} making decisions (that determine $p(x)$) that maximize expected utility
\begin{equation}
 \Exp{U}_p = \int_x U(x)~ p(x) 
\end{equation}

\item Concave utility functions imply risk aversion (and convex,
risk-taking)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Monte Carlo methods}{
}

\key{Monte Carlo methods}
\slide{Monte Carlo methods}{

\item Generally, a Monte Carlo method is a method to generate a set of
(potentially weighted) samples that approximate a distribution $p(x)$.

In the unweighted case, the samples should be i.i.d.\ $x_i\sim p(x)$

In the general (also weighted) case, we want particles that allow to
estimate expectations of anything that depends on $x$, e.g.\ $f(x)$:
\begin{equation}
\lim_{N\to \infty} \<f(x)\>_q
 = \lim_{N\to\infty} \sum_{i=1}^N w_i f(x_i)
 = \int_x f(x)~ p(x)~ dx = \<f(x)\>_p
\end{equation}

In this view, Monte Carlo methods approximate an integral.

\item Motivation: $p(x)$ itself is too complicated to express
analytically or compute $\<f(x)\>_p$ directly

~\small

\item Example: What is the probability that a solitair would come out
successful? (Original story by Stan Ulam.) Instead of trying to
analytically compute this, generate many random solitairs and count.

\item Naming: The method developed in the 40ies, where computers became
faster. Fermi, Ulam and von Neumann initiated the idea. von Neumann
called it ``Monte Carlo'' as a code name.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Rejection sampling}
\slide{Rejection Sampling}{

\item How can we generate i.i.d.\ samples $x_i\sim p(x)$?

\item Assumptions:
\begin{items}
\item We can sample $x\sim q(x)$ from a simpler distribution $q(x)$
(e.g., uniform), called \Def{proposal distribution}
\item We can numerically evaluate $p(x)$ for a specific $x$ (even if
we don't have an analytic expression of $p(x)$)
\item There exists $M$ such that $\forall_x: p(x) \le M q(x)$ (which
implies $q$ has larger or equal support as $p$)
\end{items}

~

\item Rejection Sampling:
\begin{items}
\item Sample a candiate $x \sim q(x)$
\item With probability $\frac{p(x)}{M q(x)}$ accept $x$ and add to
$\SS$; otherwise reject
\item Repeat until $|\SS|=n$
\end{items}

\item This generates an unweighted sample set $\SS$ to approximate $p(x)$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Importance sampling}
\slide{Importance sampling}{

\item Assumptions:
\begin{items}
\item We can sample $x\sim q(x)$ from a simpler distribution $q(x)$
(e.g., uniform)
\item We can numerically evaluate $p(x)$ for a specific $x$ (even if
we don't have an analytic expression of $p(x)$)
\end{items}

\item Importance Sampling:
\begin{items}
\item Sample a candiate $x \sim q(x)$
\item Add the weighted sample $(x,\frac{p(x)}{q(x)})$ to $\SS$
\item Repeat $n$ times
\end{items}

\item This generates an weighted sample set $\SS$ to approximate
$p(x)$

The weights $w_i = \frac{p(x_i)}{q(x_i)}$ are
called \Def{importance weights}

~

\item Crucial for efficiency: a good choice of the proposal $q(x)$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Applications}{

\item MCTS estimates the $Q$-function at branchings in decision trees or games

\item Inference in graphical models (models involving many depending random variables)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Student's t, Exponential, Laplace, Chi-squared, Gamma distributions}
\slide{Some more continuous distributions*}{\label{lastpage}

\begin{tabular}{p{.4\columnwidth}p{.6\columnwidth}}
Gaussian &
$\NN(x \| a,A) = \frac{1}{\|2\pi A\|^{1/2}}~ e^{-\half (x-a)^\T~ A^\1~ (x-a)}$
\\
Dirac or $\d$ &
$\d(x) = \Del{x} H(x)$
\\
Student's t\newline\tiny
(=Gaussian for $\nu\to\infty$, otherwise heavy tails) &
$p(x;\n) \propto [1+\frac{x^2}{\n}]^{-\frac{\n+1}{2}}$
\\
Exponential\newline\tiny (distribution over single event time) &
$p(x;\l) = [x\ge 0]~ \l e^{-\l x}$
\\
Laplace\newline\tiny (``double exponential'') &
$p(x;\m,b) = \frac{1}{2b} e^{-\|x-\m\|/b}$
\\
Chi-squared &
$p(x;k) \propto [x\ge0]~ x^{k/2-1} e^{-x/2}$
\\
Gamma &
$p(x;k,\t) \propto [x\ge0]~ x^{k-1} e^{-x/\t}$
\end{tabular}

}

\end{comment}


\subsection{Between probabilities and optimization: neg-log-probabilities,
exp-neg-energies, exponential family, Gibbs and Boltzmann}

There is a natural relation between probabilities and ``energy''
(or ``error''). Namely, if $p(x)$ denotes a probability for every
possible value of $x$, and
$E(x)$ denotes an energy for state $x$---or an error one assigns to
choosing $x$---then a natural relation is
\begin{equation}
p(x) = e^{-E(x)} \comma E(x) = -\log p(x) ~.
\end{equation}

Why is that? First, outside the context of physics it is perfectly
fair to just define axiomatically an energy $E(x)$ as
neg-log-probability. But let me try to give some more arguments for
why this is a useful definition.

Let assume we have $p(x)$. We want to find a quantity, let's call
it \emph{error} $E(x)$, which is a function of $p(x)$. Intuitively, if
a certain value $x_1$ is more likely than another, $p(x_1) > p(x_2)$,
then picking $x_1$ should imply less error, $E(x_1) <
E(x_2)$ (Axiom 1). Further, when we have two independent random variables $x$
and $y$, \Def{probabilities are multiplicative}, $p(x,y) = p(x)
p(y)$. We require axiomatically that \Def{error is additive},
$E(x,y) = E(x) + E(y)$. From both follows that $E$ needs to be some
logarithm of $p$!

The same argument, now more talking about \emph{energy}: Assume we
have two independent (physical) systems $x$ and $y$. $p(x,y) = p(x)
p(y)$ is the probability to find them in certain states. We
axiomatically require that \Def{energy is additive}, $E(x,y) = E(x)
+ E(y)$. Again, $E$ needs to be some logarithm of $p$. In the context
of physics, what could be questioned is ``why is $p(x)$ a function of
$E(x)$ in the first place?''. Well, that is much harder to
explain and really is a question about statistical physics. Wikipedia
under keywords ``Maxwell-Boltzmann statistics'' and ``Derivation from
microcanonical ensemble'' gives an answer. Essentially the argument is
a follows: Given many many molecules in a gas, each of which can have
a different energy $e_i$. The  total energy $E = \sum_{i=1}^n e_i$
must be conserved. What is the distribution over energy levels that
has the most microstates? The answer is the Boltzmann
distribution. (And why do we, in nature, find energy distributions
that have the most microstates? Because these are most likely.)

Bottom line is: $p(x) = e^{-E(x)}$, probabilities are multiplicative,
energies or errors additive.

Let me state some fact just to underline how useful this way of
thinking is:
\begin{itemize}
\item Given an energy function $E(x)$, its \Def{Boltzmann
distribution} is defined as
\begin{equation}
 p(x) = e^{-E(x)}. 
\end{equation}
This is sometimes also called Gibbs distribution.

\item In machine learning, when data $D$ is given and we have some
model $\b$, we typically try to maximize the likelihood
$p(D|\b)$. This is equivalent to minimizing the neg-log-likelihood
\begin{equation}
L(\b) = -\log p(D|\b) ~.
\end{equation}
This neg-log-likelihood is a typical measure for \emph{error} of the
model. And this error is additive w.r.t.\ the data, whereas the
likelihood is multiplicative, fitting perfectly to the above
discussion.

\item The Gaussian distribution $p(x) \propto \exp\{
-\half \norm{x-\m}^2/{\s^2} \}$ is related to the error $E(x)
 = \half \norm{x-\m}^2/{\s^2}$, which is nothing but the squared
 error with the precision matrix as metric. That's why squared error
 measures (classical regression) and Gaussian distributions (e.g.,
 Bayesian Ridge regression) are directly related.

  A Gaussian is the probabilistic analoque to a parabola.

\item The exponential family is defined as
\begin{equation}
 p(x|\b) = h(x) g(\b) \exp\{\b^\T \phi(x)\} 
\end{equation}
Often $h(x)=1$, so let's neglect this for now. The key point is that
the energy is linear in the features $\phi(x)$. This is exactly how
discriminative functions (for classification in Machine learning) are
typically formulated.

In the continuous case, the features $\phi(x)$ are often chosen as basis
polynomials---just as in polynomial regression. Then, $\b$ are the
coefficients of the energy polynomial and the exponential family is
just the probabilistic analogue to the space of polynomials.

\item When we have many variables $x_1,..,x_n$, the structure of a
cost function over these variables can often be expressed as being
additive in terms: $f(x_1,..,x_n) = \sum_i \phi_i(x_{\del i})$ where
$\del i$ denotes the $i$th group of variables. The respective
Boltzmann distribution is a \Def{factor graph}
$p(x_1,..,x_n) \propto \prod_i f_i(x_{\del i})
= \exp\{\sum_i \b_i \phi_i(x_{\del i})$ where $\del i$ denotes the

So, factor graphs are the probabilistic analoque to additive functions.

\item $-\log p(x)$ is also the ``optimal'' coding length you should
assign to a symbol $x$.

\Def{Entropy is expected error}: $H[p] = \sum_x -p(x) \log
p(x) = \< - \log p(x) \>_{p(x)}$, where $p$ itself it used to take the expectation.

Assume you use a ``wrong'' distribution $q(x)$ to decide on the
coding length of symbols drawn from $p(x)$. The expected length of an
encoding is $\int_x p(x) [-\log q(x)] \ge H(p)$.

The \Def{Kullback-Leibler divergence} is the difference:
\begin{equation}
 \kld{p}{q} = \int_x p(x) \log \frac{p(x)}{q(x)} \ge 0 
\end{equation}

Proof of inequality, using the Jenson inequality:
\begin{align}
- \int_x p(x) \log \frac{q(x)}{p(x)}
& \ge - \log \int_x p(x) \frac{q(x)}{p(x)} = 0
\end{align}

\end{itemize}

So, my message is that probabilities and error measures are naturally
related. However, in the first case we typically do inference, in the
second we optimize. Let's discuss the relation between inference and
optimization a bit more. For instance, given data $D$ and parameters $\b$,
we may define

\begin{myDefinition}[ML, MAP, and Bayes estimate]
Given data $D$ and a parameteric model $p(D|\b)$, we define
\begin{itemize}
\item \Def{Maximum likelihood (ML) parameter estimate:}

$\b^\text{ML} := \argmax_\b P(D | \b)$

\item \Def{Maximum a posteriori (MAP) parameter estimate:}

$\b^\text{MAP} = \argmax_\b P(\b | D)$

\item \Def{Bayesian parameter estimate:}

$P(\b|D) \propto P(D|\b)~ P(\b)$

used for \Def{Bayesian prediction:~} $P(\text{prediction}|D) = \int_\b P(\text{prediction} | \b)~ P(\b | D)$
\end{itemize}
\end{myDefinition}

Both, the MAP and the ML estimates are really just optimization
problems.

The Bayesian parameter estimate $P(\b|D)$, which can
then be used to do fully Bayesian prediction, is in principle
different. However, in practise also here optimization is a core tool
for estimating such distributions if they cannot be given
analytically. This is described next.


\subsection{Information, Entropie \& Kullback-Leibler}

Consider the following problem. We have data drawn i.i.d.\ from $p(x)$
where $x\in X$ in some discrete space $X$. Let's call every $x$
a \emph{word}. The problem is to find a mapping from words
to \emph{codes}, e.g.\ binary codes $c:~ X \to \{0,1\}^*$. The optimal
solution is in principle simple: Sort all possible words in a list,
ordered by $p(x)$ with more likely words going first; write all
possible binary codes in another list, with increasing code
lengths. Match the two lists, and this is the optimal
encoding.

Let's try to get a more analytical grip of this: Let $l(x) = |c(x)|$
be the actual code length assigned to word $x$, which is an integer
value. Let's define 
\begin{equation}
q(x) = \frac{1}{Z}~ 2^{-l(x)}
\end{equation}
with the normalization constrant $Z=\sum_x 2^{-l(x)}$. Then we have
\begin{align}
\sum_{x\in X} p(x) [-\log_2 q(x)]
&= -\sum_x p(x) \log 2^{-l(x)} + \sum_x p(x) \log Z \\
&= \sum_x p(x) l(x) + \log Z ~.
\end{align}
What about $\log Z$? Let $l^\1(s)$ be the
set of words that have been assigned codes of length $l$. There can
only be a limited number of words encoded with a given length. For
instance, $|L^\1(1)|$ must not be greater than 2, $|L^\1(2)|$ must not
be greater than 4, and $|l^\1(s)|$ must not be greater than $2^l$. We
have
\begin{align}
&\forall_s:~ \sum_{x\in X} [l(x)=s] \le 2^s \\
&\forall_s:~ \sum_{x\in X} [l(x)=s]  2^{-s} \le 1 \\
&\forall_s:~ \sum_{x\in X} 2^{-l(x)} \le 1
\end{align}
However, this way of thinking is ok for separated codes. If such codes
would be in a continuous stream of bits you'd never know where a code
starts or ends. Prefix codes fix this problem by defining a code tree
with leaves that clearly define when a code ends. For prefix codes it
similarly holds 
\begin{equation}
Z = \sum_{x\in X} 2^{-l(x)} \le 1 ~,
\end{equation}
which is called \emph{Kraft's inequality}. That finally gives
\begin{equation}
\sum_{x\in X} p(x) [-\log_2 q(x)]
\le \sum_x p(x) l(x)
\end{equation}


\subsection{The Laplace approximation: A 2nd-order Taylor of
$\log p$}

Assume we want to estimate some $q(x)$ we cannot express
analytically. E.g., $q(x) =p(x|D) \propto P(D|x) p(x)$ for some
awkward likelihood function $p(D|x)$. An example from robotics is: $x$
is stochastically controlled path of a robot. $p(x)$ is a prior
distribution over paths that includes how the robot can actually move
and some Gaussian prior (squared costs!) over controlls. If the robot
is ``linear'', $p(x)$ can be expressed nicely and analytically; it if
it non-linear, expressing $p(x)$ is already hard. However, $p(D|x)$
might indicate that we do \emph{not} see collisions on the path---but
collisions are a horrible function, usually computed by some black-box
collision detection packages that computes distances between convex
meshes, perhaps giving gradients but certainly not some analytic
function. So $q(x)$ can clearly not be expressed analytically.

One way to approximate $q(x)$ is the Laplace approximation
\begin{myDefinition}[Laplace approximation]
Given a smooth distribution $q(x)$, we define its Laplace approximation as
\begin{equation}
\tilde q(x) = \exp\{ - \tilde E(x) \} ~, 
\end{equation}
where $\tilde E(x)$ is the 2nd-order Taylor expansion
\begin{equation}
\tilde E(x) = E(x^*) + \half (x-x^*)^\T \he E(x^*) (x-x^*)
\end{equation}
of the energy $E(x) = -\log q(x)$ at the mode
\begin{equation}
x^* = \argmin_x E(x) = \argmax_x q(x) ~.
\end{equation}
\end{myDefinition}

First, we observe that the Laplace approximation is a Gaussian,
because its energy is a parabola. Further, notice that in the Taylor
expansion we skipped the linear term. That's because we are at the
mode $x^*$ where $\na E(x^*)=0$.

The Laplace approximation really is the probabilistic analoque of a
local second-order approximation of a function, just as we used it in
Newton methods. However, it is defined to be taken specifically at the
mode of the distribution.

Now, computing $x^*$ is a classical optimization problem $x^*
= \argmin_x E(x)$ which one might ideally solve using Newton
methods. These Newton methods anyway compute the local Hessian of
$E(x)$ in every step---at the optimum we therefore have the Hessian
already, which is then the precision matrix of our Gaussian.

The Laplace approximation is nice, very efficient to use, e.g., in the
context of optimal control and robotics. While we can use the
expressive power of probability theory to formalize the problem, the
Laplace approximation brings us computationally back to efficient
optimization methods.

\subsection{Variational Inference}

Another reduction of inference to optimization is variational
inference.
\begin{myDefinition}[variational inference]
Given a distribution $p(x)$, and a parameterized family of
distributions $q(x|\b)$, the variational approximation of $p(x)$ is
defined as
\begin{equation}
\argmin_q \kld{q}{p}
\end{equation}
\end{myDefinition}


\subsection{The Fisher information metric: 2nd-order Taylor of the KLD}

Recall our notion of steepest descent---it depends on the metric in
the space!

Consider the space of probability distributions $p(x;\b)$
with parameters $\b$. We think of every  $p(x;\b)$ as a point in the
space and wonder what metric is useful to compare two points  $p(x;\b_1)$
and $p(x;\b_2)$. Let's take the KLD

TODO

: Let $p\in\L^X$, that is, p
is a probability distribution over the space $X$. Further, let
$\t\in\RRR^n$ and $\t \mapsto p(\t)$ is some parameterization of the
probability distribution. Then the derivative $d_\t p(\t) \in
T_p \L^X$ is a vector in the tangent space of $\L^X$. Now, for such
vectors, for tangent vectors of the space of probability
distributions, there is a generic metric, the \Def{Fisher metric}:
[TODO: move to 'probabilities' section]

\begin{comment}

\subsection{Bayesian Filtering}

\vspace*{-5mm}
\begin{align}
p_t
&(x_t) := P(x_t \| y_{0:t}, u_{0:t\1}) \\
&= c_t~ P(y_t \| x_t, y_{0:t\1}, u_{0:t\1})~ P(x_t \| y_{0:t\1}, u_{0:t\1})\\
&= c_t~ P(y_t \| x_t)~ P(x_t \| y_{0:t\1}, u_{0:t\1})\\
&= c_t~ P(y_t \| x_t)~ \int_{x_{t\1}} P(x_t,x_{t\1} \| y_{0:t\1}, u_{0:t\1})~ dx_{t\1}\\
&= c_t~ P(y_t \| x_t)~ \int_{x_{t\1}} P(x_t \| x_{t\1}, y_{0:t\1},
u_{0:t\1})~ P(x_{t\1} \| y_{0:t\1}, u_{0:t\1})~ dx_{t\1}\\
&= c_t~ P(y_t \| x_t)~ \int_{x_{t\1}} P(x_t \| x_{t\1}, u_{t\1})~ P(x_{t\1} \| y_{0:t\1}, u_{0:t\1})~ dx_{t\1}\\
&= c_t~ {\color{red}P(y_t \| x_t)}~ \int_{x_{t\1}} {\color{red}P(x_t
  \| u_{t\1}, x_{t\1})}~ p_{t\1}(x_{t\1})~ dx_{t\1}
\end{align}


\subsubsection{Kalman filter}

\begin{align}
&\quad p_t(x_t) = \NN(x_t \| s_t, S_t) \\
&\quad P(y_t \| x_t) = \NN(y_t \| C x_t + c, W) \\
&\quad P(x_t \| u_{t\1}, x_{t\1}) = \NN(x_t \| A x_{t\1} + a, Q) \\
p_t&(x_t) \propto
P(y_t \| x_t) \int_{x_{t\1}} P(x_t \| u_{t\1}, x_{t\1})~ p_{t\1}(x_{t\1})~ dx_{t\1} \\
&= \NN(y_t \| C x_t + c, W)~ \int_{x_{t\1}}
 \NN(x_t \| A x_{t\1} + a, Q)~
 \NN(x_{t\1} \| s_{t\1}, S_{t\1})~ dx_{t\1} \\
&= \NN(y_t \| C x_t + c, W)~
 \NN(x_t \| \underbrace{A s_{t\1} + a}_{=:\hat
 s_t},~ \underbrace{Q + A S_{t\1} A^\T}_{=:\hat S_t}) \\
&= \NN(C x_t + c \| y_t , W)~
 \NN(x_t \| \hat s_t, \hat S_t) \\
&= \NN[x_t \| C^\T W^\1(y_t-c) ,~ C^\T W^\1 C] ~
 \NN(x_t \| \hat s_t, \hat S_t) \\
&= \NN(x_t \| s_t, S_t) \cdot\<\text{terms indep. of $x_t$}\> \\
S_t
&= (C^\T W^\1 C+\hat S_t^\1)^\1
 = \hat S_t
 - \underbrace{\hat S_t C^\T(W+C \hat S_t C^\T)^\1}_{\text{``Kalman gain'' $K$}} C \hat S_t \\
s_t
&= S_t [C^\T W^\1(y_t-c) + \hat S_t^\1 \hat s_t]
 = \hat s_t + K(y_t-C \hat s_t-c)
\end{align}
The second to last line uses the general Woodbury identity.

The last line uses $S_t C^\T W^\1=K$ and $S_t\hat S_t^\1=\Id-K C$

\end{comment}

\subsection{Examples and Exercises}

\input{e10-probabilities.tex}
\input{e11-probabilities2.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exsection{Maximum Entropy and ML}

(These are taken from MacKay's book \emph{Information Theory...},
Exercise 22.12 \& .13)


a) Assume that a random variable $x$ with discrete domain
$\text{dom}(x)=\XX$ comes from a probability distribution of the
form
\begin{equation}
P(x\|w) = \frac{1}{Z(w)}~ \exp\[\sum_{k=1}^d w_k f_k(x)\] ~,
\end{equation}
where the functions $f_k(x)$ are given, and the parameters $w\in\RRR^d$ are
not known. A data set $D=\{x_i\}_{i=1}^n$ of $n$ points $x$ is supplied.
Show by differentiating the log likelihood $\log P(D|w) = \sum_{i=1}^n
\log P(x_i|w)$ that the maximum-likelihood
parameters $w^* = \argmax_w \log P(D|w)$ satisfy
\begin{equation}
\sum_{x\in\XX} P(x\|w^*)~ f_k(x) = \frac{1}{n}~ \sum_{i=1}^n
f_k(x_i)
\end{equation}
where the left-hand sum is over all x, and the right-hand sum is over the
data points. A shorthand for this result is that each function-average
under the fitted model must equal the function-average found in the
data:
\begin{equation}
\<f_k\>_{P(x\|w^*)} = \<f_k\>_D
\end{equation}

b) When confronted by a probability distribution $P(x)$ about which only a
few facts are known, the maximum entropy principle (MaxEnt) offers a
rule for choosing a distribution that satisfies those constraints. According
to MaxEnt, you should select the $P(x)$ that maximizes the entropy
\begin{equation}
H(P) = -\sum_x P(x) \log P(x)
\end{equation}
subject to the constraints. Assuming the constraints assert that the
averages of certain functions $f_k(x)$ are known, i.e.,
\begin{equation}
\<f_k\>_{P(x)} = F_k ~,
\end{equation}
show, by introducing Lagrange multipliers (one for each constraint, including
normalization), that the maximum-entropy distribution has the
form
\begin{equation}
P_{\text{MaxEnt}}(x) = \frac{1}{Z}~ \exp\[ \sum_k w_k~ f_k(x) \]
\end{equation}
where the parameters $Z$ and $w_k$ are set such that the constraints
are satisfied. And hence the maximum entropy method gives identical
results to maximum likelihood fitting of an exponential-family model.






\appendix





%% \section{Relation between covariant gradient and the Newton method}
%% \label{secNewton}

%% The Newton method computes the 2nd order approximation (called 2nd
%% order Taylor expansion) of the function:
%% \begin{align}
%% f(x_0+\d) \approx f(x_0) + \frac{\del f(x_0)}{\del x} \d + \d^\T H \d
%% \end{align}
%% The matrix $H$ is called Hessian and, intuitively, describes the local
%% $n$-dimensional parabola curvature that approximates $f$. If $H$ is
%% positive-definite, the parabola is indeed positively curved, meaning
%% that it goes \emph{up} in all directions. (Would $H$ have a negative
%% eigenvalue it would describe a saddle-like function, going down along
%% the eigen-vectors.) If $H$ is positive-definite the parabola has a
%% definite minimum
%% \begin{align}
%% \d^* = - H^\1~ \frac{\del f(x_0)}{\del x}^\T ~.
%% \end{align}
%% The Newton method iterates exactly this step: It computes a local 2nd
%% order approximation of $f$, jumps to the minimum of this parabola, and
%% iterates from there.

%% This is exactly the same as covariant gradient descent,
%% but with the local Hessian $H$ replacing the metric $A$. A difference
%% is that the Hessian $H$ of a function is \emph{local} and typically
%% different for any point in space, whereas the metric is usually
%% assumed constant throughout the space.


\section{Gaussian identities}

\renewcommand{\-}{\myminus}
\newcommand{\+}{\myplus}
\renewcommand{\T}{{\!\top\!}}
\renewcommand{\mT}{{\myminus{}\top}}

\paragraph{Definitions}~\\
We define a Gaussian over $x$ with mean $a$ and covariance matrix
$A$ as the function
\begin{align}
\NN(x \| a,A) &= \frac{1}{|2\pi A|^{1/2}}~ \exp\{-\half (x\-a)^\T~ A^\1~ (x\-a)\}
\end{align}
with property $N(x \| a,A) = N(a |\ x,A)$. We also define the
canonical form with precision matrix $A$ as
\begin{align}
\NN[x \| a,A]
 = \frac{\exp\{-\half a^\T A^\1a\}}{|2\pi A^\1|^{1/2}}~
   \exp\{-\half x^\T~ A~ x + x^\T a\}
\end{align}
with properties
\begin{align}
& \NN[x \| a,A] = \NN(x \| A^\1 a,A^\1) \\
& \NN(x \| a,A) = \NN[x \| A^\1 a,A^\1] ~.
\end{align}
Non-normalized Gaussian
\begin{align}
\oNN(x,a,A)
 &= |2\pi A|^{1/2}~ \NN(x|a,A) \\
 &= \exp \{-\half (x\-a)^\T~ A^\1~ (x\-a)\}
\end{align}

\paragraph{Matrices}
[matrix cookbook: {\tiny\url{http://www.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf}}]
\renewcommand{\bar}{\widehat}
\begin{align}
&(A^\1 + B^\1)^\1 = A~ (A\+B)^\1~ B = B~ (A\+B)^\1~ A \\
&(A^\1 - B^\1)^\1 = A~ (B\-A)^\1~ B \\
&\del_x |A_x| = |A_x|~ \tr(A_x^\1~ \del_x A_x) \\
&\del_x A_x^\1 = - A_x^\1~ (\del_x A_x)~ A_x^\1 \\
&(A+UBV)^\1 = A^\1 - A^\1 U (B^\1 + VA^\1U)^\1 V A^\1 \label{wood}\\
&(A^\1+B^\1)^\1 = A - A (B + A)^\1 A \\
&(A + J^\T B J)^\1 J^\T B 
= A^\1 J^\T (B^\1 + J A^\1 J^\T)^\1 \label{wood2}\\
&(A + J^\T B J)^\1 A
= \Id - (A + J^\T B J)^\1 J^\T B J  \label{null}
\end{align}
(\ref{wood})=Woodbury; (\ref{wood2},\ref{null}) holds for pos def $A$ and $B$

\paragraph{Derivatives}
\begin{align}
& \del_x \NN(x|a,A) = \NN(x|a,A)~ (-h^\T)  \comma h:= A^\1(x\-a)\\
& \del_\t \NN(x|a,A)
 = \NN(x|a,A) ~\cdot \feed
&\[- h^\T(\del_\t x) 
   + h^\T (\del_\t a)
   - \half \tr(A^\1~ \del_\t A)
   + \half h^\T (\del_\t A) h \] \\
& \del_\t \NN[x|a,A]
 = \NN[x|a,A] ~\[ -\half x^\T \del_\t A x + \half a^\T A^\1 \del_\t A A^\1 a \feed
&+ x^\T \del_\t a - a^\T A^\1 \del_\t a + \half\tr(\del_\t A A^\1) \]
 \\
& \del_\t \oNN_x(a,A) = \oNN_x(a,A) ~\cdot \feed
& \[ h^\T(\del_\t x)
   + h^\T (\del_\t a)
   + \half h^\T (\del_\t A) h \]
\end{align}

\paragraph{Product}~\\
The product of two Gaussians can be expressed as
\begin{align}
&\NN(x \| a,A)~ \NN(x \| b,B) \feed
 &= \NN[x \| A^\1 a+B^\1 b, A^\1 + B^\1]~ \NN(a\|b,A+B) ~, \label{prodNat}\\
 &= \NN(x \| B(A\+B)^\1a + A(A\+B)^\1b ,A(A\+B)^\1B)~ \NN(a\|b,A+B) ~,\\
&\NN[x \| a,A]~ \NN[x \| b,B] \feed
 &= \NN[x \| a+b,A+B]~ \NN(A^\1 a \| B^\1 b, A^\1+B^\1) \\
 &= \NN[x|\dots]~ \NN[ A^\1 a \| A(A\+B)^\1 b, A(A\+B)^\1 B]\\
 &= \NN[x|\dots]~ \NN[ A^\1 a \| (1\-B(A\+B)^\1)~ b,~ (1\-B(A\+B)^\1)~ B] ~,\\
&\NN(x \| a,A)~ \NN[x \| b,B] \feed
 &= \NN[x \| A^\1 a+ b, A^\1 + B]~ \NN(a\|B^\1 b,A+B^\1) \\
 &= \NN[x|\dots]~ \NN[a\|(1\-B(A^\1\+B)^\1)~ b,~ (1\-B(A^\1\+B)^\1)~
 B] \label{prodNatCan}
\end{align}

\paragraph{Convolution}
\begin{align}
\textstyle\int_x \NN(x \|a,A)~ \NN(y-x \| b,B)~ dx
 &= \NN(y \| a+b, A+B)
\end{align}

\paragraph{Division}
\begin{align}
\NN(x|&a,A) ~\big/~ \NN(x|b,B) = \NN(x|c,C) ~\big/~ \NN(c| b, C+B) \feed
 & C^\1c = A^\1a - B^\1b \feed
 & C^\1 = A^\1 - B^\1 \\
\NN[x|&a,A] ~\big/~ \NN[x|b,B] \propto \NN[x|a-b,A-B]
\end{align}

\paragraph{Expectations}~\\
Let $x\sim\NN(x\|a,A)$,
\begin{align}
&\Exp{x}{g(x)} := \textstyle\int_x \NN(x \| a,A)~ g(x)~ dx \\
%&\Exp{x}{g(f+Fx)} = 
&\Exp{x}{x} = a \comma \Exp{x}{x x^\T} = A + a a^\T\\
&\Exp{x}{f+Fx} = f+Fa \\
&\Exp{x}{x^\T x} = a^\T a + \tr(A)\\
&\Exp{x}{(x\-m)^\T R(x\-m)} = (a\-m)^\T R(a\-m) + \tr(RA)
\end{align}

\paragraph{Transformation}~
Linear transformations imply the following identities,
\begin{align}
& \NN(x\|a,A) = \NN(x+f\|a+f,~A) \comma
  \NN(x\|a,A) = |F|~ \NN(Fx \| Fa,~FAF^\T) \\
& \NN(F x + f \| a,A)
 = \frac{1}{|F|}~ \NN(x \| ~ F^\1 (a-f),~ F^\1 AF^\mT) \\
&= \frac{1}{|F|}~ \NN[x \| ~ F^\T A^\1 (a-f),~ F^\T A^\1 F] ~, \\
& \NN[F x + f \| a,A] = \frac{1}{|F|}~ \NN[x \| ~ F^\T(a-Af),~ F^\T A F] ~.
\end{align}

\paragraph{``Propagation''}~ (propagating a message along a coupling, using
 eqs (\ref{prodNat}) and (\ref{prodNatCan}), respectively)\\
\begin{align}
& \textstyle\int_y \NN(x \| a + Fy, A)~ \NN(y \| b, B)~ dy
 = \NN(x \| a + Fb, A+FBF^\T) \\
& \textstyle\int_y \NN(x \| a + Fy, A)~ \NN[y \| b, B]~ dy
 = \NN[x \| (F^\mT\-K)(b+BF^\1a),~ (F^\mT\-K)BF^\1] ~,\\
 &\quad K=F^\mT B(F^\mT A^\1 F^\1\+B)^\1
\end{align}

%\begin{align}
%& x' = F x + f \\
%& \NN(x|a,A) = |F|~ \NN(Fx+f|~ Fa+f,~ FAF^\T) \\
%& \NN(F x + f|a,A) = \frac{1}{|F|}~ \NN(x|~ F^\1(a-f),~ F^\1AF^{-1T}) \\
%& \NN[F x + f|a,A] = \frac{1}{|F|}~ \NN[x|~ F^\T(a-Af),~ F^\T A F] \\
%& P(x) = |F|~ P(x'=F x + f) \comma P(x') = \frac{1}{|F|}~ P(x=F^\1(x' - f))
%\end{align}
%If a forward dependency $P(y|x)$ is given as a linear noise transition
%$(f,F,Q)$ and if evidence $y^*$ is given, this induces a potential on
%$x$:
%\begin{align}
%\NN(y| Fx+f,Q) = \NN(Fx+f| y,Q)
% = U(x) \propto \NN(x|F^\1(y-f),~ F^\1 Q F^{-1T})
%\end{align}

\paragraph{marginal \& conditional:}
\begin{align}
\NN(x \| a,A)~ \NN(y \| b+Fx,B)
 &= \NN\bigg( \arr{c}{x\\ y} \bigg| \arr{c}{a\\ b+Fa} ,~
             \arr{cc}{A & A^\T F^\T\\ F A & B\!+\!F A^\T F^\T} \bigg) \\
%
\NN\bigg( \arr{c}{x\\ y} \bigg| \arr{c}{a\\ b} ,~ 
         \arr{cc}{A & C\\ C^\T & B} \bigg)
&= \NN(x \| a,A) \cdot \NN(y \| b+C^\T A^\1(x\-a),~ B - C^\T A^\1 C) \\
%
\NN[ x \| a,A ]~ \NN(y \| b+Fx,B )
 &= \NN\bigg[ \arr{c}{x\\ y} \bigg| \arr{c}{a+F^\T B^\1 b \\ B^\1 b} ,~
             \arr{cc}{A+F^\T B^\1 F & -F^\T B^\1 \\ -B^\1 F & B^\1} \bigg] \\
%
\NN[x \| a,A ]~ \NN[y \| b+Fx,B ]
 &= \NN\bigg[ \arr{c}{x\\ y} \bigg| \arr{c}{a+F^\T B^\1 b \\ b} ,~
             \arr{cc}{A+F^\T B^\1  F & -F^\T \\ -F & B} \bigg] \\
%
\NN\bigg[ \arr{c}{x\\ y} \bigg| \arr{c}{a\\ b} ,~ 
         \arr{cc}{A & C\\ C^\T & B} \bigg]
&= \NN[x \| a - C B^\1 b,~ A - C B^\1 C^\T] \cdot \NN[y \| b-C^\T x,B] \\
\de{A}{C}{D}{B}
 &= |A|~ |\bar B| = |\bar A|~ |B| ~,
 \text{where } \arr{l}{ \bar A = A - C B^\1 D \\ \bar B = B - D A^\1 C }\\
\ma{A}{C}{D}{B}^\1
 &= \ma{\bar A^\1}{-A^\1 C \bar B^\1}{-\bar B^\1 D A^\1}{\bar B^\1} \\
 &= \ma{\bar A^\1}{-\bar A^\1 C B^\1}{-B^\1 D \bar A^\1}{\bar B^\1}
\end{align}

\paragraph{pair-wise belief} We have a message $\a(x)=\NN[x|s,S]$,
 transition $P(y|x) = \NN(y|A x+a,Q)$, and a message
 $\b(y)=\NN[y|v,V]$, what is the belief $b(y,x)=\a(x)P(y|x)\b(y)$?
\begin{align}
b(y,x)
 &= \NN[x|s,S]~ \NN(y|A x+a,Q^\1)~ \NN[y|v,V] \\
&=
\NN\bigg[ \arr{c}{x\\ y} \bigg| \arr{c}{s \\ 0} ,~
             \arr{cc}{S & 0 \\ 0 & 0} \bigg]~
\NN\bigg[ \arr{c}{x\\ y} \bigg| \arr{c}{A^\T Q^\1 a \\ Q^\1 a} ,~
             \arr{cc}{A^\T Q^\1 A & -A^\T Q^\1 \\ -Q^\1 A & Q^\1} \bigg]~
\NN\bigg[ \arr{c}{x\\ y} \bigg| \arr{c}{0 \\ v} ,~
             \arr{cc}{0 & 0 \\ 0 & V} \bigg] \\
&\propto
\NN\bigg[ \arr{c}{x\\ y} \bigg| \arr{c}{s + A^\T Q^\1 a\\ v + Q^\1 a} ,~
             \arr{cc}{S + A^\T Q^\1 A & -A^\T Q^\1 \\ -Q^\1 A & V+Q^\1} \bigg]
\end{align}

\paragraph{Entropy}
\begin{align}
H(\NN(a,A)) &= \half \log |2\pi e A|
\end{align}

\paragraph{Kullback-Leibler divergence}
\begin{align}
&p=\NN(x|a,A) \comma q=\NN(x|b,B) \comma n = \text{dim}(x)
 \comma \kld{p}{q} = \sum_x p(x) \log\frac{p(x)}{q(x)} \\
&2~ \kld{p}{q}
= \log\frac{|B|}{|A|} + \tr(B^\1A) + (b-a)^\T B^\1 (b-a) - n \\
&4~ \kld[\text{sym}]{p}{q}
= \tr(B^\1A) + \tr(A^\1B) + (b-a)^\T (A^\1+B^\1) (b-a) - 2n
\end{align}

$\l$-divergence
\begin{align}
2~ \kld[\l]{p}{q}
&= \l~ \kld{p}{\l p+(1\!-\!\l)q} ~+~ (1\!-\!\l)~ \kld{p}{(1\!-\!\l) p + \l q}
\end{align}

For $\l=.5$: Jensen-Shannon divergence.

\paragraph{Log-likelihoods}
\begin{align}
\log \NN(x|a,A)
 &= - \half \[ log|2\pi A| + (x\-a)^\T~ A^\1~ (x\-a) \] \\
\log \NN[x|a,A]
 &= - \half \[ log|2\pi A^\1| + a^\T A^\1 a + x^\T A x - 2 x^\T a \] \\
\sum_x \NN(x|b,B) \log \NN(x|a,A)
 &= -\kld{\NN(b,B)}{\NN(a,A)} - H(\NN(b,B))
\end{align}

\paragraph{Mixture of Gaussians}~
Collapsing a MoG into a single Gaussian
\begin{align}
&\argmin_{b,B} \kld{\sum_i p_i~ \NN(a_i,A_i)}{\NN(b,B)} \\
&=\quad\(
b=\sum_i p_i a_i ~,~
B=\sum_i p_i (A_i + a_i a_i^\T - b\, b^\T)\)
\end{align}

%% THAT STUFF IS WRONG!!
%% Marginal of a MOG
%% \begin{align}
%% &P(x,y) = \sum_i p_i~ \NN(\ve{x}{y}| \ve{a_i}{b_i},\ma{A_i}{C_i}{C_i^\T}{B_i})
%% \feed
%% &P(y|x) = \sum_i p_i~ \NN(y| b_i + C_i^\T A_i^\1(x-a_i),~ B_i - C_i^\T
%% A_i^\1 C_i^\T) \\
%% &\approx \NN(y|e,E) \comma e=\sum_i p_i (b_i + C_i^\T A_i^\1(x-a_i))
%% \comma \feed 
%% &E = \sum_i p_i \[ B_i - C_i^\T
%% A_i^\1 C_i^\T + b_i b_i^\T + C_i^\T A_i^\1(x-a_i)(x-a_i)^\T
%% A_i^\1{}^\T C_i + 2 C_i^\T A_i^\1(x-a_i) b^\T - e e^\T\] \\
%% & F = - \sum_i p_i~ C_i^\T A_i^\1 \comma
%%   f =   \sum_i p_i~ (b_i - C_i^\T A_i^\1 a_i) \comma
%%   Q = ? \\
%% %
%% &P(x,y) = \sum_i p_i~ \NN[\ve{x}{y}| \ve{a_i}{b_i},\ma{A_i}{C_i}{C_i^\T}{B_i}]
%% \feed
%% &P(y|x) = \sum_i p_i~ \NN[y| b_i - C_i^\T x,~ B_i ] \\
%% &\approx \NN(y|e,E) \comma
%%  E=\sum_i p_i (B_i^\1 + B^\1 (b_i - C_i^\T x)(b_i - C_i^\T x)^\T
%%  B^\1{}^\T - e\, e^\T) \comma \feed 
%% &e = \sum_i p_i~ \[ B_i^\1 (b_i - C_i^\T x) \] \\
%% & F = - \sum_i p_i~ B_i^\1 C_i^\T \comma
%%   f =   \sum_i p_i~ B_i^\1 b_i \comma
%%   Q = ?
%% \end{align}

%\paragraph{Kalman filter (fwd) equations}
%\begin{align}
%& x'|x \sim \{F~x+f,~ Q\} \comma y|x \sim \{C~x,~ R\} \\
%& x'|y',x \sim \{F~x+f, (Q^\1 + C^\T R^\1 C)^\1 C^\T R^\1
%(y'-C \hat x)\}
%\end{align}

%\paragraph{linear fwd-bwd equations without observations}
%\begin{align}
%& \a_t(x) = \NN(a_t,A_t) = P(x | \text{start})
%  \comma \b(x) = \oNN(b_t,B_t) = P(\text{goal}|x) \\ 
%& a_{t+1} = F~ a_t + f
%  \comma A_{t+1} = F A_t F^\T + Q \\
%& b_{\tau+1} = F^\1(b_t-f)
%  \comma B_{\tau+1} = F^\1 (B_\tau + Q) F^{-1T} \\
%&\text{( truely: $\b_{\tau+1} = \frac{1}{|F|}~ \NN(b_\tau,B_\tau)$ )}
%\end{align}

%\paragraph{non-linear fwd-bwd equations without observations}
%\begin{align}
%&P(x'|x) = \NN(x'| \phi(x),Q) \\
%&\a_t(x) = \NN(x|a_t,A_t) \comma (a_t,A_t) = UT_\phi(a_{t-1},A_{T-1}) + (0,Q) \\
%&\b_t(x) = \oNN_x(b_t,B_t) \comma (b_t,B_t) =
% UT_{\phi-1}(b_{t-1},B_{T-1} + Q) \\
%&Z_{t,\tau} = |2\pi B_\tau|^{1/2}~ \NN(a_t|b_\tau,A_t + B_\tau) \cdot
%\NN(\cdots?\cdots)\\
%&\g_{t,\tau}(x) = \NN(x|c_t,C_t)
% \comma C_{t,\tau} = A_t~ (A_t + B_\tau)^\1~ B_\tau
% \comma c_{t,\tau} = C_{t,\tau}~ (A_t^\1~ a_t + B_\tau^\1~ b_\tau) ~.
%\end{align}

%\paragraph{action selection - simple control}
%\begin{align}
%& P(y|x,u) = \NN(x+u,Q) \\
%& P(r \| u,x) = P(r\|u) + (1-P(r\|u))~ P(r\|x) ??\\
%& q_\tau(x,u) = \textstyle\int_y \NN(y|x+u,Q)~ \oNN_y(b,B)
% = |2\pi B|^{1/2}~ \NN(u| b-x, B + Q) \\
%& \argmax{u} q_\tau(x,u) = b-x
%\end{align}

%\paragraph{action selection - noisy control}
%\begin{align}
%& P(y|x,u) = \NN(x+u,u^2 Q) \\
%& q_\tau(x,u) = \textstyle\int_y \NN(y|x+u,Q)~ \NN(y|b,B) = \NN(u| b-x, B + u^2 Q) \\
%%& \argmax{u} q_\tau(x,u) = b-x
%\end{align}

%[[todo: unscented transform]


\section{3D geometry basics (for robotics)}

This document introduces to some basic geometry, focussing on 3D
transformations, and introduces proper conventions for notation. There
exist one-to-one implementations of the concepts and equations in
libORS.

\subsection{Rotations}

There are many ways to represent rotations in $SO(3)$. We restrict
ourselves to three basic ones: rotation matrix, rotation vector, and
quaternion. The rotation vector is also the most natural
representation for a ``rotation velocity'' (angular velocities). Euler
angles or raw-pitch-roll are an alternative, but they have
singularities and I don't recommend using them in practice.

\begin{description}
\item[A rotation matrix] is a matrix $R\in\RRR^{3\times3}$ which is orthonormal
(columns and rows are orthogonal unit vectors, implying determinant
1). While a $3\times3$ matrix has 9 degrees of freedom (DoFs), the
constraint of orthogonality and determinant 1 constraints this: The
set of rotation matrices has only 3 DoFs ($\sim$ the local Lie algebra
is 3-dim).

The application of $R$ on a vector $x$ is simply the matrix-vector
product $R x$.

Concatenation of two rotations $R_1$ and $R_2$ is the normal
matrix-matrix product $R_1 R_2$.

Inversion is the transpose, $R^\1 = R^\T$.

\item[A rotation vector] is an unconstraint vector $w\in\RRR^3$. The
vector's direction $\ul w = \frac{w}{|w|}$ determines the rotation
axis, the vector's length $|w|=\t$ determins the rotation angle (in
radians, using the right thumb convention).

The application of a rotation described by $w\in\RRR^3$ on a vector
$x\in\RRR^3$ is given as (Rodrigues' formula)
\begin{align}
w \cdot x
 &= \cos\t~ x
  + \sin\t~ (\ul w\times x)
  + (1-\cos\t)~ \ul w(\ul w^\T x)
\end{align}
where $\t=|w|$ is the rotation angle and $\ul w=w/\t$ the unit length
rotation axis.

The inverse rotation is described by the negative of the rotation
vector.

Concatenation is non-trivial in this representation and we don't
discuss it here. In practice, a rotation vector is first converted to
a rotation matrix or quaternion.

Convertion to a matrix: For every vector $w\in\RRR^3$ we define its skew
symmetric matrix as
\begin{align}
\hat w = \mat{ccc}{0 & -w_3 & w_2 \\ w_3 & 0 & -w_1 \\-w_2 & w_1 & 0} ~.
\end{align}
Note that such skew-symmetric matrices are related to the cross
product: $w \times v = \hat w~ v$, where the cross product is
rewritten as a matrix product. The rotation matrix $R(w)$ that
corresponds to a given rotation vector $w$ is:
\begin{align}\label{eqRodriguez}
R(w)
 &= \exp(\hat w) \\
 &= \cos\t~ I + \sin\t~ \hat w/\t + (1-\cos\t)~ w w^\T/\t^2
\end{align}
The $\exp$ function is called exponential map (generating a group
element (=rotation matrix) via an element of the Lie algebra (=skew
matrix)). The other formular is called Rodrigues' formular: the first
term is a diagonal matrix ($I$ is the 3D identity matrix), the second
terms the skew symmetric part, the last term the symmetric part ($w
w^\T$ is also called outper product).

\item[Angular velocity \& derivative of a rotation matrix:] We
represent angular velocities by a vector $w\in\RRR^3$, the direction
$\ul w$ determines the rotation axis, the length $|w|$ is the rotation
velocity (in radians per second). When a body's orientation at time
$t$ is described by a rotation matrix $R(t)$ and the body's angular
velocity is $w$, then
\begin{align}\label{eqDotR}
\dot R(t) = \hat w~ R(t)~.
\end{align}
(That's intuitive to see for a rotation about the $x$-axis with
velocity 1.) Some insights from this relation: Since $R(t)$ must
always be a rotation matrix (fulfill orthogonality and determinant 1),
its derivative $\dot R(t)$ must also fulfill certain constraints; in
particular it can only live in a 3-dimensional sub-space. It turns out
that the derivative $\dot R$ of a rotation matrix $R$ must always be a
skew symmetric matrix $\hat w$ times $R$ -- anything else would be
inconsistent with the contraints of orthogonality and determinant 1.

Note also that, assuming $R(0)=I$, the solution to the differential
equation $\dot R(t) = \hat w~ R(t)$ can be written as
$R(t)=\exp(t \hat w)$, where here the exponential function notation
is used to denote a more general so-called exponential map, as used in
the context of Lie groups. It also follows that $R(w)$ from
(\ref{eqRodriguez}) is the rotation matrix you get when you rotate for
1 second with angular velocity described by $w$.

\item[Quaternion] (I'm not describing the general definition, only the
``quaternion to represent rotation'' definition.) A quaternion is a
unit length 4D vector $r\in\RRR^4$; the first entry $r_0$ is related
to the rotation angle $\t$ via $r_0=\cos(\t/2)$, the last three
entries $\bar r\equiv r_{1:3}$ are related to the unit length rotation
axis $\ul w$ via $\bar r = \sin(\t/2)~ \ul w$.

The inverse of a quaternion is given by negating $\bar r$, $r^\1 =
(r_0,-\bar r)$ (or,
alternatively, negating $r_0$).

The concatenation of two rotations $r$, $r'$ is given as the quaternion
product
\begin{align}\label{eqQuat}
r \circ r'
 = (r_0 r'_0 - \bar r^\T \bar r',~
    r_0 \bar r' + r'_0 \bar r + \bar r' \times \bar r)
\end{align}

The application of a rotation quaternion $r$ on a vector $x$ can be expressed
by converting the vector first to the quaternion $(0,x)$, then computing
\begin{align}
r \cdot x = (r \circ (0,x) \circ r^\1)_{1:3} ~,
\end{align}
I think a bit more efficient is to first convert the rotation
quaternion $r$ to the equivalent rotation matrix $R$, as given by
\begin{align}
R
 &= \mat{ccc}{
    1-r_{22}-r_{33} & r_{12}-r_{03} &    r_{13}+r_{02} \\
    r_{12}+r_{03} &   1-r_{11}-r_{33} &  r_{23}-r_{01} \\
    r_{13}-r_{02} &   r_{23}+r_{01} &    1-r_{11}-r_{22}
    } \feed & ~ r_{ij} := 2 r_i r_j ~.
\end{align}
(Note: In comparison to (\ref{eqRodriguez}) this does not require to
compute a $\sin$ or $\cos$.) Inversely, the quaterion $r$ for a given
matrix $R$ is
\begin{align}
    r_0 &= \half\sqrt{1+\tr R}\\
    r_3 &= (R_{21}-R_{12})/(4 r_0)\\
    r_2 &= (R_{13}-R_{31})/(4 r_0)\\
    r_1 &= (R_{32}-R_{23})/(4 r_0) ~.
\end{align}

\item[Angular velocity $\to$ quaternion velocity] Given an
    angular velocity $w\in\RRR^3$ and a current quaterion
    $r(t)\in\RRR$, what is the time derivative $\dot r(t)$ (in analogy
    to Eq.~(\ref{eqDotR}))? For simplicity, let's first assume
    $|w|=1$. For a small time interval $\d$, $w$ generates a
    rotation vector $\d w$, which converts to a quaterion
\begin{align}
\Delta r = (\cos(\d/2), \sin(\d/2) w) ~.
\end{align}
    That rotation is concatenated LHS to the original quaternion,
\begin{align}
r(t+\d)
 = \Delta r \circ r(t) ~.
%%  \\
%% &\vspace*{-5mm}= (\cos(\d/2) r_0 - \sin(\d/2) w^\T \bar r,~
%%     \cos(\d/2) \bar r + \sin(\d/2)[r_0 w + \bar r \times w])
\end{align}
   Now, if we take the derivative w.r.t.\ $\d$ and evaluate it at
   $\d=0$, all the $\cos(\d/2)$ terms become $-\sin(\d/2)$ and evaluate
   to zero, all the $\sin(\d/2)$ terms become $\cos(\d/2)$ and evaluate
   to one, and we have
\begin{align}
%% \dot r(t+\d)
%% &= \half(-\sin(\d/2) r_0 - \cos(\d/2) w^\T \bar r,~
%%     -\sin(\d/2) \bar r + \cos(\d/2)[r_0 w + \bar r \times w])\\
\dot r(t)
&= \half( - w^\T \bar r,~  r_0 w + \bar r \times w )
 = \half (0,w) \circ r(t)
\end{align}
Here $(0,w)\in\RRR^4$ is a four-vector; for $|w|=1$ it is a normalized
quaternion. However, due to the linearity the equation holds for
any $w$.

\item[Quaternion velocity \protect$\to$ angular velocity] The following is
relevant when taking the derivative w.r.t.\ the parameters of a
quaternion, e.g., for a ball joint represented as quaternion. Given
$\dot r$, we have
\begin{align}
\dot r \circ r^\1
&= \half (0,w) \circ r \circ r^\1 = \half (0,w)
\end{align}
which allows us to read off the angular velocity induced by a change of
quaternion. However, the RHS zero will hold true only iff $\dot
r$ is orthogonal to $r$ (where $\dot r^\T r = \dot r_0 r_0 + \dot {\bar
r^\T} \bar r = 0$, see \refeq{eqQuat}). In case $\dot r^\T r \not=0$,
the change in length of the quaterion does not represent any angular
velocity; in typical kinematics engines a non-unit length is
ignored. Therefore one first orthogonalizes $\dot
r \gets \dot r - r(\dot r^\T r)$.

As a special case of application, consider computing the partial
derivative w.r.t.\ quaternion coordinates, where $\dot r$ is the unit
vectors $e_0,..,e_3$. In this case, the orthogonalization becomes
simply $e_i \gets e_i - r r_i$ and
\begin{align}
(e_i - r_i r) \circ r^\1
&= e_i \circ r^\1 - r_i (1,0,0,0) \\
w_i
&= 2 [e_i \circ r^\1]_{1:3} ~,
\end{align}
where $w_i$ is the rotation vector implied by $\dot r = e_i$. In case
the original quaternion $r$ wasn't normalized (which could be, if a
standard optimization algorithm searches in the quaternion
configuration space), then $r$ actually represents the normalized
quaternion $\bar r = \frac{1}{\sqrt{r^2}} r$, and (due to linearity of the above), the rotation vector implied by $\dot r = e_i$ is
\begin{align}
w_i
&= \frac{2}{\sqrt{r^2}}~ [e_i \circ r^\1]_{1:3} ~.
\end{align}
\end{description}



\subsection{Transformations}

We consider two types of transformations here: either static
(translation+rotation), or dynamic
(translation+velocity+rotation+angular velocity). The first maps
between two static reference frames, the latter between moving
reference frames, e.g. between reference frames attached to moving
rigid bodies.

\subsubsection{Static transformations}

Concerning the static transformations, again there are different
representations:
\begin{description}
\item[A homogeneous matrix] is a $4\times 4$-matrix of the form
\begin{align}
T = \mat{cc}{R & t \\ 0 & 1}
\end{align}
where $R$ is a $3\times 3$-matrix (rotation in our case) and $t$ a
$3$-vector (translation).

In homogeneous coordinates, vectors $x\in\RRR^3$ are expanded to 4D
vectors $\mat{c}{x\\1} \in \RRR^4$ by appending a 1.

Application of a transform $T$ on a vector $x\in\RRR^3$ is then given
as the normal matrix-vector product
\begin{align}
x' = T \cdot x
 &= T~ \mat{c}{x \\ 1}
  = \mat{cc}{R & t \\ 0 & 1}~ \mat{c}{x \\ 1}
  = \mat{c}{Rx + t \\ 1} ~.
\end{align}
%% \emph{Note:} This equation motivates the standard convention that
%%  a pair $(R,t)$ of a rotation and translation is always to be
%%  understood as \emph{first} rotating \emph{then} translating.

Concatenation is given by the ordinary 4-dim matrix-matrix product.

The inverse transform is
\begin{align}
T^\1
 &= \mat{cc}{R & t \\ 0 & 1}^\1
  = \mat{cc}{R^\1 & -R^\1 t \\ 0 & 1}
\end{align}

\item[Translation and quaternion:] A transformation can efficiently be
  stored as a pair $(t,r)$ of a translation vector $t$ and a rotation
  quaternion $r$. Analogous to the above, the application of $(t,r)$
  on a vector $x$ is $x' = t + r\cdot x$; the inverse is $(t,r)^\1 =
  (-r^\1\cdot t, r^\1)$; the concatenation is $(t_1,r_1) \circ
  (t_2,r_2) = (t_1 + r_1\cdot t_2, r_1 \circ r_2)$.
\end{description}

\subsubsection{Dynamic transformations}

Just as static transformations map between (static) coordinate frames,
dynamic transformations map between moving (inertial) frames which
are, e.g., attached to moving bodies. A dynamic transformation is
described by a tuple $(t,r,v,w)$ with translation $t$, rotation $r$,
velocity $v$ and angular velocity $w$. Under a dynamic transform
$(t,r,v,w)$ a position and velocity $(x,\dot x)$ maps to a new
position and velocity $(x',\dot x')$ given as
\begin{align}
& x'=t + r\cdot x \\
& \dot x' = v + w \times (r\cdot x)+ r\cdot\dot x
\end{align}
(the second term is the additional linear velocity of $\dot x'$
arising from the angular velocity $w$ of the dynamic transform). The
concatenation $(t,r,v,w) = (t_1,r_1,v_1,w_1) \circ (t_2,r_2,v_2,w_2)$
of two dynamic transforms is given as
\begin{align}
& t = t_1 + r_1 \cdot t_2 \\
& v = v_1 + w_1 \times (r_1 \cdot t_2) + r_1 \cdot v_2 \\
& r = r_1 \circ r_2 \\
& w = w_1 + r_1 \cdot w_2
\end{align}
For completeness, the footnote\footnote{Transformation of accelerations:
\begin{align}
\dot v
 &= \dot v_1
      + \dot w_1 \times (r_1 \cdot t_2)
      + w_1 \times (w_1 \times (r_1 \cdot t_2))\feed
      &\quad+ 2\, w_1 \times (r_1 \cdot v_2)
      + r_1 \cdot \dot v_2 \\
\dot w
 &= \dot w_1 + w_1 \times (r_1 \cdot w_2) + r_1 \cdot \dot w_2
\end{align}
Used identities: for any vectors $a,b,c$ and rotation $r$:
\begin{align}
&r \cdot (a \times b) = (r \cdot a) \times (r \cdot b)\\
& a \times (b \times c) = b (a c) - c (ab) \\
&\del_t (r \cdot a) = w \times (r \cdot a) + r \cdot \dot a \\
%& \del_t (r_0 \circ r \cdot t)
% = w_0 \times (r_0 \circ r \cdot t)
% + r_0 \cdot \[w \times (r \cdot t) + r \cdot v\]
% = (w_0 + r_0 \cdot w) \times (r_0 \circ r \cdot t) + r_0 r \cdot v  \\
& \del_t (w \times a) = \dot w \times t + w \times \dot a
\end{align}
} also describes how
accelerations transform, including the case when the transform itself is
accelerating. The inverse $(t',r',v',w') = (t,r,v,w)^\1$ of a dynamic
transform is given as
\begin{align}
& t' = -r^\1 \cdot t \\
& r' =  r^\1 \\
& v' =  r^\1 \cdot (w \times t - v) \\
& w' = -r^\1 \cdot w
\end{align}


\begin{description}
\item[Sequences of transformations] by $T_{A\to
B}$ we denote the transformation from frame $A$ to frame
$B$. The frames $A$ and $B$ can be thought of coordinate frames
(tuples of an offset (in an affine space) and three local orthonormal
basis vectors) attached to two bodies $A$ and $B$. It holds
\begin{align}
T_{A\to C} = T_{A\to B} \circ T_{B\to C}
\end{align}
where $\circ$ is the concatenation described above. Let $p$ be a point
(rigorously, in the affine space). We write $p^A$ for the coordinates
of point $p$ relative to frame $A$; and $p^B$ for the
coordinates of point $p$ relative to frame $B$. It holds
\begin{align}
p^A = T_{A\to B}~ p^B ~.
\end{align}
\end{description}

\subsubsection{A note on affine coordinate frames}

Instead of the notation $T_{A\to B}$, other text books often use
notations such as $T_{AB}$ or $T^A_B$. A common question regarding
notation $T_{A\to B}$ is the following:
\begin{quote}
\emph{The notation $T_{A\to B}$ is
confusing, since it transforms coordinates from frame $B$ to frame
$A$. Why not the other way around?}
\end{quote}
I think the notation $T_{A\to B}$ is intuitive for the following
reasons. The core is to understand that a transformation can be
thought of in two ways: as a transformation of the \emph{coordinate
frame itself}, and as transformation of the \emph{coordinates relative
to a coodrinate frame}. I'll first give a non-formal explanation and
later more formal definitions of affine frames and their
transformation.

Think of $T_{W\to B}$ as translating and rotating a real rigid body:
First, the body is located at the world origin; then the body is moved
by a translation $t$; then the body is rotated (around its own center)
as described by $R$. In that sense, $T_{W\to B} = \mat{cc}{R & t \\ 0
& 1} $ describes the ``forward'' transformation of the body. Consider
that a coordinate frame $B$ is attached to the rigid body and a frame
$W$ to the world origin. Given a point $p$ in the world, we can
express its coordinates relative to the world, $p^W$, or relative to
the body $p^B$. You can convince yourself with simple examples that
$p^W = T_{W\to B}~ p^B$, that is, $T_{W\to B}$ \emph{also} describes
the ``backward'' transformation of body-relative-coordinates to
world-relative-coordinates.

Formally: Let $(A,V)$ be an affine space. A coordinate frame is a
tuple $(o,\bd e_1,..,\bd e_n)$ of an origin $o \in A$ and basis
vectors $\bd e_i \in V$. Given a point $p\in A$, its coordinates
$p_{1:n}$ w.r.t.\ a coordinate frame $(o,\bd e_1,..,\bd e_n)$ are given
implicitly via
\begin{align}
p = o + \sum\nolimits_i p_i \bd e_i ~.
\end{align}
A transformation $T_{W\to B}$ is a (``forward'')
transformation of the coordinate frame itself:
\begin{align}
(o^B,\bd e^B_1,..,\bd e^B_n)
 &= (o^W + t, R\bd e^W_1,..,R\bd e^W_n)
\end{align}
where $t\in V$ is the affine translation in $A$ and $R$ the rotation
in $V$. Note that the coordinates $(\bd e^B_i)^W_{1:n}$ of a basis
vector $\bd e^B_i$ relative to frame $W$ are the columns of $R$:
\begin{align}
\bd e^B_i
 &= \sum_j (\bd e^B_i)^W_j \bd e^W_j
  = \sum_j R_{ji} \bd e^W_j
\end{align}
Given this transformation of the coordinate frame itself, the
coordinates transform as follows:
\begin{align}
p &= o^W + \sum_i p^W_i~ \bd e^W_i \\
p &= o^B + \sum_i p^B_i~ \bd e^B_i \\
  &= o^W + t + \sum_i p^B_i~ (R \bd e^W_i) \\
  &= o^W + \sum_i t^W_i~ e^W_i + \sum_j p^B_j~ (R \bd e^W_j) \\
  &= o^W + \sum_i t^W_i~ e^W_i + \sum_j p^B_j~ (\sum_i R_{ij}~ \bd e^W_i) \\
  &= o^W + \sum_i \[t^W_i + \sum_j R_{ij}~ p^B_j\]~ e^W_i \\
\To
 &~ p^W_i = t^W_i + \sum_j R_{ij}~ p^B_j ~.
\end{align}
Another way to express this formally: $T_{W\to B}$
maps \emph{covariant} vectors (including ``basis vectors'') forward,
but \emph{contra-variant} vectors (including ``coordinates'')
backward.




\subsection{Kinematic chains}

In this section we only consider static transformations compost of
translation and rotation. But given the general concatenation rules
above, everything said here generealizes directly to the dynamic case.

\subsubsection{Rigid and actuated transforms}

A actuated kinematic chain with $n$ joints is a series of transformations of the form
\begin{align}
T_{W\to 1} \circ Q_1 \circ T_{1\to2} \circ Q_2 \circ T_{2\to3} \circ
Q_3 \circ \cdots
\end{align}
Each $T_{i\1\to i}$ describes so-called ``links'' (or bones) of the
kinematic chain: the rigid (non-actuated) transformation from the
$i\1$th joint to the $i$th joint. The first $T_{W\to 1}$ is the
transformation from \emph{world} coordinates to the first joint.

Each $Q_i$ is the actuated transformation of the joint -- usually simply
a rotation around the joint's x-axis with a specific angle, the
so-called \emph{joint angle}. These joint angles (and therefore each
$Q_i$) are actuated and may change over time.

When we control the robot we essentially tell it to actuate its joint
so as to change the joint angles. There are two fundamental
computations necessary for control:
\begin{enumerate}
\item For a given $n$-dimensional vector $q \in \RRR^n$ of joint
angles, compute the absolute frames $T_{W\to i}$ (\emph{world} to link
transformation) of each link $i$.
\item For a given $n$-dimensional vector $\dot q \in \RRR^n$ of joint
angle velocities, compute absolute (\emph{world}-relative) velocity
and angular velocity of the $i$th link.
\end{enumerate}

The first problem is solved by ``forward chaining'' the
transformations: we can compute the absolute transforms $T_{W\to i}$
(i.e., the transformation from \emph{world} to the $i$th link) for
each link, namely:
\begin{align}
T_{W\to i} = T_{W\to i\1} \circ Q_i \circ T_{i\1\to i} ~.
\end{align}
Iterating this for $i=2,..,n$ we get positions and orientations
$T_{W\to i}$ of all links in world coordinates.

The second problem is addressed in the next section.


\subsubsection{Jacobian \& Hessian}

\begin{figure}[t]
\begin{center}
\show[.15]{kinematics}
\end{center}
\caption{\label{figJac}
  Illustration for the Jacobian and Hessian.}
\end{figure}

Assume we have computed the absolute position and orientation of each
link in an actuated kinematic chain. Then we want to know how a point
$p_i^W$ attached to the $i$th frame (and coordinates expressed w.r.t.\
the world frame $W$) changes when rotating the $j$th joint. Equally we
want to know how some arbitrary vector $a_i^W$ attached to the $i$th
frame (coordinates relative to the world frame $W$) rotates when
rotating the $j$th joint. Let $a_j^W$ be the unit length rotation axis
of the $j$th joint (which, by convention, is $r_{W\to j} \cdot
(1,0,0)$ if $r_{W\to j}$ is the rotation in $T_{W\to j}$). In the
following we drop the superscript $W$ because all coordinates are
expressed in the world frame. Let $q_j$ be the joint angle of the
$j$th joint. For the purpose of computing a partial deriviative of
something attached to link $i$ w.r.t.\ the actuation at joint $j$ we
can think of everything inbetween as rigid.  Hence, the point and
vector Jacobian and Hessian are simply:
\begin{align}
d_{ij}
 &:=p_i-p_j \feed
\frac{\del p_i}{\del q_j}
 &= a_j \times d_{ij} \\
\frac{\del a_i}{\del q_j}
 &= \a_j \times a_i \\
\frac{\del^2 p_i}{\del q_j~ \del q_k}
 &=\frac{\del a_j}{\del q_k} \times d_{ij} + a_j \times \frac{\del d_{ij}}{\del q_k} \feed
 &= (a_k \times a_j) \times d_{ij}
   + a_j \times [a_k \times (p_i - p_k) - a_k \times (p_j - p_k)] \feed
 &= (a_k \times a_j) \times d_{ij}
   + a_j \times (a_k \times d_{ij}) \feed
 &\[\text{using $a \times (b \times c) + b \times (c \times a) + c \times (a \times b) = 0$}\]\feed
 &= a_k \times (a_j \times d_{ij}) \\
\frac{\del^2 a_i}{\del q_j~ \del q_k}
 &= (a_k \times \a_j) \times a_i + \a_j \times (a_k \times a_i) \feed
 &= a_k \times (\a_j \times a_i)
\end{align}

Efficient computation: Assume we have an articulated
kinematic \emph{tree} of links and joints. We write $j\!<\!i$ if joint
$j$ is inward from link (or joint) $i$.  Then, for each body $i$
consider all inward edges $j\!<\!i$ and further inward edges
$k\!\le\!j$ and compute $H_{ijk}=\del_{\t_j}\del_{\t_k} p_i$. Note
that $H_{ijk}$ is symmetric in $j$ and $k$ -- so computing for
$k\!\le\!j$ and copying the rest is sufficient.

\section{Further}

\begin{itemize}
\item Differential Geometry

Emphasize strong relation between a Riemannian metric (and respective geodesic) and cost (in an optimization formulation). Pullbacks and costs. Only super brief, connections.

\item Manifolds

Local tangent spaces, connection. example of kinematics

\item Lie groups

exp and log

\item Information Geometry

[Integrate notes on information geometry]

\end{itemize}

%% \subsubsection{Basic kinematics and dynamics}

%% Direct way to compute mass tensor. Note, Featherstone's algorithms is
%% much faster (linear instead of quadratic).

%% define: $d_{ai} := p_a-p_i$

%% masses and inertia tensors: $m_a$ and $I_a$

%% \begin{align}
%% J_{ai}
%%  &= \frac{\del p_a}{\del q_i}
%%   = [i<a]~ a_i \times d_{ai} \\
%% \overset\circ J_{ai}
%%  &= \frac{\del a_a}{\del q_i}
%%   = [i<a]~ a_i \times a_a \\
%% H_{aij}
%%  &= \frac{\del p_a}{\del q_i \del q_j}
%%   = [i<a][j<a]~ a_j \times (a_i \times d_{ai}) \\
%% \overset\circ H_{aij}
%%  &= \frac{\del a_a}{\del q_i \del q_j}
%%   = [i<a][j<a]~ a_j \times (a_i \times a_a) \\
%% M_{ij}
%%  &= \sum_a [i<a][j<a]~ \big[
%%     m_a (a_i \times d_{ai})(a_j \times d_{aj})
%%    + a_i^T I_a a_j \big] \\
%% \frac{\del M_{ij}}{\del q_k}
%%  &= \sum_a [i<a][j<a][k<a]~ \big[
%%     m_a (H_{aik} J_{aj} + J_{ai} H_{ajk})
%%    + (a_k\times a_i)^T I_a a_j + a_i^T I_a (a_k\times a_j) \big] \\
%% C_{ijk}
%%  &= \frac{\del M_{ij}}{\del q_k} - \half \frac{\del M_{jk}}{\del q_i}
%% \end{align}


%% \subsection{A note on partial derivatives w.r.t.\ a vector}

%% \newcommand{\dimbox}[4]{\framebox{$#4$}}
%% %% \newlength{\mywidth}
%% %% \newcommand{\dimbox}[4]{
%% %% \setlength{\unitlength}{1.1em}
%% %% \begin{picture}(#3,#1)(0,#2)
%% %% \framebox(#3,#1)[c]{$#4$}
%% %% \end{picture}
%% %% }

%% \renewcommand{\de}[2]{\frac{\partial #1}{\partial #2}}

%% Let $x \in \RRR^n$. Consider a scalar function $f(x)\in\RRR$. What
%% exactly is the partial derivative $\Del{x}f(x)$? We can
%% write it as follows:
%% \begin{align}
%% \Del{x}f(x)
%% %&= (\de{f(x)}{x_1}, \de{f(x)}{x_2}, \dots,\de{f(x)}{x_n})
%%  &=  \mat{c}{\de{f(x)}{x_1} \\ \de{f(x)}{x_2} \\ \dots
%%       \\ \de{f(x)}{x_n} }^\T
%% \end{align}
%% Why did we write this as a transposed vector? By definition, a
%% derivative is something that, when you multiply it with an
%% infinitesimal displacement $\d x \in \RRR^n$, it returns the infinitesimal change
%% $\d f(x) \in \RRR$:
%% \begin{align}
%% \d f(x) = \de{f(x)}{x}~ \d x ~.
%% \end{align}
%% Since in our case $\d x \in \RRR^n$ is a vector, for this equation to make sense,
%% $\de{f(x)}{x}$ must be a transposed vector:
%% \begin{align}
%% \d f(x)
%%  &= \mat{c}{\de{f(x)}{x_1} \\ \de{f(x)}{x_2} \\ \dots
%%         \\ \de{f(x)}{x_n} }^\T
%%     \mat{c}{\d x_1 \\ \d x_2 \\ \dots \\ \d x_n } \\
%%  &= \sum_i \frac{\del f(x)}{\del x_i}~ \d x_i ~.
%% \end{align}
%% Expressed more formally: The partial derivative corresponds to a
%% 1-form (also called dual vector). The parameters of a 1-form actually
%% form a covariant vector, not a contra-variant vector; which in normal
%% notation corresponds to a transposed vector, not a normal vector. See
%% Wikipedia ``Covariance and contra-variance of vectors''.

%% Consider a vector-valued mapping $\phi(x) \in \RRR^d$. What exactly is
%% the partial derivative $\Del{x}\phi(x)$? We can write it
%% as follows:
%% \begin{align}
%% \Del{x}\phi(x)
%%  &= \mat{cccc}{
%% \de{\phi_1(x)}{x_1} & \de{\phi_1(x)}{x_2} & \dots & \de{\phi_1(x)}{x_n} \\
%% \de{\phi_2(x)}{x_1} & \de{\phi_2(x)}{x_2} & \dots & \de{\phi_2(x)}{x_n} \\
%% \vdots & & & \vdots \\
%% \de{\phi_d(x)}{x_1} & \de{\phi_d(x)}{x_2} & \dots & \de{\phi_d(x)}{x_n} } \\
%% &= \dimbox{2}{.7}{4}{d\times n} %~ \dimbox{3}{1.2}{1}{d}
%% \end{align}
%% Why is this a $d\times n$ matrix and not the transposed $n\times d$
%% matrix? Let's check the infinitesimal variations again: We have $\d
%% x \in \RRR^n$ and want $\d \phi \in \RRR^d$:
%% \begin{align}
%% \d \phi(x)
%%  &= \de{\phi(x)}{x}~ \d x \\
%%  &= \mat{cccc}{
%% \de{\phi_1(x)}{x_1} & \de{\phi_1(x)}{x_2} & \dots & \de{\phi_1(x)}{x_n} \\
%% \de{\phi_2(x)}{x_1} & \de{\phi_2(x)}{x_2} & \dots & \de{\phi_2(x)}{x_n} \\
%% \vdots & & & \vdots \\
%% \de{\phi_d(x)}{x_1} & \de{\phi_d(x)}{x_2} & \dots & \de{\phi_d(x)}{x_n} }~
%% \mat{c}{\d x_1 \\ \d x_2 \\ \dots \\ \d x_n } \\
%% &= \dimbox{2}{.7}{4}{d\times n}~ \dimbox{4}{1.7}{1}{n} \\
%% &= \mat{c}{
%% \sum_i \frac{\del \phi_1(x)}{\del x_i}~ \d x_i \\
%% \sum_i \frac{\del \phi_2(x)}{\del x_i}~ \d x_i \\
%% \vdots \\
%% \sum_i \frac{\del \phi_d(x)}{\del x_i}~ \d x_i }
%% \end{align}
%% Loosly speaking: taking a partial derivative always \emph{appends} a
%% (covariant) index to a tensor. The reason is that the resulting
%% tensor needs to multiply to a (contra-variant infinitesimal variation)
%% vector from the right.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \clearpage
%% Exam Tip: These are the headings of all questions that appeared in
%% exams -- no guarantee that similar ones might appear. To guess what
%% the question behind such a heading might be, look up an exercise with
%% similar heading.

%% \begin{items}
%%   \item Derivatives
%%   \item Vector Spaces
%%   \item Inner Product
%%   \item Projections
%%   \item Fundamental Theorem and SVD
%%   \item Eigenvectors
%%   \item Singular and Eigenvalues
%%   \item Volume
%%   \item Linear Robot
%%   \item Jacobian
%%   \item Taylor expansion
%%   \item Lagrangian Method
%%   \item Gradient Descent \& Newton Method
%%   \item KKT
%%   \item Log Barrier
%%   \item Squared Penalty \& Augmented Lagrangian
%%   \item Linear Program
%%   \item KKT conditions

%%     ~

%%   \item Gaussian Distribution
%%   \item Laplace Approximation
%%   \item Bayesian Reasoning
%%   \item Posterior mean of Gaussian

%%     ~

%%   \item Acquisition Function
%%   \item No Free Lunch
%% \end{items}

\clearpage
\addcontentsline{toc}{section}{Index}
\printindex

\end{document}

