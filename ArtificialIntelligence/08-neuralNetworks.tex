\input{../latex/shared}

\renewcommand{\course}{Artificial Intelligence}
\renewcommand{\coursepicture}{course_ai}
\renewcommand{\coursedate}{Summer 2023}

\renewcommand{\topic}{Computation Graphs \& Neural Nets}
\renewcommand{\keywords}{}

\newcommand{\pa}{\theta}

\slides

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\story{
Neural Networks (NNs) are a central technology in ML and AI. However, rather than discribing them directly, we first want to introduce the more basic concept of Computation Graphs and automatic differentiation in such graphs. The technical realization of NNs (e.g.\ in languages such as pyTorch) is based on the general principle of computation graphs, and this formalism gives a more general perspective on what is ``learnable'' in AI: namely anything that can be programmed in a computation graph is auto-differentiable and becomes trainable.

We concretely introduce the abstract framework, the difference between partial and total derivatives in such graphs, as well as the two types of propagating partial derivatives: forward and backward.

On this basis we introduce NNs as intances of computation graphs, that are composed of linear and non-linear operators. The linear operators (fully connected, or convolutional) describe a notion of ``connectivity'' (of signals through the layers) and introduce large numbers of parameters to the function. The non-linear operators (e.g. ReLU, max-pooling, etc) are typically parameter-free. Alternating such operators and building large computation graphs of such operators provides an extreme freedom in formulating powerful function models, including for image and text processing.

We briefly mention typical architectural patterns in successful models, e.g.\ convolution, residual connections, recurrency, and transformers. We also briefly mention essential things that matter: initialization, regularization data augmentation, and batch SGD training.
}

%% \sublecture{Role of ML in Decision Making?}{
%% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% %% \slide{}{

%% %% \item Recap our working definition of AI (Lecture 0):

%% %% ~\small

%% %% \emph{``AI is a research discipline about systems that take optimal/desirable decisions on the basis of all available information''}

%% %% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{What is Machine Learning?}{

%% \item Basic ML (supervised ML) is about:

%% \cen{\emph{Train a function $f: x\mapsto y$ to minimize a loss $\LL(f,D)$ on data $D=\{ (x_i,y_i) \}_{i=1}^n$}}

%% \pause

%% \item How does that relate to decision making?
%% \begin{items}
%% \item Basic ML is not really about defining/solving decision processes

%% {\tiny (Optimizing $f$ can be formalized as a single-step decision problem: Choosing $f$ to minizing risk $\LL(f,D)$.)

%% }

%% \item Basic ML does not define/consider interactive processes, where decisions change a world state, and long-term effects need to be considered

%% \item Basic ML does not describe ``Learning'' as an interactive process -- learning is $\argmin_f \LL(f,D)$
%% \end{items}

%% \pause

%% \item \textbf{BUT,} using Basic ML within Bandits/MCTS/RL/etc is the key enabler:
%% \begin{items}
%% \item ML is the key method to estimate $Q$-values or models from data, esp.\ in continuous high-dimensional case
%% \item ML is the key method to deal with partial observability (perception, computer vision), also by learning latent representations (recurrent policies)
%% \item ML is the key technology to formulate structured policy models
%% \end{items}

%% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Machine Learning beyond Basic ML}{

%% \item The term ``Machine Learning'' is more broad than Basic ML:

%% ~

%% \item Fitting more structured models to data, which includes
%% \begin{items}
%% \item Unsupervised learning (semi-supervised learning)
%% \item Time series, recurrent processes
%% \item Graphical Models
%% \end{items}
%% {\tiny ...but in these cases the scenarios are not interactive, the data $D$ is static

%% }

%% ~

%% \item And ML is sometimes used to also include:
%% \begin{items}
%% \item Bandits
%% \item Active Learning \quad {\tiny (use queries to collect own data -- strongly related to continuous/infinite bandits)}
%% \item Reinforcement Learning
%% \end{items}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Computation Graphs \& Automatic Differentiation}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\item We want to train parameterized $f_\pa: x\mapsto y$ to minimize a loss $\sum_{i=1}^n \ell(f_\pa(x_i), y_i)$

~

\item Training is typically based on gradients $\na_\pa \ell(f_\pa(x_i),y_i)$

~\pause

\item The success of Neural Networks is based on
\begin{items}
\item Flexibility in defining $f(x)$ as computation graph (e.g., in PyTorch)
\item \textbf{Automatic Differentiation} of such computation graphs
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example}{

$$f(x,g) = 3x + 2g \quad\text{and}\quad g(x)=2 x$$

\cen{\emph{Q:~ What is the derivative of $f$ w.r.t.\ $x$?}}

~\pause

\item Right Answer:~ Which one do you mean? $\frac{\del}{\del x} f(x,g)$ or $\frac{d}{dx} f(x,g)$?

~\pause

\begin{equation*}
\Del{x} f(x,g) = 3 
\end{equation*}

\pause

\begin{align*}
\frac{df}{dx}
 &= \Del{x}\[ 3x+2(2x) \] = 7 \\
\frac{df}{dx}
 &= \Del{x} f(x,g) + \Del{g} f(x,g)~ \Del{x} g(x) = 3 + 2\cdot2 = 7
\end{align*}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Partial \& Total Derivative}{

\item The \emph{partial} derivative only considers a single function $f(a,b,c,..)$ and asks how the output of this single function varies with one of its arguments.  (Not caring that the arguments might be functions of yet something else).

{\small

\item Given $f:~ \RRR^n \to \RRR$ of $n$ arguments, $f(x_1,..,x_n)$, the \emph{partial} derivative is the derivative w.r.t.\ only one of its arguments:
\begin{equation*}
\Del{x_i} f(x_1,..,x_n) = \lim_{h\to
0} \frac{f(x_1,..,x_i+h,..,x_n)-f(x)}{h} ~.
\end{equation*}

}

~\pause

\item The \emph{total} derivative considers full networks of dependencies between quantities and asks how one quantity varies with some other.


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Computation Graphs}{

\item A \textbf{computation graph} (or \textbf{function network}) is a DAG of $n$ variables $x_i$ where
each variable is a deterministic function of a set of parent variables, as specified by the parents' indices $[i]\subset\{1,..,n\}$, that is
$$x_i = f_i( x_{[i]} )$$
where $x_{[i]} = (x_j)_{j \in [i]}$ is the tuple of parent variables.

~

{\tiny (This could also be called \emph{deterministic Bayes net}.)

}

~\pause

\item Given a computation graph, we can define the \textbf{total derivative}:
\begin{items}
\item Given a variation $dx$ of some variable, how would another variable vary, accounting for all dependencies down the DAG, in the linear limit?
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Chain rules}
\slide{Chain rules}{

\item \textbf{Forward version:}\anchor{210,-50}{\showh[.08]{chainRule-fwd}} ~ (e.g., used for robotics Jacobians)
\begin{align*}
\frac{df}{dx} &= \sum_{g\in[f]} \frac{\del f}{\del g}~ \frac{dg}{dx}
\end{align*}

{\tiny

Why ``forward''? You've computed $\frac{dg}{dx}$ already, now you move forward to $\frac{df}{dx}$.

Note: If $x\in[f]$, the sum includes the term $\frac{\del f}{\del x}~ \frac{dx}{dx} \equiv \frac{\del f}{\del x}$. We could also write
$\frac{df}{dx} =
\frac{\del f}{\del x} + \sum_{g\in[f]\atop g \not= x} \frac{\del f}{\del g}~ \frac{dg}{dx}$.

}

~

\item \textbf{Backward version:}\anchor{200,-60}{\showh[.08]{chainRule-bwd}} ~ (used in NNs)
\begin{align*}
\frac{df}{dx}
&= \sum_{g:x\in[g]} \frac{d f}{d g}~ \frac{\del g}{\del x}
\end{align*}

{\tiny

Why ``backward''? You've computed $\frac{df}{dg}$ already, now you move backward to $\frac{df}{dx}$.

Note: If $x\in[f]$, the sum includes the term $\frac{d f}{d f}~ \frac{\del f}{\del x} \equiv \frac{\del f}{\del x}$. We could also write
$\frac{df}{dx}
= \frac{\del f}{\del x} + \sum_{g:x\in[g]\atop
g\not= f} \frac{d f}{d g}~ \frac{\del g}{\del x}$.

}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Complexity of the Chain Rule}{

{\tiny [for simplicity, memory complexity only -- compute complexity is strongly related]}

~

\item Consider a chain of three variables $x,~ y(x), z(y)$ -- should you use fwd or bwd?

~

~\pause

\twocol{.45}{.45}{
\item Forward:
\begin{items}
\item Compute $\frac{dy}{dx} \in \RRR^{d_y \times d_x}$
\item Compute $\frac{dz}{dx} \in \RRR^{d_z \times d_x}$
\item All steps have complexity with $d_x$
\end{items}

}{

\item Backward:
\begin{items}
\item Compute $\frac{dz}{dy} \in \RRR^{d_z \times d_y}$
\item Compute $\frac{dz}{dx} \in \RRR^{d_z \times d_x}$
\item All steps have complexity with $d_z$
\end{items}

}

~

~\pause

\item $d_x$ large and $d_z$ small  $\to$ use backward! ~ (as in Neural Networks: $x\equiv\pa$, $z\equiv\ell$)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \sublecture{Machine Learning primer}{

%% \item Basic ML$^0$ (function approximation) is all about specifying the
%% \begin{items}
%% \item \textbf{Model:} How is the function $f:x\mapsto y$ represented/parameterized?
%% \item \textbf{Objective:} Given a data set $D$, what loss should $f$ minimize?
%% \item \textbf{Solver:} What algorithm do we use to minimize the loss?
%% \end{items}

%% ~\pause

%% \item Given data $D=\{ (x_i,y_i) \}_{i=1}^n$, the standard objective is to minimize the ``error'' on the data
%% \begin{align*}
%% f^* \argmin _{f\in\HH} \sum_{i=1}^n \ell(f(x_i), y_i) ~,
%% \end{align*}
%% where $\ell(\hat y, y) > 0$ penalizes a discrepancy between a model output $\hat y$ and the data $y$.

%% \begin{items}
%% \item Squared error $\ell(\hat y, y) = (\hat y - y)^2$
%% \item Classification error $\ell(\hat y, y) = [ \hat y \not= y ]$
%% \item neg-log likelihood $\ell(\hat y, y) = -\log p(y \| \hat y)$
%% \item etc
%% \end{items}

%% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Example: Polynomial Ridge Regression}{

%% \item \textbf{Model:} Given input $x\in\RRR^d$ and output $y\in\RRR$, we represent functions $f_\pa$ as polynomials over $x$,
%% $$f(x) = \sum_{j=1}^k \phi_j(x)~ \b_j = \phi(x)^\T \pa$$
%% where $\phi(x)$ is the \textbf{feature} vector of all monomials, e.g.\ quadratic:
%% $\phi(x) = (1,x_1,..,x_d,x_1^2,x_1x_2,x_1x_3,..,x_d^2)$

%% ~\pause

%% \item \textbf{Objective:} Given data $D=\{(x_i,y_i)\}_{i=1}^n$ we define the loss as
%% $$L^{\text{ridge}}(\pa) = \sum_{i=1}^n (y_i - \phi(x_i)^\T \pa)^2 + \l \sum_{j=2}^k \b_j^2$$

%% ~\pause

%% \item \textbf{Solver:} We can solve this analytically:
%% $$\hat \pa^\text{ridge} = \argmin_\pa L^\text{ridge}(\pa) = (X^\T X + \l I)^\1 X^\T y$$

%% {\tiny where $y=(y_1,..,y_n)$ is the vector of outputs, and $X_{i\cdot} = \phi(x_i)$ contains the feature vectors in rows.

%% }
%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Neural Networks}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{What is a Neural Network? ~ (Recap Lecture 04)}{

\newcommand{\nlin}{\underset{\rotatebox{-90}{\tiny $\leftarrow$nlin}}}
\newcommand{\lin}{\underset{\rotatebox{-90}{\tiny $\leftarrow$lin}}}

\item A NN is a parameterized function $f_\pa: \RRR^d \mapsto \RRR^M$, with ``weights'' $\pa$
\begin{items}
\item Given a data set $D=\{(x_i,y_i)\}_{i=1}^n$, we minimize empirical risk (cf.\ Lecture 04, slide 4)
$$\pa^* = \argmin_\pa \sum_{i=1}^n \ell(f_\pa(x_i), y_i) + \text{~regularization}$$
\end{items}

\medskip\small

\item Linear model (i.e., a model linear in parameters $\b$):
\qquad$f_\t(x) = \underbrace{\b~{}^\T}_{\rotatebox{-90}{\tiny $\leftarrow$lin}}\nlin{\phi}(x)$

\vspace*{-5mm}

\item 3-layer NN (with parameters $\t=(W_1,W_2,W_3, b1_,b_3,b_3)$):
$$f_\t(x) = \lin{W_3}~ \nlin\s\[~ \lin {W_2}~\nlin\s[~ \lin {W_1} ~x + b_1 ~] + b_2 ~\] + b_3 $$

\item In more complex neural networks, \textbf{$f_\pa$ is an arbitrary computation graph}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{(Recap Lecture 04)}{

~

\twocol{.45}{.45}{
\showh[1]{NeuralNet-new-regression}
}{
%\only<2>{\show{NeuralNet-new-classification}}
\showh[1]{NeuralNet-new-NN}
}
%% \only<1>{\show[.6]{NeuralNet-new-regression}}
%% \only<2>{\show[.6]{NeuralNet-new-classification}}
%% \only<3>{\show[.6]{NeuralNet-new-NN}}

~

\tiny

($L$-layer NN means $L-1$ hidden layers plus 1 output layer. The input $x_0$ is not counted.)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{NN gradient}
\key{NN back propagation}
\slide{Computing the gradient: Forward \& Backward pass in NNs}{

\item \textbf{Forward:} Compute prediction, iterating forward: ~ Given single data point $(x,y^*)$:
$$\forall_{l=1,..,L\1}:~ z_l = W_l x_{l\1} + b_l \comma x_l=\s(z_l)~;\quad \text{with loss~} \ell(z_L,y^*)$$

\item Computation graph is a chain $\to$ use the \textbf{backward chain rule} to get all $\frac{d \ell}{d w}$:

\medskip

\footnotesize
For reference:
\begin{items}
\item First compute $\d_L = \frac{d \ell}{d f} = \frac{d \ell}{d z_L} \in\RRR^{1\times M}$, which is the gradient (row vector) w.r.t.\ outputs $z_L$
\item then \textbf{backpropagate} the gradient $\frac{d \ell}{d z_l} \in\RRR^{1\times h_l}$ w.r.t.\ all other layers $z_l$ as:
\begin{align*}
\forall_{l=L\1,..,1}:~ \d_l
&\defeq \frac{d \ell}{d z_l} 
 = \frac{d \ell}{d z_{l\po}} \frac{\del z_{l\po}}{\del x_l} \frac{\del x_l}{\del z_l}
 = [\d_{l\po}~ W_{l\po}] \circ [\s'(z_l)]^\T
\end{align*}
where $\circ$ is an \emph{element-wise product}. The gradient w.r.t.\ parameters:
\begin{align*}
\frac{d \ell}{d W_{l,ij}}
 &= \frac{d \ell}{d z_{l,i}}~ \frac{\del z_{l,i}}{\del W_{l,ij}}
  = \d_{l,i}~ x_{l\1,j}\quad \text{or}\quad
\frac{d \ell}{d W_l}
  = \d_l^\T x_{l\1}^\T
\comma   \frac{d \ell}{d b_l} = \d_l^\T
\end{align*}
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Neural Network models}{

%% \item The $l$-th layer of a NN is described by:
%% \begin{items}
%% \item \textbf{weights} $W_l,b_l$, which define the linear mapping $z_l = W_l x_{l\1} + b_l ~ \in \RRR^{h_l}$
%% \item \textbf{non-linear activation function}, which defines the activation $x_l = \s(z_l) ~ \in \RRR^{h_l}$
%% \end{items}

%% ~\pause

%% \item A full $L$-layer NN computes all layers forward ~ (``forward propagation''):
%% $$\forall_{l=1,..,L\1}:~ z_l = W_l x_{l\1} + b_l \comma x_l=\s(z_l)$$
%% where $x_0\equiv x$ is the input, and $f_\pa(x) \equiv z_L$ the output

%% ~


%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Linear Operators}{

{\tiny (Equations for a linear mapping $x \mapsto z$ in the 1D case $x\in\RRR^h$.)}

\medskip

\item Fully connected: Matrix multiplication $z_i = \sum_{j=1}^h W_{ij} x_j + b_i$
\begin{items}
\item Each $z_i$ depends on each $x_j$; (analogy of the connectivity (dendrites/axon) between neurons)
\item $z\in\RRR^o$ has arbitrary size $o$, depending on matrix size $W\in\RRR{o\times h}$
\end{items}

\item Convolution: $z_i = \sum_{d=-s}^{+s} w_d~ x_{i+d}$
\begin{items}
\item Each $z_i$ depends on a ``receptive field'' $(x_{i-s},..,x_{i+s})$ of width $2s+1$
\item The weights $w_d$, $d=-s,..,s$, are shared between all $z_i$, and are called \emph{kernel}
\item Compare to continuous convolution $(f*g)(t) = \int g(\tau) f(t+\tau) d\tau$ with kernel $g$
\end{items}

\item Multi-channel convolution: $z_{ik} = \sum_{d=-s}^{+s} W_{kd}~ x_{i+d}$
\begin{items}
\item For $k=1,..,K$ this outputs $K$ channels
\item Each channel $z_{\cdot k}$ has a different convolution kernel $W_{k\cdot}$, and can learn to encode a different signal
\end{items}

\cen{\textbf{All linear operators describe ``connectivity'' and introduce weight parameters.}}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Non-Linear Operators}{

\medskip

\item Element-wise non-linearities: (classically: ``activation functions'')
\begin{items}
\item rectified linear unit (ReLU): $\s(x) = [x]_+ = \max\{0,x\}$ % = x [x\ge 0]$
\item leaky ReLU: $\s(x) = \max\{0.01x, x\}$ % =  \begin{cases} 0.01 x & x<0 \\ x & x\ge 0\end{cases}$
\item sigmoid, logistic: $\s(x) = 1/(1+e^{-x})$ ~ [old'ish:  tanh: $\s(z) = \tanh(z)$]
\end{items}

\item Pooling non-linearity:
\begin{items}
\item max-pooling: map multiple activations to just one: their max (reduces layer size!)
\item E.g., $s$-max in 1D case: $z_i = \max\{x_{i/s+1},..,x_{i/s+s}\}$  (Similar to convolution, but taking max.)
\end{items}

\item Layer-wise non-linearities:
\begin{items}
\item soft-max: given layer $z\in\RRR^h$, map to $x\in \RRR^h$ with $x_i = \frac{e^{z_i}}{\sum_j e^{z_j}}$
\item layer-norm: given a layer $z\in\RRR^h$, compute mean $\mu$ and variance $\s^2$ of $z$, and map to $x_i = w \circ \frac{z_i - \m}{\sqrt{\s^2+\e}} + b$, with elem-wise parameters $w, b\in\RRR^h$
\end{items}

\medskip

\cen{\textbf{Non-linear operators usually do not introduce parameters}}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \key{Neural Network function class}
%% \slide{Neural Network models}{

%% \item  of a NN is determined by
%% \begin{items}
%% \item weights $W

%% \item A (fwd-forward) NN $\RRR^{h_0} \mapsto \RRR^{h_L}$ with $L$ layers, each $h_l$-dimensional, defines the function
%% \begin{tabular}{l@{~}l@{~}l}
%% 1-layer: & $f_\pa(x) = W_1 x + b_1$, & $W_1 \in \RRR^{h_1\times h_0}, b_1\in\RRR^{h_1}$ \\
%% 2-layer: & $f_\pa(x) = W_2 \s(W_1 x+b_1) + b_2$, & $W_l \in \RRR^{h_l\times h_{l\1}}, b_l\in\RRR^{h_l}$ \\
%% $L$-layer: & $f_\pa(x) = W_L \s(\cdots \s(W_1 x+b_1) \cdots) + b_L$\hspace*{-5mm}
%% \end{tabular}

%% ~

%% \item The parameter $\pa=(W_{1:L}, b_{1:L})$ is the collection of all\\
%% \textbf{weights} $W_l \in \RRR^{h_l\times h_{l\1}}$ and \textbf{biases} $b_l\in\RRR^{h_l}$

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Neural Network models}{

%% \item We can think of the second-to-last layer $x_{L\1}$ as a feature vector
%% $$\phi_\pa(x) = x_{L\1}$$

%% \item This aligns NNs models with what we discussed so far. But the crucial difference is:

%% \cen{\textbf{In NNs, the features $\phi_\pa(x)$ are also parameterized and trained!}}

%% While in previous lectures, we had to fix $\phi(x)$ by hand, NNs allow us to learn features and \textbf{intermediate representations}

%% ~\tiny

%% \item Note: It is a common approach to train NNs as usual, but after training fix the trained features $\phi(x)$ (``remove the head (=output layer) and fix the remaining backbone of the NN'') and use these trained features for similar problems or other kinds of ML on top.

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{NNs as unversal function approximators}{

%% \item A 1-layer NN is linear in the input

%% \item Already a 2-layer NN with $h_1\to \infty$ hidden neurons is a universal function approximator
%% \begin{items}
%% \item Corresponds to $k\to\infty$ features $\phi(x)\in\RRR^k$ that are well tuned
%% \end{items}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \key{NN loss functions}
%% \slide{Loss functions as usual}{

%% \item For regression with for $h_L$-dimensional output:
%% \begin{items}
%% \item for a single data point $(x,y^*)$, $\ell(f(x),y^*) = (f(x) - y^*)^2$

%% \item the loss gradient is $\frac{\del\ell}{\del f} = 2 (f-y^*)^\T$
%% \end{items}

%% ~

%% \item For multi-class classification with $h_L=M$ outputs
%% %% , and $f_\pa(x)\in\RRR^M$ represents the \emph{discriminative function},
%% %% which means that the predicted class is the $\argmax_{y\in\{1,..,M\}}
%% %% [f_\t(x)]_y$.
%% \begin{items}
%% \item Neg-log-likelihood or cross entropy loss:
%% \begin{items}
%% \item for a single data point $(x,y^*)$, $\ell(f(x),y^*) = -\log p(y^* | x)$

%% \item the loss gradient at output $y$ is $\frac{\del\ell}{\del f_y} = p(y|x) - [y=y^*]$
%% \end{items}

%% \item One-vs-all hinge loss:
%% \begin{items}
%% \item for a single data point $(x,y^*)$, $\ell(f(x),y^*) = \sum_{y\not=y^*} [1 - (f_{y^*}-f_y)]_+$,

%% \item the loss gradient at non-target outputs $y\not=y^*$ is $\frac{\del\ell}{\del f_y} = [f_{y^*} < f_y+1 ]$

%% \item the loss gradient at the target output $y^*$ is $\frac{\del\ell}{\del f_{y^*}} = -\sum_{y\not=y^*} [f_{y^*} < f_y+1]$

%% \item this is also called \textbf{Perceptron Algorithm}
%% \end{items}
%% \end{items}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{}{

%% \item This forward and backward computations are done for each data point $(x_i,y_i)$.

%% \item Since the total loss is the sum $\LL(\pa,D) = \sum_i \ell(f_\pa(x_i), y_i)$, the total gradient is the sum of gradients per data point.

%% \item Efficient implementations send multiple data points (tensors) simultaneously through the network (fwd and bwd), which speeds up computations.

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Behavior of Gradient Propagation}{

%% \item Propagating $\d_l$ back through many layers can lead to problems

%% \item For the classical sigmoid $\s(z)$, $\s(z)'$ is always $<1$ ~ $\To$ \textbf{vanishing gradient}

%% $\to$ Modern activations functions (ReLU) reduce this problem

%% ~

%% \item The initialization of weights is very important!

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Why are NNs so successful now?}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\item Reason 1: Big Data
\begin{items}
\item Objective of ML shifted from sample efficiency to coping with billions of samples
\item Great for this: Stochastic gradient descent, high-capacity models, regularization/training tricks
\end{items}

~\pause

\item Reason 2: A new ``programming'' paradigm
\begin{items}
\item Normal NNs (=deep chain of layers) was only the beginning!
\item A breakthrough are novel programming languages (e.g.\ pyTorch) that allow users to specify more and more complex and clever structured networks (\emph{=computation graphs})
\begin{items}
\item Classic: bottleneck autoencoders, Convolutions, Recurrent NNs (LSTM, GRUs)
\item Newer: ResNet, Transformers, complex CV systems (e.g., MaskRCNN), NeRFs, etc
\end{items}
\item \textbf{AutoDiff makes anything that can be programmed trainable}
\item Many great advances are about inventing/programming novel network structures that can reflect fundamental structure of your problem
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: CNN structures}{

~

\show[1]{AlexNet}
\hfill AlexNet (NeurIPS'12)
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Convolutional NN}
\slide{Convolutional NNs}{

~\small

\item Standard fully connected layer: full matrix $W_l$ has $h_l h_{l\1}$ parameters, so that $z_l = W_l x_{l\1}$ 

\item Convolutional: Each neuron (entry of $z_l$) receives input from a square receptive field, with $k\times k$ parameters. All neurons \emph{share} these parameters $\to$ \emph{translation invariance}. The whole layer only has $k^2$ parameters.

\item There are often multiple neurons with the same receptive field (channels), to represent different ``filters''. Stride means downsampling. Padding at borders.

~

\item Pooling applies a predefined operation on the receptive field (no parameters): max or average. Typically for downsampling.


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: Deep Residual Networks}{

~

\show[.9]{ResNet}
\hfill Residual Networks (CVPR'16)
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: Deep ResNet structures}{

~

\show{ResNeXt}
\hfill ResNeXt (CVPR'17)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{\label{lastpage}

\item In these examples, these diagrams are like a ``programming language'', specifying the network structure

~

\item But more modern structures become more and more creative, and AutoDiff languages more and more general to program any structures

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{LSTM}
\slide{Recurrent Networks: Long Short-Term Memory (LSTM)}{

\small
\item Given an input time series $x_t$, process (e.g., memory) and predict an output time series $y_t$
\anchor{-110,-60}{\showh[.3]{recurrentNN}}

~

~

\showh[.65]{LSTM}\\
\hfill{\tiny (Hochreiter, Schmidthuber, 1997)}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{LSTM \& Gated Recurrrent Units}{

~\small

\item LSTM
\begin{items}
\item $c$ is a memory signal, that is multiplied with a sigmoid signal $\G_f$. If that is saturated ($\G_f \approx 1$), the memory is preserved; and backpropagation copies gradients back

\item If $\G_i$ is close to 1, a new signal $\tilde c$ is written into memory

\item If $\G_o$ is close to 1, the memory contributes to the normal neural activations $a$
\end{items}

~

\item Gated Recurrent Units:
\begin{items}
\item Cleaner and more modern: Gated Recurrent Units
\item but perhaps just similar performance
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Transformer}{

\footnotesize

\twocol{.3}{.65}{

\tiny

``Attention'' (NeurIPS'17)

\showh[.8]{transformer1}

}{

\tiny 
\item J.~Thickstun ('21): The Transformer Model in Equations\\
\show[.9]{transformer2}

\pause

\item PaLM-E (Driess, ICML'23)\\
\show{transformer3}

}



}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Discussion}{

\item All these NN architectures are just parameterizations of $f_\t(x)$

\item Training all these NN architectures is typically done in the same manner
\begin{items}
\item empirical risk (loss) minimzation $\pa^* = \argmin_\pa \sum_{i=1}^n \ell(f_\pa(x_i), y_i)$
\item Exploiting \textbf{AutoDiff} through the computation graph, and gradient-based optimization algorithms
\end{items}

~

\item The \emph{advances} are in the formulation of the computation graphs
\begin{items}
\item Novel programming languages like pyTorch enable creativity to conceive of novel $f_\t(x)$
\end{items}
\item Sometimes also in the creativity to define novel loss functions
\begin{items}
\item e.g. GANs, diffusion models
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Appendix: Things that matter}{

\item Initialization

\item Regularization

\item Data Augmentation

\item Batch Training

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{NN initialization}
\slide{Initialization}{

\small

\item Choose random weights that don't grow or vanish the gradient:
\begin{items}
\item E.g., initialize weight vectors in $W_{l,i\cdot}$ with standard deviation $1$, i.e., each entry with sdv $\frac{1}{\sqrt{h_{l\1}}}$
\item Roughly: If each element of $z_l$ has standard deviation $\epsilon$, the same should be true for $z_{l\po}$.
\end{items}
\hfill{\tiny ``On the importance of initialization..'' (ICML'13)}

\medskip

\item Choose each weight vector $W_{l,i\cdot}$ to point in a uniform random direction $\to$ same as above

\medskip

\item Choose biases $b_{l,i}$ randomly so that the ReLU hinges cover the input well (think of distributing hinge features for continuous piece-wise linear regression)

~

\item Using pre-trained networks:
\begin{items}
\item ImageNet5k,  AlexNet, VGG (arxiv'14), ResNet, ResNeXt
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{NN regularization}
\key{NN Dropout}
\slide{New types of regularization}{

\item Conventionally: add a $L_2$ or $L_1$ regularization.
\begin{items}
\item adds a penalty $\l W_{l,ij}^2$ (Ridge) or $\l|W_{l,ij}|$ (Lasso) for every weight
\item In practise, compute the unregularized gradient as usual, then add $\l W_{l,ij}$ (for $L_2$),
or $\l\sign{W_{l,ij}}$ (for $L_1$) to the gradient
\item Historically, this is called \textbf{weight decay}, as the additional gradient (executed after the unregularized weight update) just decays weights
\end{items}

\item Core modern approach: Dropout
\begin{items}
\item (see next slide)
\end{items}

\item Others:
\begin{items}
\item Data Augmentation
\item Training ensembles, bagging (averaging bootstrapped models)
\item Additional embedding objectives (e.g.\ semi-supervised embedding)
\item Early stopping
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Dropout}{

\item What is dropout?

\begin{items}
\item During both, forward passing and gradient computation, pretend that randomly selected ``neurons'' would not exist
\item For each entry $i$ in each layer $x_{l,i}$, decide with probability $p$ ($\approx .5$) whether it drops out: set the respective activation $x_{l,i}$, and gradient $\l_{l,i}$, equal to zero
\item At prediction time, ``average'' dropouts, i.e., multiply each activation $x_{l,i}$ with $p$
\end{items}

\item Theoretial foundation?
\begin{items}
\item Each dropout pattern defines another network. We have a random ensemble of networks, sharing parameters, each of which is trained on the same task. Eventually, we want to average of this ensemble.
\item \emph{Srivastava et al: Dropout: a simple way to prevent neural networks from overfitting, JMLR 2014.}
\item ``a way  of  approximately  combining  exponentially  many  different  neural  network architectures efficiently''
\item ``$p$ can  simply  be  set  at  0.5,  which  seems  to  be  close  to  optimal  for  a  wide  range  ofnetworks and tasks''
%\item on test/prediction time, take true averages
\end{items}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{data augmentation}
\slide{Data Augmentation}{

\item A very interesting form of regularization is to modify the data!

\item Generate more data by applying invariances to the given data. The model then learns to generalize as described by these invariances.

\item This is a form of regularization that directly incorporates expert knowledge

~

\show[.6]{dataAugmentation}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Optimization with Mini Batches}{

\item For small data size:

We can compute the loss and its gradient $\sum_{i=1}^n \nabla_\pa \ell(f_\pa(x_i), y_i)$.
\begin{items}
\item Use classical gradient-based optimization methods
\item default: L-BFGS, oldish but efficient: Rprop
\item Called \textbf{batch learning} ~ (in contrast to online learning)
\end{items}

~

\item For large data size: \quad The $\sum_{i=1}^n$ is highly inefficient!
\begin{items}
\item Adapt weights based on much smaller data subsets, \textbf{mini batches}
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Stochastic Gradient Descent}
\slide{Stochastic Gradient Descent}{

\item Compute the loss and gradient for a mini batch $\hat D \subset D$ of fixed size $k$.
\begin{align*}
\LL(\pa,\hat D)
&= \sum_{i\in\hat D} \ell(f_\pa(x_i), y_i) \\
\nabla_\pa \LL(\pa,\hat D)
&= \sum_{i\in\hat D} \nabla_\pa \ell(f_\pa(x_i), y_i)
\end{align*}

\item Naive Stochastic Gradient Descent iterates
\vspace*{-2ex}
$$\pa \gets \pa - \eta \nabla_\pa \LL(\pa,\hat D)$$
\vspace*{-4ex}
\begin{items}
\item Choice of learning rate $\eta$ is crucial for convergence!
\item Exponential cooling: $\eta = \eta_0^t$
\end{items}

{\tiny Yurii Nesterov (1983): \emph{A method for solving the convex programming problm with convergence rate $O(1/k^2)$}

}

\item Modern SGD versions (Adam and Nadam) make this more efficient. See appendix.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \key{Historical Discussion**}
%% \slide{Historical discussion}{

%% (This is completely subjective.)
%% \anchor{130,-20}{\href{http://link.springer.com/journal/10994/82/3/page/1}{\showh[.15]{ML-25th.jpg}}}

%% \item Early (from 40ies):
%% \begin{items}
%% \item McCulloch Pitts, Hebbian learning, Rosenblatt, Werbos (backpropagation)
%% \end{items}

%% \item 80ies:
%% \begin{items}
%% \item Start of connectionism, NIPS
%% \item ML wants to distinguish itself from pure statistics (``machines'', ``agents'')
%% \end{items}

%% \item '90-'10:
%% \begin{items}
%% \item More theory, better grounded, Statistical Learning theory
%% \item Good ML is pure statistics (again) (Frequentists, SVM)
%% \item ...or pure Bayesian (Graphical Models, Bayesian X)
%% \item sample-efficiency, great generalization, guarantees, theory
%% \item Great successes, in applications across disciplines; supervised, unsupervised, structured
%% \end{items}

%% \item '10-'20:
%% \begin{items}
%% \item Big Data. NNs. Size matters. GPUs.
%% \item Disproportionate focus on images
%% \item Software engineering becomes central
%% \end{items}

%% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{}{

%% \item NNs did not become ``better'' than they were 20y ago. What changed is the metrics by which they're are evaluated:

%% \pause

%% \item Old:
%% \begin{items}
%% \item Sample efficiency \& generalization; get the most from little data
%% \item Guarantees (both, w.r.t.\ generalization and optimization)
%% \item \textbf{generalize} much better than nearest neighbor
%% \end{items}

%% \item New:
%% \begin{items}
%% \item Ability to cope with billions of samples
%% \item[$\to$] no batch processing, but \textbf{stochastic} optimization (Adam) without monotone convergence
%% \item[$\to$] nearest neighbor methods infeasible, \textbf{compress} data into high-capacity NNs
%% \end{items}

%% %% Old:
%% %% \begin{items}
%% %% \item Sample efficiency \& generalization; get the most from little data
%% %% \item Guarantees (both, w.r.t.\ generalization and optimization)
%% %% \item Being only as good as a nearest neighbor methods is embarrasing
%% %% \end{items}

%% %% New:
%% %% \begin{items}
%% %% \item Ability to cope with billions of samples $\to$ no batch processing, but stochastic optimization
%% %% \item Happy to end up in some local optimum. (Theory on ``every local optimum of a large deep net is good''.)
%% %% \item Stochastic optimization methods (Adam) without monotone convergence
%% %% \item Nobody compares to nearest neighbor methods -- nearest neighbor on 1B data points is too expensive anyway. I guess that it'd perform very well (for a descent kernel) and a NN could be glad to perform equally well
%% %% \end{items}

%% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{NNs vs.\ nearest neighbor}{

%% \small

%% \item Imagine an autonomous car. Instead of carrying a neural net, it carries 1 Petabyte of data (500 hard drives, several billion pictures). In every split second it records an image from a camera and wants to query the database to returen the 100 most similar pictures. Perhaps with a non-trivial similarity metric. That's not reasonable!

%% \item In that sense, NNs are much better than nearest neighbor. They store/compress/memorize huge amounts of data. Sample efficiency and the precise generalization behavior beome less relevant.

%% \item That's how the metrics changed from '90-'10 to nowadays

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Images \& Time Series}{

%% \item My guess: 90\% of the recent success of NNs is in the areas of images or time series

%% \item For images, convolutional NNs (CNNs) impose a very sensible prior; the representations that emerge in CNNs are in fact similar to representations in the visual area of our brain.

%% \item For time series, long-short term memory (LSTM) networks represent long-term dependencies in a way that is well trainable -- something that is hard to do with other model structures.

%% \item Both these structural priors, combined with huge data and capacity, make these methods very strong.

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

More details on stochastic gradient descent, e.g.\ Adam

$\to$ \emph{Optimization Algorithms} course

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slidesfoot
