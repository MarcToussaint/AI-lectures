
@inproceedings{2016-finn-GuidedCostLearning,
	title = {Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization},
	url = {https://proceedings.mlr.press/v48/finn16.html},
	shorttitle = {Guided Cost Learning},
	abstract = {Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control ({IOC}) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for {MaxEnt} {IOC}. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.},
	booktitle = {International Conference on Machine Learning},
	pages = {49--58},
	booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
	delete_delete_delete_publisher = {{PMLR}},
	author = {Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
	urlyear = {2024},
	year = {2016},
	langid = {english},
	delete_delete_delete_note = {{ISSN}: 1938-7228},
	file = {Full Text PDF:/home/mtoussai/Zotero/storage/6VR8EWJU/Finn et al. - 2016 - Guided Cost Learning Deep Inverse Optimal Control.pdf:application/pdf},
}

@article{-ziebart-MaximumEntropyInverse,
	title = {Maximum Entropy Inverse Reinforcement Learning},
	abstract = {Recent research has shown the beneﬁt of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-deﬁned, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods.},
	author = {Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},
	langid = {english},
	file = {Ziebart et al. - Maximum Entropy Inverse Reinforcement Learning.pdf:/home/mtoussai/Zotero/storage/7U4XSJJA/Ziebart et al. - Maximum Entropy Inverse Reinforcement Learning.pdf:application/pdf},
}

@inproceedings{2008-syed-ApprenticeshipLearningUsing,
	location = {New York, {NY}, {USA}},
	title = {Apprenticeship learning using linear programming},
	isbn = {978-1-60558-205-4},
	url = {https://dl.acm.org/delete_delete_delete_doi/10.1145/1390156.1390286},
	delete_delete_delete_doi = {10.1145/1390156.1390286},
	series = {{ICML} '08},
	abstract = {In apprenticeship learning, the goal is to learn a policy in a Markov decision process that is at least as good as a policy demonstrated by an expert. The difficulty arises in that the {MDP}'s true reward function is assumed to be unknown. We show how to frame apprenticeship learning as a linear programming problem, and show that using an off-the-shelf {LP} solver to solve this problem results in a substantial improvement in running time over existing methods---up to two orders of magnitude faster in our experiments. Additionally, our approach produces stationary policies, while all existing methods for apprenticeship learning output policies that are "mixed", i.e. randomized combinations of stationary policies. The technique used is general enough to convert any mixed policy to a stationary policy.},
	pages = {1032--1039},
	booktitle = {Proceedings of the 25th international conference on Machine learning},
	delete_delete_delete_publisher = {Association for Computing Machinery},
	author = {Syed, Umar and Bowling, Michael and Schapire, Robert E.},
	urlyear = {2024},
	year = {2008},
	file = {Full Text PDF:/home/mtoussai/Zotero/storage/SZ57AQUU/Syed et al. - 2008 - Apprenticeship learning using linear programming.pdf:application/pdf},
}

@inproceedings{2010-syed-ReductionApprenticeshipLearning,
	title = {A Reduction from Apprenticeship Learning to Classification},
	volume = {23},
	url = {https://proceedings.neurips.cc/paper/2010/hash/5c572eca050594c7bc3c36e7e8ab9550-Abstract.html},
	booktitle = {Advances in Neural Information Processing Systems},
	delete_delete_delete_publisher = {Curran Associates, Inc.},
	author = {Syed, Umar and Schapire, Robert E},
	urlyear = {2024},
	year = {2010},
	file = {Full Text PDF:/home/mtoussai/Zotero/storage/FNZFLSMB/Syed and Schapire - 2010 - A Reduction from Apprenticeship Learning to Classi.pdf:application/pdf},
}

@inproceedings{2004-abbeel-ApprenticeshipLearningInversea,
	location = {Banff, Alberta, Canada},
	title = {Apprenticeship learning via inverse reinforcement learning},
	url = {http://portal.acm.org/citation.cfm?delete_delete_delete_doid=1015330.1015430},
	delete_delete_delete_doi = {10.1145/1015330.1015430},
	booktitle = {Twenty-first international conference},
	pages = {1},
	booktitle = {Twenty-first international conference on Machine learning  - {ICML} '04},
	delete_delete_delete_publisher = {{ACM} Press},
	author = {Abbeel, Pieter and Ng, Andrew Y.},
	urlyear = {2024},
	year = {2004},
	langid = {english},
	file = {Available Version (via Google Scholar):/home/mtoussai/Zotero/storage/BXTLIE5H/Abbeel and Ng - 2004 - Apprenticeship learning via inverse reinforcement .pdf:application/pdf},
}

@inproceedings{2000-ng-AlgorithmsInverseReinforcement,
	title = {Algorithms for inverse reinforcement learning.},
	volume = {1},
	url = {http://www.datascienceassn.org/sites/default/files/Algorithms%20for%20Inverse%20Reinforcement%20Learning.pdf},
	pages = {2},
	booktitle = {Icml},
	author = {Ng, Andrew Y. and Russell, Stuart},
	urlyear = {2024},
	year = {2000},
	delete_delete_delete_note = {Issue: 2},
	file = {Available Version (via Google Scholar):/home/mtoussai/Zotero/storage/ZYFBIX2T/Ng and Russell - 2000 - Algorithms for inverse reinforcement learning..pdf:application/pdf},
}

@misc{2018-fu-LearningRobustRewards,
	title = {Learning Robust Rewards with Adversarial Inverse Reinforcement Learning},
	url = {http://arxiv.org/abs/1710.11248},
	abstract = {Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose adverserial inverse reinforcement learning ({AIRL}), a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that {AIRL} is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that {AIRL} greatly outperforms prior methods in these transfer settings.},
	number = {{arXiv}:1710.11248},
	delete_delete_delete_publisher = {{arXiv}},
	author = {Fu, Justin and Luo, Katie and Levine, Sergey},
	urlyear = {2024},
	year = {2018},
	eprinttype = {arxiv},
	eprint = {1710.11248 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/mtoussai/Zotero/storage/HKYR55CI/Fu et al. - 2018 - Learning Robust Rewards with Adversarial Inverse R.pdf:application/pdf;arXiv.org Snapshot:/home/mtoussai/Zotero/storage/I53R7JDH/1710.html:text/html},
}

@misc{2018-tucker-InverseReinforcementLearning,
	title = {Inverse reinforcement learning for video games},
	url = {http://arxiv.org/abs/1810.10593},
	abstract = {Deep reinforcement learning achieves superhuman performance in a range of video game environments, but requires that a designer manually specify a reward function. It is often easier to provide demonstrations of a target behavior than to design a reward function describing that behavior. Inverse reinforcement learning ({IRL}) algorithms can infer a reward from demonstrations in low-dimensional continuous control environments, but there has been little work on applying {IRL} to high-dimensional video games. In our {CNN}-{AIRL} baseline, we modify the state-of-the-art adversarial {IRL} ({AIRL}) algorithm to use {CNNs} for the generator and discriminator. To stabilize training, we normalize the reward and increase the size of the discriminator training dataset. We additionally learn a low-dimensional state representation using a novel autoencoder architecture tuned for video game environments. This embedding is used as input to the reward network, improving the sample efficiency of expert demonstrations. Our method achieves high-level performance on the simple Catcher video game, substantially outperforming the {CNN}-{AIRL} baseline. We also score points on the Enduro Atari racing game, but do not match expert performance, highlighting the need for further work.},
	number = {{arXiv}:1810.10593},
	delete_delete_delete_publisher = {{arXiv}},
	author = {Tucker, Aaron and Gleave, Adam and Russell, Stuart},
	urlyear = {2024},
	year = {2018},
	eprinttype = {arxiv},
	eprint = {1810.10593 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, I.2.6},
	file = {arXiv Fulltext PDF:/home/mtoussai/Zotero/storage/JRC78XNN/Tucker et al. - 2018 - Inverse reinforcement learning for video games.pdf:application/pdf;arXiv.org Snapshot:/home/mtoussai/Zotero/storage/PRNKIIHB/1810.html:text/html},
}

@incollection{2011-akrour-PreferenceBasedPolicyLearning,
	location = {Berlin, Heidelberg},
	title = {Preference-Based Policy Learning},
	volume = {6911},
	isbn = {978-3-642-23779-9 978-3-642-23780-5},
	url = {https://link.springer.com/10.1007/978-3-642-23780-5_11},
	pages = {12--27},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	delete_delete_delete_publisher = {Springer Berlin Heidelberg},
	author = {Akrour, Riad and Schoenauer, Marc and Sebag, Michele},
	editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
	urlyear = {2024},
	year = {2011},
	langid = {english},
	delete_delete_delete_doi = {10.1007/978-3-642-23780-5_11},
	delete_delete_delete_note = {Series Title: Lecture Notes in Computer Science},
	file = {Available Version (via Google Scholar):/home/mtoussai/Zotero/storage/AFUYMRFP/Akrour et al. - 2011 - Preference-Based Policy Learning.pdf:application/pdf},
}

@book{2017-sadigh-ActivePreferencebasedLearning,
	title = {Active preference-based learning of reward functions},
	url = {https://escholarship.org/uc/item/88k894w7},
	author = {Sadigh, Dorsa and Dragan, Anca D. and Sastry, Shankar and Seshia, Sanjit A.},
	urlyear = {2024},
	year = {2017},
	file = {Available Version (via Google Scholar):/home/mtoussai/Zotero/storage/4JPD4W4C/Sadigh et al. - 2017 - Active preference-based learning of reward functio.pdf:application/pdf},
}

@article{2024-hejna-InversePreferenceLearning,
	title = {Inverse preference learning: Preference-based rl without a reward function},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/3be7859b36d9440372cae0a293f2e4cc-Abstract-Conference.html},
	shorttitle = {Inverse preference learning},
	journal = {Advances in Neural Information Processing Systems},
	author = {Hejna, Joey and Sadigh, Dorsa},
	urlyear = {2024},
	year = {2024},
	file = {Available Version (via Google Scholar):/home/mtoussai/Zotero/storage/SZB6KT8R/Hejna and Sadigh - 2024 - Inverse preference learning Preference-based rl w.pdf:application/pdf},
}

@incollection{2012-akrour-APRILActivePreference,
	location = {Berlin, Heidelberg},
	title = {{APRIL}: Active Preference Learning-Based Reinforcement Learning},
	volume = {7524},
	isbn = {978-3-642-33485-6 978-3-642-33486-3},
	url = {http://link.springer.com/10.1007/978-3-642-33486-3_8},
	shorttitle = {{APRIL}},
	pages = {116--131},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	delete_delete_delete_publisher = {Springer Berlin Heidelberg},
	author = {Akrour, Riad and Schoenauer, Marc and Sebag, Michèle},
	editor = {Flach, Peter A. and De Bie, Tijl and Cristianini, Nello},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urlyear = {2024},
	year = {2012},
	delete_delete_delete_doi = {10.1007/978-3-642-33486-3_8},
	delete_delete_delete_note = {Series Title: Lecture Notes in Computer Science},
	file = {Available Version (via Google Scholar):/home/mtoussai/Zotero/storage/RMLZDAQK/Akrour et al. - 2012 - APRIL Active Preference Learning-Based Reinforcem.pdf:application/pdf},
}

@article{2016-hadfield-menell-CooperativeInverseReinforcement,
	title = {Cooperative inverse reinforcement learning},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html},
	journal = {Advances in neural information processing systems},
	author = {Hadfield-Menell, Dylan and Russell, Stuart J. and Abbeel, Pieter and Dragan, Anca},
	urlyear = {2024},
	year = {2016},
	file = {Available Version (via Google Scholar):/home/mtoussai/Zotero/storage/5JBMV9WZ/Hadfield-Menell et al. - 2016 - Cooperative inverse reinforcement learning.pdf:application/pdf},
}

@article{2017-christiano-DeepReinforcementLearning,
	title = {Deep reinforcement learning from human preferences},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/7017-deep-reinforcement-learning-from-},
	journal = {Advances in neural information processing systems},
	author = {Christiano, Paul F. and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
	urlyear = {2024},
	year = {2017},
	file = {Available Version (via Google Scholar):/home/mtoussai/Zotero/storage/8Y6K28I3/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf:application/pdf},
}

@book{2019-russell-HumanCompatibleAI,
	title = {Human compatible: {AI} and the problem of control},
	url = {https://books.google.com/books?hl=en&lr=&id=Gg-TDwAAQBAJ&oi=fnd&pg=PT8&dq=human+compatible+russell&ots=qoZKXK7gQ0&sig=p4x57HjxfMAVCpQ4O_XcE7J4ECY},
	shorttitle = {Human compatible},
	delete_delete_delete_publisher = {Penguin Uk},
	author = {Russell, Stuart},
	urlyear = {2024},
	year = {2019},
	file = {Available Version (via Google Scholar):/home/mtoussai/Zotero/storage/G4NPFVKQ/Russell - 2019 - Human compatible AI and the problem of control.pdf:application/pdf},
}

@article{2014-goodfellow-GenerativeAdversarialNets,
	title = {Generative adversarial nets},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper/5423-generative-adversarial-nets},
	journal = {Advances in neural information processing systems},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urlyear = {2024},
	year = {2014},
	file = {Available Version (via Google Scholar):/home/mtoussai/Zotero/storage/3SDGHWF6/Goodfellow et al. - 2014 - Generative adversarial nets.pdf:application/pdf},
}

@inproceedings{2023-hejnaiii-FewshotPreferenceLearning,
	title = {Few-shot preference learning for human-in-the-loop rl},
	url = {https://proceedings.mlr.press/v205/iii23a.html},
	pages = {2014--2025},
	booktitle = {Conference on Robot Learning},
	delete_delete_delete_publisher = {{PMLR}},
	author = {Hejna {III}, Donald Joseph and Sadigh, Dorsa},
	urlyear = {2024},
	year = {2023},
	file = {Available Version (via Google Scholar):/home/mtoussai/Zotero/storage/TXAIR35Q/Hejna III and Sadigh - 2023 - Few-shot preference learning for human-in-the-loop.pdf:application/pdf},
}
