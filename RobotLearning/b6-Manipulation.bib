@book{2017-lynch-ModernRobotics,
  title = {Modern Robotics},
  author = {Lynch, Kevin M. and Park, Frank C.},
  year = {2017},
  delete_delete_delete_publisher = {Cambridge University Press},
  url = {https://books.google.com/books?hl=en&lr=&id=5NzFDgAAQBAJ&oi=fnd&pg=PR11&dq=modern+robotics+book&ots=qsJmY4kXPh&sig=o1uhr6h_eJKF33_HBe2xZaT32Ow},
  urlyear = {2024}
}

@online{2017-mahler-DexNetDeepLearning,
  title = {Dex-{{Net}} 2.0: {{Deep Learning}} to {{Plan Robust Grasps}} with {{Synthetic Point Clouds}} and {{Analytic Grasp Metrics}}},
  shorttitle = {Dex-{{Net}} 2.0},
  author = {Mahler, Jeffrey and Liang, Jacky and Niyaz, Sherdil and Laskey, Michael and Doan, Richard and Liu, Xinyu and Ojea, Juan Aparicio and Goldberg, Ken},
  year = {2017},
  eprint = {1703.09312},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.09312},
  urlyear = {2024},
  abstract = {To reduce data collection time for deep learning of robust robotic grasp plans, we explore training from a synthetic dataset of 6.7 million point clouds, grasps, and analytic grasp metrics generated from thousands of 3D models from Dex-Net 1.0 in randomized poses on a table. We use the resulting dataset, Dex-Net 2.0, to train a Grasp Quality Convolutional Neural Network (GQ-CNN) model that rapidly predicts the probability of success of grasps from depth images, where grasps are specified as the planar position, angle, and depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000 trials on an ABB YuMi comparing grasp planning methods on singulated objects suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be used to plan grasps in 0.8sec with a success rate of 93\% on eight known objects with adversarial geometry and is 3x faster than registering point clouds to a precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp planner also has the highest success rate on a dataset of 10 novel rigid objects and achieves 99\% precision (one false positive out of 69 grasps classified as robust) on a dataset of 40 novel household objects, some of which are articulated or deformable. Code, datasets, videos, and supplementary material are available at http://berkeleyautomation.github.io/dex-net .},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics},
  file = {/home/mtoussai/Zotero/storage/PPEMWU45/Mahler et al. - 2017 - Dex-Net 2.0 Deep Learning to Plan Robust Grasps w.pdf;/home/mtoussai/Zotero/storage/DKTLAIRN/1703.html}
}

@article{2017-tenpas-GraspPoseDetection,
  title = {Grasp {{Pose Detection}} in {{Point Clouds}}},
  author = {Ten Pas, Andreas and Gualtieri, Marcus and Saenko, Kate and Platt, Robert},
  year = {2017},
  journal = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {36},
  number = {13-14},
  pages = {1455--1473},
  issn = {0278-3649, 1741-3176},
  delete_delete_delete_doi = {10.1177/0278364917735594},
  url = {http://journals.sagepub.com/delete_delete_delete_doi/10.1177/0278364917735594},
  urlyear = {2024},
  abstract = {Recently, a number of grasp detection methods have been proposed that can be used to localize robotic grasp configurations directly from sensor data without estimating object pose. The underlying idea is to treat grasp perception analogously to object detection in computer vision. These methods take as input a noisy and partially occluded RGBD image or point cloud and produce as output pose estimates of viable grasps, without assuming a known CAD model of the object. Although these methods generalize grasp knowledge to new objects well, they have not yet been demonstrated to be reliable enough for wide use. Many grasp detection methods achieve grasp success rates (grasp successes as a fraction of the total number of grasp attempts) between 75\% and 95\% for novel objects presented in isolation or in light clutter. Not only are these success rates too low for practical grasping applications, but the light clutter scenarios that are evaluated often do not reflect the realities of real-world grasping. This paper proposes a number of innovations that together result in an improvement in grasp detection performance. The specific improvement in performance due to each of our contributions is quantitatively measured either in simulation or on robotic hardware. Ultimately, we report a series of robotic experiments that average a 93\% end-to-end grasp success rate for novel objects presented in dense clutter.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/HCFQITMT/Ten Pas et al. - 2017 - Grasp Pose Detection in Point Clouds.pdf}
}

@inproceedings{2018-kalashnikov-ScalableDeepReinforcement,
  title = {Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation},
  booktitle = {Conference on Robot Learning},
  author = {Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent},
  year = {2018},
  pages = {651--673},
  delete_delete_delete_publisher = {PMLR},
  url = {https://proceedings.mlr.press/v87/kalashnikov18a},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/9UD9H8WQ/Kalashnikov et al. - 2018 - Scalable deep reinforcement learning for vision-ba.pdf}
}

@article{2018-mason-RoboticManipulation,
  title = {Toward {{Robotic Manipulation}}},
  author = {Mason, Matthew T.},
  year = {2018},
  journal = {Annual Review of Control, Robotics, and Autonomous Systems},
  shortjournal = {Annu. Rev. Control Robot. Auton. Syst.},
  volume = {1},
  number = {1},
  pages = {1--28},
  issn = {2573-5144, 2573-5144},
  delete_delete_delete_doi = {10.1146/annurev-control-060117-104848},
  url = {https://www.annualreviews.org/delete_delete_delete_doi/10.1146/annurev-control-060117-104848},
  urlyear = {2024},
  abstract = {This article surveys manipulation, including both biological and robotic manipulation. Biology inspires robotics and demonstrates aspects of manipulation that are far in the future of robotics. Robotics develops concepts and principles that become evident only in the creative process. Robotics also provides a test of our understanding. As Richard Feynman put it: “What I cannot create, I do not understand.”},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/N29Y5BID/Mason - 2018 - Toward Robotic Manipulation.pdf}
}

@inproceedings{2019-liang-PointnetgpdDetectingGrasp,
  title = {Pointnetgpd: {{Detecting}} Grasp Configurations from Point Sets},
  shorttitle = {Pointnetgpd},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Liang, Hongzhuo and Ma, Xiaojian and Li, Shuang and Görner, Michael and Tang, Song and Fang, Bin and Sun, Fuchun and Zhang, Jianwei},
  year = {2019},
  pages = {3629--3635},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/8794435/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/5CJC7JSP/Liang et al. - 2019 - Pointnetgpd Detecting grasp configurations from p.pdf}
}

@inproceedings{2019-mousavian-6dofGraspnetVariational,
  title = {6-Dof Graspnet: {{Variational}} Grasp Generation for Object Manipulation},
  shorttitle = {6-Dof Graspnet},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision},
  author = {Mousavian, Arsalan and Eppner, Clemens and Fox, Dieter},
  year = {2019},
  pages = {2901--2910},
  url = {http://openaccess.thecvf.com/content_ICCV_2019/html/Mousavian_6-DOF_GraspNet_Variational_Grasp_Generation_for_Object_Manipulation_ICCV_2019_paper.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/42AS48WE/Mousavian et al. - 2019 - 6-dof graspnet Variational grasp generation for o.pdf}
}

@inproceedings{2020-fang-Graspnet1billionLargescaleBenchmark,
  title = {Graspnet-1billion: {{A}} Large-Scale Benchmark for General Object Grasping},
  shorttitle = {Graspnet-1billion},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Fang, Hao-Shu and Wang, Chenxi and Gou, Minghao and Lu, Cewu},
  year = {2020},
  pages = {11444--11453},
  url = {http://openaccess.thecvf.com/content_CVPR_2020/html/Fang_GraspNet-1Billion_A_Large-Scale_Benchmark_for_General_Object_Grasping_CVPR_2020_paper.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/I36Y8VH7/Fang et al. - 2020 - Graspnet-1billion A large-scale benchmark for gen.pdf}
}

@article{2020-kleeberger-SurveyLearningBasedRobotic,
  title = {A {{Survey}} on {{Learning-Based Robotic Grasping}}},
  author = {Kleeberger, Kilian and Bormann, Richard and Kraus, Werner and Huber, Marco F.},
  year = {2020},
  journal = {Current Robotics Reports},
  shortjournal = {Curr Robot Rep},
  volume = {1},
  number = {4},
  pages = {239--249},
  issn = {2662-4087},
  delete_delete_delete_doi = {10.1007/s43154-020-00021-6},
  url = {https://delete_delete_delete_doi.org/10.1007/s43154-020-00021-6},
  urlyear = {2024},
  abstract = {This review provides a comprehensive overview of machine learning approaches for vision-based robotic grasping and manipulation. Current trends and developments as well as various criteria for categorization of approaches are provided.},
  langid = {english},
  keywords = {Artificial intelligence,Deep learning,Robotic grasping and manipulation,Sim-to-real transfer,Simulations},
  file = {/home/mtoussai/Zotero/storage/VYEIBQVX/Kleeberger et al. - 2020 - A Survey on Learning-Based Robotic Grasping.pdf}
}

@article{2020-song-GraspingWildLearning,
  title = {Grasping in the Wild: {{Learning}} 6dof Closed-Loop Grasping from Low-Cost Demonstrations},
  shorttitle = {Grasping in the Wild},
  author = {Song, Shuran and Zeng, Andy and Lee, Johnny and Funkhouser, Thomas},
  year = {2020},
  journal = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {3},
  pages = {4978--4985},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/9126187/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/U7573WRM/Song et al. - 2020 - Grasping in the wild Learning 6dof closed-loop gr.pdf}
}

@article{2020-zeng-TossingbotLearningThrow,
  title = {Tossingbot: {{Learning}} to Throw Arbitrary Objects with Residual Physics},
  shorttitle = {Tossingbot},
  author = {Zeng, Andy and Song, Shuran and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},
  year = {2020},
  journal = {IEEE Transactions on Robotics},
  volume = {36},
  number = {4},
  pages = {1307--1319},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/9104757/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/KC8NACZT/Zeng et al. - 2020 - TossingBot Learning to Throw Arbitrary Objects wi.pdf}
}

@inproceedings{2021-breyer-VolumetricGraspingNetwork,
  title = {Volumetric Grasping Network: {{Real-time}} 6 Dof Grasp Detection in Clutter},
  shorttitle = {Volumetric Grasping Network},
  booktitle = {Conference on {{Robot Learning}}},
  author = {Breyer, Michel and Chung, Jen Jen and Ott, Lionel and Siegwart, Roland and Nieto, Juan},
  year = {2021},
  pages = {1602--1611},
  delete_delete_delete_publisher = {PMLR},
  url = {https://proceedings.mlr.press/v155/breyer21a.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/2VHGCFMP/Breyer et al. - 2021 - Volumetric grasping network Real-time 6 dof grasp.pdf}
}

@inproceedings{2021-eppner-AcronymLargescaleGrasp,
  title = {Acronym: {{A}} Large-Scale Grasp Dataset Based on Simulation},
  shorttitle = {Acronym},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Eppner, Clemens and Mousavian, Arsalan and Fox, Dieter},
  year = {2021},
  pages = {6222--6227},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/9560844/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/EU2UEKC8/Eppner et al. - 2021 - Acronym A large-scale grasp dataset based on simu.pdf}
}

@inproceedings{2021-sundermeyer-ContactgraspnetEfficient6dof,
  title = {Contact-Graspnet: {{Efficient}} 6-Dof Grasp Generation in Cluttered Scenes},
  shorttitle = {Contact-Graspnet},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Sundermeyer, Martin and Mousavian, Arsalan and Triebel, Rudolph and Fox, Dieter},
  year = {2021},
  pages = {13438--13444},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/9561877/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/C8GY3NSS/Sundermeyer et al. - 2021 - Contact-graspnet Efficient 6-dof grasp generation.pdf}
}

@article{2022-ha-DeepVisualConstraints,
  title = {Deep Visual Constraints: {{Neural}} Implicit Models for Manipulation Planning from Visual Input},
  shorttitle = {Deep Visual Constraints},
  author = {Ha, Jung-Su and Driess, Danny and Toussaint, Marc},
  year = {2022},
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {4},
  pages = {10857--10864},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/9844753/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/PUZWSE5K/Ha et al. - 2022 - Deep visual constraints Neural implicit models fo.pdf}
}

@incollection{2022-manuelli-KPAMKeyPointAffordances,
  title = {{{KPAM}}: {{KeyPoint Affordances}} for {{Category-Level Robotic Manipulation}}},
  shorttitle = {{{KPAM}}},
  booktitle = {Robotics {{Research}}},
  author = {Manuelli, Lucas and Gao, Wei and Florence, Peter and Tedrake, Russ},
  editor = {Asfour, Tamim and Yoshida, Eiichi and Park, Jaeheung and Christensen, Henrik and Khatib, Oussama},
  year = {2022},
  volume = {20},
  pages = {132--157},
  delete_delete_delete_publisher = {Springer International Publishing},
  location = {Cham},
  delete_delete_delete_doi = {10.1007/978-3-030-95459-8_9},
  url = {https://link.springer.com/10.1007/978-3-030-95459-8_9},
  urlyear = {2024},
  isbn = {978-3-030-95458-1 978-3-030-95459-8},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/MSL38PAU/Manuelli et al. - 2019 - kPAM KeyPoint Affordances for Category-Level Robo.pdf}
}

@inproceedings{2022-simeonov-NeuralDescriptorFields,
  title = {Neural Descriptor Fields: {{Se}} (3)-Equivariant Object Representations for Manipulation},
  shorttitle = {Neural Descriptor Fields},
  booktitle = {2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Simeonov, Anthony and Du, Yilun and Tagliasacchi, Andrea and Tenenbaum, Joshua B. and Rodriguez, Alberto and Agrawal, Pulkit and Sitzmann, Vincent},
  year = {2022},
  pages = {6394--6400},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/9812146/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/GBWCG3SH/Simeonov et al. - 2022 - Neural descriptor fields Se (3)-equivariant objec.pdf}
}

@article{2022-xu-UniversalManipulationPolicy,
  title = {Universal Manipulation Policy Network for Articulated Objects},
  author = {Xu, Zhenjia and He, Zhanpeng and Song, Shuran},
  year = {2022},
  journal = {IEEE robotics and automation letters},
  volume = {7},
  number = {2},
  pages = {2447--2454},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/9681198/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/IUAF37UD/Xu et al. - 2022 - UMPNet Universal Manipulation Policy Network for .pdf}
}

@inproceedings{2023-chi-DiffusionPolicyVisuomotora,
  title = {Diffusion {{Policy}}: {{Visuomotor Policy Learning}} via {{Action Diffusion}}},
  shorttitle = {Diffusion {{Policy}}},
  booktitle = {Robotics: {{Science}} and {{Systems XIX}}},
  author = {Chi, Cheng and Feng, Siyuan and Du, Yilun and Xu, Zhenjia and Cousineau, Eric and Burchfiel, Benjamin and Song, Shuran},
  year = {2023},
  delete_delete_delete_publisher = {{Robotics: Science and Systems Foundation}},
  delete_delete_delete_doi = {10.15607/RSS.2023.XIX.026},
  url = {http://www.roboticsproceedings.org/rss19/p026.pdf},
  urlyear = {2024},
  abstract = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot’s visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details will be publicly available.},
  booktitle = {Robotics: {{Science}} and {{Systems}} 2023},
  isbn = {978-0-9923747-9-2},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/J4YZRG6N/Chi et al. - 2023 - Diffusion Policy Visuomotor Policy Learning via A.pdf}
}

@article{2023-fang-AnygraspRobustEfficient,
  title = {Anygrasp: {{Robust}} and Efficient Grasp Perception in Spatial and Temporal Domains},
  shorttitle = {Anygrasp},
  author = {Fang, Hao-Shu and Wang, Chenxi and Fang, Hongjie and Gou, Minghao and Liu, Jirong and Yan, Hengxu and Liu, Wenhai and Xie, Yichen and Lu, Cewu},
  year = {2023},
  journal = {IEEE Transactions on Robotics},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/10167687/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/VPY7PGCA/Fang et al. - 2023 - Anygrasp Robust and efficient grasp perception in.pdf}
}

@article{2023-newbury-DeepLearningApproaches,
  title = {Deep Learning Approaches to Grasp Synthesis: {{A}} Review},
  shorttitle = {Deep Learning Approaches to Grasp Synthesis},
  author = {Newbury, Rhys and Gu, Morris and Chumbley, Lachlan and Mousavian, Arsalan and Eppner, Clemens and Leitner, Jürgen and Bohg, Jeannette and Morales, Antonio and Asfour, Tamim and Kragic, Danica},
  year = {2023},
  journal = {IEEE Transactions on Robotics},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/10149823/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/A4MGJJAP/Newbury et al. - 2023 - Deep learning approaches to grasp synthesis A rev.pdf}
}

@online{2023-shi-WaypointBasedImitationLearning,
  title = {Waypoint-{{Based Imitation Learning}} for {{Robotic Manipulation}}},
  author = {Shi, Lucy Xiaoyang and Sharma, Archit and Zhao, Tony Z. and Finn, Chelsea},
  year = {2023},
  eprint = {2307.14326},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2307.14326},
  urlyear = {2024},
  abstract = {While imitation learning methods have seen a resurgent interest for robotic manipulation, the well-known problem of compounding errors continues to afflict behavioral cloning (BC). Waypoints can help address this problem by reducing the horizon of the learning problem for BC, and thus, the errors compounded over time. However, waypoint labeling is underspecified, and requires additional human supervision. Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints which when interpolated linearly can approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by up to 25\% in simulation and by 4-28\% on real-world bimanual manipulation tasks, reducing the decision making horizon by up to a factor of 10. Videos and code are available at https://lucys0.github.io/awe/},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/mtoussai/Zotero/storage/C54KEBPP/Shi et al. - 2023 - Waypoint-Based Imitation Learning for Robotic Mani.pdf;/home/mtoussai/Zotero/storage/6K5TBX67/2307.html}
}

@online{2023-tedrake-RoboticManipulationLecture,
  title = {Robotic {{Manipulation}} - {{Lecture Website}}},
  author = {Tedrake, Russ},
  year = {2023},
  url = {https://manipulation.csail.mit.edu/index.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/MF2HIBRF/index.html}
}

@inproceedings{2023-urain-SeDiffusionfieldsLearning,
  title = {Se (3)-Diffusionfields: {{Learning}} Smooth Cost Functions for Joint Grasp and Motion Optimization through Diffusion},
  shorttitle = {Se (3)-Diffusionfields},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Urain, Julen and Funk, Niklas and Peters, Jan and Chalvatzaki, Georgia},
  year = {2023},
  pages = {5923--5930},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/10161569/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/J5CPWDUT/Urain et al. - 2023 - Se (3)-diffusionfields Learning smooth cost funct.pdf}
}

@online{2024-eisner-FlowBot3DLearning3D,
  title = {{{FlowBot3D}}: {{Learning 3D Articulation Flow}} to {{Manipulate Articulated Objects}}},
  shorttitle = {{{FlowBot3D}}},
  author = {Eisner, Ben and Zhang, Harry and Held, David},
  year = {2024},
  eprint = {2205.04382},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2205.04382},
  urlyear = {2024},
  abstract = {We explore a novel method to perceive and manipulate 3D articulated objects that generalizes to enable a robot to articulate unseen classes of objects. We propose a vision-based system that learns to predict the potential motions of the parts of a variety of articulated objects to guide downstream motion planning of the system to articulate the objects. To predict the object motions, we train a neural network to output a dense vector field representing the point-wise motion direction of the points in the point cloud under articulation. We then deploy an analytical motion planner based on this vector field to achieve a policy that yields maximum articulation. We train the vision system entirely in simulation, and we demonstrate the capability of our system to generalize to unseen object instances and novel categories in both simulation and the real world, deploying our policy on a Sawyer robot with no finetuning. Results show that our system achieves state-of-the-art performance in both simulated and real-world experiments.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/home/mtoussai/Zotero/storage/2D2BVESP/Eisner et al. - 2024 - FlowBot3D Learning 3D Articulation Flow to Manipu.pdf;/home/mtoussai/Zotero/storage/QLZ2HUXS/2205.html}
}

@online{2024-gao-BiKVILKeypointsbasedVisual,
  title = {Bi-{{KVIL}}: {{Keypoints-based Visual Imitation Learning}} of {{Bimanual Manipulation Tasks}}},
  shorttitle = {Bi-{{KVIL}}},
  author = {Gao, Jianfeng and Jin, Xiaoshu and Krebs, Franziska and Jaquier, Noémie and Asfour, Tamim},
  year = {2024},
  eprint = {2403.03270},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2403.03270},
  urlyear = {2024},
  abstract = {Visual imitation learning has achieved impressive progress in learning unimanual manipulation tasks from a small set of visual observations, thanks to the latest advances in computer vision. However, learning bimanual coordination strategies and complex object relations from bimanual visual demonstrations, as well as generalizing them to categorical objects in novel cluttered scenes remain unsolved challenges. In this paper, we extend our previous work on keypoints-based visual imitation learning (\textbackslash mbox\{K-VIL\})\textasciitilde\textbackslash cite\{gao\_kvil\_2023\} to bimanual manipulation tasks. The proposed Bi-KVIL jointly extracts so-called \textbackslash emph\{Hybrid Master-Slave Relationships\} (HMSR) among objects and hands, bimanual coordination strategies, and sub-symbolic task representations. Our bimanual task representation is object-centric, embodiment-independent, and viewpoint-invariant, thus generalizing well to categorical objects in novel scenes. We evaluate our approach in various real-world applications, showcasing its ability to learn fine-grained bimanual manipulation tasks from a small number of human demonstration videos. Videos and source code are available at https://sites.google.com/view/bi-kvil.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics},
  file = {/home/mtoussai/Zotero/storage/GDGJTWMI/Gao et al. - 2024 - Bi-KVIL Keypoints-based Visual Imitation Learning.pdf;/home/mtoussai/Zotero/storage/FNIREUFB/2403.html}
}
