@inproceedings{1999-ng-PolicyInvarianceReward,
  title = {Policy Invariance under Reward Transformations: {{Theory}} and Application to Reward Shaping},
  shorttitle = {Policy Invariance under Reward Transformations},
  booktitle = {Icml},
  author = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart},
  year = {1999},
  volume = {99},
  pages = {278--287},
  url = {https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/F323NFHJ/Ng et al. - 1999 - Policy invariance under reward transformations Th.pdf}
}

@article{2002-brafman-RmaxaGeneralPolynomial,
  title = {R-Max-a General Polynomial Time Algorithm for near-Optimal Reinforcement Learning},
  author = {Brafman, Ronen I. and Tennenholtz, Moshe},
  year = {2002},
  journal = {Journal of Machine Learning Research},
  volume = {3},
  pages = {213--231},
  url = {https://www.jmlr.org/papers/volume3/brafman02a/brafman02a.pdf?ref=https://githubhelp.com},
  urlyear = {2024},
  issue = {Oct},
  file = {/home/mtoussai/Zotero/storage/3VLQIT2M/Brafman and Tennenholtz - 2002 - R-max-a general polynomial time algorithm for near.pdf}
}

@article{2002-kearns-NearoptimalReinforcementLearning,
  title = {Near-Optimal Reinforcement Learning in Polynomial Time},
  author = {Kearns, Michael and Singh, Satinder},
  year = {2002},
  journal = {Machine Learning},
  volume = {49},
  number = {2/3},
  pages = {209--232},
  issn = {08856125},
  delete_delete_delete_doi = {10.1023/A:1017984413808},
  url = {http://link.springer.com/10.1023/A:1017984413808},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/JP3VWHYA/Kearns and Singh - 2002 - [No title found].pdf}
}

@article{2003-lagoudakis-LeastsquaresPolicyIteration,
  title = {Least-Squares Policy Iteration},
  author = {Lagoudakis, Michail G. and Parr, Ronald},
  year = {2003},
  journal = {The Journal of Machine Learning Research},
  volume = {4},
  pages = {1107--1149},
  delete_delete_delete_publisher = {JMLR. org},
  url = {https://www.jmlr.org/papers/volume4/temp/jmlr.4.6.zip},
  urlyear = {2024}
}

@inproceedings{2009-kober-LearningMotorPrimitives,
  title = {Learning Motor Primitives for Robotics},
  booktitle = {2009 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Kober, Jens and Peters, Jan},
  year = {2009},
  pages = {2112--2118},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/5152577/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/XQVRD7Q4/Kober and Peters - 2009 - Learning motor primitives for robotics.pdf}
}

@article{2010-abbeel-AutonomousHelicopterAerobatics,
  title = {Autonomous {{Helicopter Aerobatics}} through {{Apprenticeship Learning}}},
  author = {Abbeel, Pieter and Coates, Adam and Ng, Andrew Y.},
  year = {2010},
  journal = {The International Journal of Robotics Research},
  volume = {29},
  number = {13},
  pages = {1608--1639},
  delete_delete_delete_publisher = {SAGE Publications Ltd STM},
  issn = {0278-3649},
  delete_delete_delete_doi = {10.1177/0278364910371999},
  url = {https://delete_delete_delete_doi.org/10.1177/0278364910371999},
  urlyear = {2024},
  abstract = {Autonomous helicopter flight is widely regarded to be a highly challenging control problem. Despite this fact, human experts can reliably fly helicopters through a wide range of maneuvers, including aerobatic maneuvers at the edge of the helicopter’s capabilities. We present apprenticeship learning algorithms, which leverage expert demonstrations to efficiently learn good controllers for tasks being demonstrated by an expert. These apprenticeship learning algorithms have enabled us to significantly extend the state of the art in autonomous helicopter aerobatics. Our experimental results include the first autonomous execution of a wide range of maneuvers, including but not limited to in-place flips, in-place rolls, loops and hurricanes, and even auto-rotation landings, chaos and tic-tocs, which only exceptional human pilots can perform. Our results also include complete airshows, which require autonomous transitions between many of these maneuvers. Our controllers perform as well as, and often even better than, our expert pilot.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/5HZNU5CM/Abbeel et al. - 2010 - Autonomous Helicopter Aerobatics through Apprentic.pdf}
}

@inproceedings{2010-maillard-FinitesampleAnalysisBellman,
  title = {Finite-Sample Analysis of {{Bellman}} Residual Minimization},
  booktitle = {Proceedings of 2nd {{Asian Conference}} on {{Machine Learning}}},
  author = {Maillard, Odalric-Ambrym and Munos, Rémi and Lazaric, Alessandro and Ghavamzadeh, Mohammad},
  year = {2010},
  pages = {299--314},
  delete_delete_delete_publisher = {{JMLR Workshop and Conference Proceedings}},
  url = {http://proceedings.mlr.press/v13/maillard10a.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/8N257LFZ/Maillard et al. - 2010 - Finite-sample analysis of Bellman residual minimiz.pdf}
}

@inproceedings{2013-levine-GuidedPolicySearch,
  title = {Guided Policy Search},
  booktitle = {International Conference on Machine Learning},
  author = {Levine, Sergey and Koltun, Vladlen},
  year = {2013},
  pages = {1--9},
  delete_delete_delete_publisher = {PMLR},
  url = {https://proceedings.mlr.press/v28/levine13.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/NRYNXVL2/Levine and Koltun - 2013 - Guided policy search.pdf}
}

@inproceedings{2014-silver-DeterministicPolicyGradient,
  title = {Deterministic Policy Gradient Algorithms},
  booktitle = {International Conference on Machine Learning},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  year = {2014},
  pages = {387--395},
  delete_delete_delete_publisher = {Pmlr},
  url = {http://proceedings.mlr.press/v32/silver14.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/IYVSCM9I/Silver et al. - 2014 - Deterministic policy gradient algorithms.pdf}
}

@inproceedings{2015-hausknecht-DeepRecurrentQlearning,
  title = {Deep Recurrent Q-Learning for Partially Observable Mdps},
  booktitle = {2015 Aaai Fall Symposium Series},
  author = {Hausknecht, Matthew and Stone, Peter},
  year = {2015},
  url = {https://cdn.aaai.org/ocs/11673/11673-51288-1-PB.pdf},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/FVQWYNPS/Hausknecht and Stone - 2015 - Deep recurrent q-learning for partially observable.pdf}
}

@article{2015-mnih-HumanlevelControlDeep,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg},
  year = {2015},
  journal = {nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  delete_delete_delete_publisher = {Nature Publishing Group UK London},
  url = {https://www.nature.com/articles/nature14236},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/LE7I9KX2/nature14236.html}
}

@article{2017-geist-BellmanResidualBad,
  title = {Is the {{Bellman}} Residual a Bad Proxy?},
  author = {Geist, Matthieu and Piot, Bilal and Pietquin, Olivier},
  year = {2017},
  journal = {Advances in Neural Information Processing Systems},
  volume = {30},
  url = {https://proceedings.neurips.cc/paper/2017/hash/e0ab531ec312161511493b002f9be2ee-Abstract.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/94G45M84/Geist et al. - 2017 - Is the Bellman residual a bad proxy.pdf}
}

@online{2017-salimans-EvolutionStrategiesScalable,
  title = {Evolution {{Strategies}} as a {{Scalable Alternative}} to {{Reinforcement Learning}}},
  author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  year = {2017},
  eprint = {1703.03864},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1703.03864},
  urlyear = {2024},
  abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/mtoussai/Zotero/storage/4WKKNJ63/Salimans et al. - 2017 - Evolution Strategies as a Scalable Alternative to .pdf;/home/mtoussai/Zotero/storage/3TK9DIJI/1703.html}
}

@inproceedings{2017-tobin-DomainRandomizationTransferring,
  title = {Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World},
  booktitle = {2017 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems ({{IROS}})},
  author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  year = {2017},
  pages = {23--30},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/8202133/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/S828W577/Tobin et al. - 2017 - Domain Randomization for Transferring Deep Neural .pdf}
}

@inproceedings{2018-fujimoto-AddressingFunctionApproximation,
  title = {Addressing Function Approximation Error in Actor-Critic Methods},
  booktitle = {International Conference on Machine Learning},
  author = {Fujimoto, Scott and Hoof, Herke and Meger, David},
  year = {2018},
  pages = {1587--1596},
  delete_delete_delete_publisher = {PMLR},
  url = {https://proceedings.mlr.press/v80/fujimoto18a.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/IFMWMVEZ/Fujimoto et al. - 2018 - Addressing function approximation error in actor-c.pdf}
}

@inproceedings{2018-haarnoja-SoftActorcriticOffpolicy,
  title = {Soft Actor-Critic: {{Off-policy}} Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  shorttitle = {Soft Actor-Critic},
  booktitle = {International Conference on Machine Learning},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  pages = {1861--1870},
  delete_delete_delete_publisher = {PMLR},
  url = {https://proceedings.mlr.press/v80/haarnoja18b},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/5ZVTAITJ/Haarnoja et al. - 2018 - Soft actor-critic Off-policy maximum entropy deep.pdf}
}

@inproceedings{2018-hessel-RainbowCombiningImprovements,
  title = {Rainbow: {{Combining}} Improvements in Deep Reinforcement Learning},
  shorttitle = {Rainbow},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  year = {2018},
  volume = {32},
  number = {1},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/11796},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/QUBKXVWV/Hessel et al. - 2018 - Rainbow Combining improvements in deep reinforcem.pdf}
}

@online{2018-plappert-ParameterSpaceNoise,
  title = {Parameter {{Space Noise}} for {{Exploration}}},
  author = {Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y. and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Andrychowicz, Marcin},
  year = {2018},
  eprint = {1706.01905},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1706.01905},
  urlyear = {2024},
  abstract = {Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/mtoussai/Zotero/storage/3X3IFXHV/Plappert et al. - 2018 - Parameter Space Noise for Exploration.pdf;/home/mtoussai/Zotero/storage/U6MPXGBZ/1706.html}
}

@online{2018-such-DeepNeuroevolutionGenetic,
  title = {Deep {{Neuroevolution}}: {{Genetic Algorithms Are}} a {{Competitive Alternative}} for {{Training Deep Neural Networks}} for {{Reinforcement Learning}}},
  shorttitle = {Deep {{Neuroevolution}}},
  author = {Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2018},
  eprint = {1712.06567},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1712.06567},
  urlyear = {2024},
  abstract = {Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of neuroevolution techniques that improve performance. We demonstrate the latter by showing that combining DNNs with novelty search, which encourages exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g.\textbackslash{} DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA is faster than ES, A3C, and DQN (it can train Atari in \$\{\textbackslash raise.17ex\textbackslash hbox\{\$\textbackslash scriptstyle\textbackslash sim\$\}\}\$4 hours on one desktop or \$\{\textbackslash raise.17ex\textbackslash hbox\{\$\textbackslash scriptstyle\textbackslash sim\$\}\}\$1 hour distributed on 720 cores), and enables a state-of-the-art, up to 10,000-fold compact encoding technique.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/mtoussai/Zotero/storage/UPTQ8JRE/Such et al. - 2018 - Deep Neuroevolution Genetic Algorithms Are a Comp.pdf;/home/mtoussai/Zotero/storage/HRLK94NF/1712.html}
}

@inproceedings{2019-fujimoto-OffpolicyDeepReinforcement,
  title = {Off-Policy Deep Reinforcement Learning without Exploration},
  booktitle = {International Conference on Machine Learning},
  author = {Fujimoto, Scott and Meger, David and Precup, Doina},
  year = {2019},
  pages = {2052--2062},
  delete_delete_delete_publisher = {PMLR},
  url = {http://proceedings.mlr.press/v97/fujimoto19a.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/23BZB8Q4/Fujimoto et al. - 2019 - Off-policy deep reinforcement learning without exp.pdf}
}

@online{2019-lillicrap-ContinuousControlDeep,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2019},
  eprint = {1509.02971},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1509.02971},
  urlyear = {2024},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/mtoussai/Zotero/storage/RKIY228X/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learnin.pdf;/home/mtoussai/Zotero/storage/LS3EXCL8/1509.html}
}

@inproceedings{2019-molchanov-SimtoMultiReal,
  title = {Sim-to-({{Multi}})-{{Real}}: {{Transfer}} of {{Low-Level Robust Control Policies}} to {{Multiple Quadrotors}}},
  shorttitle = {Sim-to-({{Multi}})-{{Real}}},
  booktitle = {2019 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Molchanov, Artem and Chen, Tao and Hönig, Wolfgang and Preiss, James A. and Ayanian, Nora and Sukhatme, Gaurav S.},
  year = {2019},
  pages = {59--66},
  issn = {2153-0866},
  delete_delete_delete_doi = {10.1109/IROS40897.2019.8967695},
  url = {https://ieeexplore.ieee.org/document/8967695},
  urlyear = {2024},
  abstract = {Quadrotor stabilizing controllers often require careful, model-specific tuning for safe operation. We use reinforcement learning to train policies in simulation that transfer remarkably well to multiple different physical quadrotors. Our policies are low-level, i.e., we map the rotorcrafts' state directly to the motor outputs. The trained control policies are very robust to external disturbances and can withstand harsh initial conditions such as throws. We show how different training methodologies (change of the cost function, modeling of noise, use of domain randomization) might affect flight performance. To the best of our knowledge, this is the first work that demonstrates that a simple neural network can learn a robust stabilizing low-level quadrotor controller (without the use of a stabilizing PD controller) that is shown to generalize to multiple quadrotors. The video of our experiments can be found at https://sites.google.com/view/sim-to-multi-quad.},
  booktitle = {2019 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  file = {/home/mtoussai/Zotero/storage/NZIGZUEJ/Molchanov et al. - 2019 - Sim-to-(Multi)-Real Transfer of Low-Level Robust .pdf;/home/mtoussai/Zotero/storage/8XWXZX8F/8967695.html}
}

@inproceedings{2019-saleh-DeterministicBellmanResidual,
  title = {Deterministic Bellman Residual Minimization},
  booktitle = {Proceedings of {{Optimization Foundations}} for {{Reinforcement Learning Workshop}} at {{NeurIPS}}},
  author = {Saleh, Ehsan and Jiang, Nan},
  year = {2019},
  url = {https://optrl2019.github.io/assets/accepted_papers/8.pdf},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/6GHDUL4Q/Saleh and Jiang - 2019 - Deterministic bellman residual minimization.pdf}
}

@inproceedings{2020-chen-LearningCheating,
  title = {Learning by Cheating},
  booktitle = {Conference on {{Robot Learning}}},
  author = {Chen, Dian and Zhou, Brady and Koltun, Vladlen and Krähenbühl, Philipp},
  year = {2020},
  pages = {66--75},
  delete_delete_delete_publisher = {PMLR},
  url = {http://proceedings.mlr.press/v100/chen20a.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/5YI4TLHU/Chen et al. - 2020 - Learning by cheating.pdf}
}

@online{2020-hafner-DreamControlLearning,
  title = {Dream to {{Control}}: {{Learning Behaviors}} by {{Latent Imagination}}},
  shorttitle = {Dream to {{Control}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  year = {2020},
  eprint = {1912.01603},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1912.01603},
  urlyear = {2024},
  abstract = {Learned world models summarize an agent’s experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/mtoussai/Zotero/storage/TQ976CLY/Hafner et al. - 2020 - Dream to Control Learning Behaviors by Latent Ima.pdf}
}

@article{2020-lee-LearningQuadrupedalLocomotion,
  title = {Learning Quadrupedal Locomotion over Challenging Terrain},
  author = {Lee, Joonho and Hwangbo, Jemin and Wellhausen, Lorenz and Koltun, Vladlen and Hutter, Marco},
  year = {2020},
  journal = {Science Robotics},
  shortjournal = {Sci. Robot.},
  volume = {5},
  number = {47},
  pages = {eabc5986},
  issn = {2470-9476},
  delete_delete_delete_doi = {10.1126/scirobotics.abc5986},
  url = {https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/448343/1/2020_science_robotics_lee_locomotion.pdf},
  urlyear = {2024},
  abstract = {A learning-based locomotion controller enables a quadrupedal ANYmal robot to traverse challenging natural environments.           ,              Legged locomotion can extend the operational domain of robots to some of the most challenging environments on Earth. However, conventional controllers for legged locomotion are based on elaborate state machines that explicitly trigger the execution of motion primitives and reflexes. These designs have increased in complexity but fallen short of the generality and robustness of animal locomotion. Here, we present a robust controller for blind quadrupedal locomotion in challenging natural environments. Our approach incorporates proprioceptive feedback in locomotion control and demonstrates zero-shot generalization from simulation to natural environments. The controller is trained by reinforcement learning in simulation. The controller is driven by a neural network policy that acts on a stream of proprioceptive signals. The controller retains its robustness under conditions that were never encountered during training: deformable terrains such as mud and snow, dynamic footholds such as rubble, and overground impediments such as thick vegetation and gushing water. The presented work indicates that robust locomotion in natural environments can be achieved by training in simple domains.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/CHNMBD8E/Lee et al. - 2020 - Learning quadrupedal locomotion over challenging t.pdf}
}

@article{2021-fujimoto-MinimalistApproachOffline,
  title = {A Minimalist Approach to Offline Reinforcement Learning},
  author = {Fujimoto, Scott and Gu, Shixiang Shane},
  year = {2021},
  journal = {Advances in neural information processing systems},
  volume = {34},
  pages = {20132--20145},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/a8166da05c5a094f7dc03724b41886e5-Abstract.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/QDTQ3RMA/Fujimoto and Gu - 2021 - A minimalist approach to offline reinforcement lea.pdf}
}

@unpublished{2021-pertsch-DemonstrationGuidedReinforcementLearning,
  title = {Demonstration-{{Guided}} Reinforcement Learning with Learned Skills},
  author = {Pertsch, Karl and Lee, Youngwoon and Wu, Yue and Lim, Joseph J.},
  year = {2021},
  eprint = {2107.10253},
  eprinttype = {arXiv}
}

@inproceedings{2022-eberhard-PinkNoiseAll,
  title = {Pink Noise Is All You Need: {{Colored}} Noise Exploration in Deep Reinforcement Learning},
  shorttitle = {Pink Noise Is All You Need},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Eberhard, Onno and Hollenstein, Jakob and Pinneri, Cristina and Martius, Georg},
  year = {2022},
  url = {https://openreview.net/forum?id=hQ9V5QN27eS},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/AGNT8JKJ/Eberhard et al. - 2022 - Pink noise is all you need Colored noise explorat.pdf}
}

@inproceedings{2022-sinha-S4rlSurprisinglySimple,
  title = {S4rl: {{Surprisingly}} Simple Self-Supervision for Offline Reinforcement Learning in Robotics},
  shorttitle = {S4rl},
  booktitle = {Conference on {{Robot Learning}}},
  author = {Sinha, Samarth and Mandlekar, Ajay and Garg, Animesh},
  year = {2022},
  pages = {907--917},
  delete_delete_delete_publisher = {PMLR},
  url = {https://proceedings.mlr.press/v164/sinha22a.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/LFS2KERX/Sinha et al. - 2022 - S4rl Surprisingly simple self-supervision for off.pdf}
}

@article{2022-wurman-OutracingChampionGran,
  title = {Outracing Champion {{Gran Turismo}} Drivers with Deep Reinforcement Learning},
  author = {Wurman, Peter R. and Barrett, Samuel and Kawamoto, Kenta and MacGlashan, James and Subramanian, Kaushik and Walsh, Thomas J. and Capobianco, Roberto and Devlic, Alisa and Eckert, Franziska and Fuchs, Florian and Gilpin, Leilani and Khandelwal, Piyush and Kompella, Varun and Lin, HaoChih and MacAlpine, Patrick and Oller, Declan and Seno, Takuma and Sherstan, Craig and Thomure, Michael D. and Aghabozorgi, Houmehr and Barrett, Leon and Douglas, Rory and Whitehead, Dion and Dürr, Peter and Stone, Peter and Spranger, Michael and Kitano, Hiroaki},
  year = {2022},
  journal = {Nature},
  volume = {602},
  number = {7896},
  pages = {223--228},
  delete_delete_delete_publisher = {Nature Publishing Group},
  issn = {1476-4687},
  delete_delete_delete_doi = {10.1038/s41586-021-04357-7},
  url = {https://www.nature.com/articles/s41586-021-04357-7},
  urlyear = {2024},
  abstract = {Many potential applications of artificial intelligence involve making real-time decisions in physical systems while interacting with humans. Automobile racing represents an extreme example of these conditions; drivers must execute complex tactical manoeuvres to pass or block opponents while operating their vehicles at their traction limits1. Racing simulations, such as the PlayStation game Gran Turismo, faithfully reproduce the non-linear control challenges of real race cars while also encapsulating the complex multi-agent interactions. Here we describe how we trained agents for Gran Turismo that can compete with the world’s best e-sports drivers. We combine state-of-the-art, model-free, deep reinforcement learning algorithms with mixed-scenario training to learn an integrated control policy that combines exceptional speed with impressive tactics. In addition, we construct a reward function that enables the agent to be competitive while adhering to racing’s important, but under-specified, sportsmanship rules. We demonstrate the capabilities of our agent, Gran Turismo Sophy, by winning a head-to-head competition against four of the world’s best Gran Turismo drivers. By describing how we trained championship-level racers, we demonstrate the possibilities and challenges of using these techniques to control complex dynamical systems in domains where agents must respect imprecisely defined human norms.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/Q585Q8CZ/Wurman et al. - 2022 - Outracing champion Gran Turismo drivers with deep .pdf}
}

@article{2023-kaufmann-ChampionlevelDroneRacing,
  title = {Champion-Level Drone Racing Using Deep Reinforcement Learning},
  author = {Kaufmann, Elia and Bauersfeld, Leonard and Loquercio, Antonio and Müller, Matthias and Koltun, Vladlen and Scaramuzza, Davide},
  year = {2023},
  journal = {Nature},
  volume = {620},
  number = {7976},
  pages = {982--987},
  delete_delete_delete_publisher = {Nature Publishing Group},
  issn = {1476-4687},
  delete_delete_delete_doi = {10.1038/s41586-023-06419-4},
  url = {https://www.nature.com/articles/s41586-023-06419-4},
  urlyear = {2024},
  abstract = {First-person view (FPV) drone racing is a televised sport in which professional competitors pilot high-speed aircraft through a 3D circuit. Each pilot sees the environment from the perspective of their drone by means of video streamed from an onboard camera. Reaching the level of professional pilots with an autonomous drone is challenging because the robot needs to fly at its physical limits while estimating its speed and location in the circuit exclusively from onboard sensors1. Here we introduce Swift, an autonomous system that can race physical vehicles at the level of the human world champions. The system combines deep reinforcement learning (RL) in simulation with data collected in the physical world. Swift competed against three human champions, including the world champions of two international leagues, in real-world head-to-head races. Swift won several races against each of the human champions and demonstrated the fastest recorded race time. This work represents a milestone for mobile robotics and machine intelligence2, which may inspire the deployment of hybrid learning-based solutions in other physical systems.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/XS2XHDPM/Kaufmann et al. - 2023 - Champion-level drone racing using deep reinforceme.pdf}
}

@online{2023-kumar-PreTrainingRobotsOffline,
  title = {Pre-{{Training}} for {{Robots}}: {{Offline RL Enables Learning New Tasks}} from a {{Handful}} of {{Trials}}},
  shorttitle = {Pre-{{Training}} for {{Robots}}},
  author = {Kumar, Aviral and Singh, Anikait and Ebert, Frederik and Nakamoto, Mitsuhiko and Yang, Yanlai and Finn, Chelsea and Levine, Sergey},
  year = {2023},
  eprint = {2210.05178},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.05178},
  urlyear = {2024},
  abstract = {Progress in deep learning highlights the tremendous potential of utilizing diverse robotic datasets for attaining effective generalization and makes it enticing to consider leveraging broad datasets for attaining robust generalization in robotic learning as well. However, in practice, we often want to learn a new skill in a new environment that is unlikely to be contained in the prior data. Therefore we ask: how can we leverage existing diverse offline datasets in combination with small amounts of task-specific data to solve new tasks, while still enjoying the generalization benefits of training on large amounts of data? In this paper, we demonstrate that end-to-end offline RL can be an effective approach for delete_delete_delete_doing this, without the need for any representation learning or vision-based pre-training. We present pre-training for robots (PTR), a framework based on offline RL that attempts to effectively learn new tasks by combining pre-training on existing robotic datasets with rapid fine-tuning on a new task, with as few as 10 demonstrations. PTR utilizes an existing offline RL method, conservative Q-learning (CQL), but extends it to include several crucial design decisions that enable PTR to actually work and outperform a variety of prior methods. To our knowledge, PTR is the first RL method that succeeds at learning new tasks in a new domain on a real WidowX robot with as few as 10 task demonstrations, by effectively leveraging an existing dataset of diverse multi-task robot data collected in a variety of toy kitchens. We also demonstrate that PTR can enable effective autonomous fine-tuning and improvement in a handful of trials, without needing any demonstrations. An accompanying overview video can be found in the supplementary material and at thi URL: https://sites.google.com/view/ptr-final/},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/mtoussai/Zotero/storage/UTZWNQLZ/Kumar et al. - 2023 - Pre-Training for Robots Offline RL Enables Learni.pdf;/home/mtoussai/Zotero/storage/BT7HGIB2/2210.html}
}
