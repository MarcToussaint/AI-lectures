@article{-duan-OneShotImitationLearning,
  title = {One-{{Shot Imitation Learning}}},
  author = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly and Ho, OpenAI Jonathan and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
  abstract = {Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/UYQLSYU3/Duan et al. - One-Shot Imitation Learning.pdf}
}

@article{1988-pomerleau-AlvinnAutonomousLand,
  title = {Alvinn: {{An}} Autonomous Land Vehicle in a Neural Network},
  shorttitle = {Alvinn},
  author = {Pomerleau, Dean A.},
  year = {1988},
  journal = {Advances in neural information processing systems},
  volume = {1},
  url = {https://proceedings.neurips.cc/paper/1988/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/HWYQXFWK/Pomerleau - 1988 - Alvinn An autonomous land vehicle in a neural net.pdf}
}

@inproceedings{1997-atkeson-RobotLearningDemonstration,
  title = {Robot Learning from Demonstration},
  booktitle = {{{ICML}}},
  author = {Atkeson, Christopher G. and Schaal, Stefan},
  year = {1997},
  volume = {97},
  pages = {12--20},
  url = {https://mcgovern-fagg.org/amy_html/courses/cs5973_fall2005/lfd.pdf},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/LE72XZRU/Atkeson and Schaal - 1997 - Robot learning from demonstration.pdf}
}

@article{2003-schaal-ComputationalApproachesMotor,
  title = {Computational Approaches to Motor Learning by Imitation},
  author = {Schaal, Stefan and Ijspeert, Auke and Billard, Aude},
  editor = {Frith, C.D. and Wolpert, D.M.},
  year = {2003},
  journal = {Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences},
  shortjournal = {Phil. Trans. R. Soc. Lond. B},
  volume = {358},
  number = {1431},
  pages = {537--547},
  issn = {0962-8436, 1471-2970},
  delete_delete_delete_doi = {10.1098/rstb.2002.1258},
  url = {https://royalsocietypublishing.org/delete_delete_delete_doi/10.1098/rstb.2002.1258},
  urlyear = {2024},
  abstract = {Movement imitation requires a complex set of mechanisms that map an observed movement of a teacher onto one's own movement apparatus. Relevant problems include movement recognition, pose estimation, pose tracking, body correspondence, coordinate transformation from external to egocentric space, matching of observed against previously learned movement, resolution of redundant degrees–of–freedom that are unconstrained by the observation, suitable movement representations for imitation, modularization of motor control, etc. All of these topics by themselves are active research problems in computational and neurobiological sciences, such that their combination into a complete imitation system remains a daunting undertaking—indeed, one could argue that we need to understand the complete perception–action loop. As a strategy to untangle the complexity of imitation, this paper will examine imitation purely from a computational point of view, i.e. we will review statistical and mathematical approaches that have been suggested for tackling parts of the imitation problem, and discuss their merits, disadvantages and underlying principles. Given the focus on action recognition of other contributions in this special issue, this paper will primarily emphasize the motor side of imitation, assuming that a perceptual system has already identified important features of a demonstrated movement and created their corresponding spatial information. Based on the formalization of motor control in terms of control policies and their associated performance criteria, useful taxonomies of imitation learning can be generated that clarify different approaches and future research directions.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/4JP8TFM3/Schaal et al. - 2003 - Computational approaches to motor learning by imit.pdf}
}

@inproceedings{2004-abbeel-ApprenticeshipLearningInverse,
  title = {Apprenticeship Learning via Inverse Reinforcement Learning},
  booktitle = {Proceedings of the Twenty-First International Conference on {{Machine}} Learning},
  author = {Abbeel, Pieter and Ng, Andrew Y.},
  year = {2004},
  series = {{{ICML}} '04},
  pages = {1},
  delete_delete_delete_publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  delete_delete_delete_doi = {10.1145/1015330.1015430},
  url = {https://dl.acm.org/delete_delete_delete_doi/10.1145/1015330.1015430},
  urlyear = {2024},
  abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
  isbn = {978-1-58113-838-2},
  file = {/home/mtoussai/Zotero/storage/RYHNMSCZ/Abbeel and Ng - 2004 - Apprenticeship learning via inverse reinforcement .pdf}
}

@inproceedings{2007-calinon-IncrementalLearningGestures,
  title = {Incremental Learning of Gestures by Imitation in a Humanoid Robot},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE}} International Conference on {{Human-robot}} Interaction},
  author = {Calinon, Sylvain and Billard, Aude},
  year = {2007},
  pages = {255--262},
  delete_delete_delete_publisher = {ACM},
  location = {Arlington Virginia USA},
  delete_delete_delete_doi = {10.1145/1228716.1228751},
  url = {https://dl.acm.org/delete_delete_delete_doi/10.1145/1228716.1228751},
  urlyear = {2024},
  booktitle = {{{HRI07}}: {{International Conference}} on {{Human Robot Interaction}}},
  isbn = {978-1-59593-617-2},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/NPBZGNEN/Calinon and Billard - 2007 - Incremental learning of gestures by imitation in a.pdf}
}

@inproceedings{2007-syed-GameTheoreticApproachApprenticeship,
  title = {A {{Game-Theoretic Approach}} to {{Apprenticeship Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Syed, Umar and Schapire, Robert E},
  year = {2007},
  volume = {20},
  delete_delete_delete_publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2007/hash/ca3ec598002d2e7662e2ef4bdd58278b-Abstract.html},
  urlyear = {2024},
  abstract = {We study the problem of an apprentice learning to behave in an environment with an unknown reward function by observing the behavior of an expert. We follow on the work of Abbeel and Ng [1] who considered a framework in which the true reward function is assumed to be a linear combination of a set of known and observable features. We give a new algorithm that, like theirs, is guaranteed to learn a policy that is nearly as good as the expert's, given enough examples. However, unlike their algorithm, we show that ours may produce a policy that is substantially better than the expert's. Moreover, our algorithm is computationally faster, is easier to implement, and can be applied even in the absence of an expert. The method is based on a game-theoretic view of the problem, which leads naturally to a direct application of the multiplicative-weights algorithm of Freund and Schapire [2] for playing repeated matrix games. In addition to our formal presentation and analysis of the new algorithm, we sketch how the method can be applied when the transition function itself is unknown, and we provide an experimental demonstration of the algorithm on a toy video-game environment.},
  file = {/home/mtoussai/Zotero/storage/YE4WPIQ3/Syed and Schapire - 2007 - A Game-Theoretic Approach to Apprenticeship Learni.pdf}
}

@inproceedings{2008-do-ImitationHumanMotion,
  title = {Imitation of Human Motion on a Humanoid Robot Using Non-Linear Optimization},
  booktitle = {Humanoids 2008-8th {{IEEE-RAS International Conference}} on {{Humanoid Robots}}},
  author = {Do, Martin and Azad, Pedram and Asfour, Tamim and Dillmann, Rudiger},
  year = {2008},
  pages = {545--552},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/4756029/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/LIIKZLZU/Do et al. - 2008 - Imitation of human motion on a humanoid robot usin.pdf}
}

@article{2009-argall-SurveyRobotLearninga,
  title = {A Survey of Robot Learning from Demonstration},
  author = {Argall, Brenna D. and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
  year = {2009},
  journal = {Robotics and autonomous systems},
  volume = {57},
  number = {5},
  pages = {469--483},
  delete_delete_delete_publisher = {Elsevier},
  url = {https://www.sciencedirect.com/science/article/pii/S0921889008001772?casa_token=23LVhxWg4jgAAAAA:GehDaKG7uEQPK4tGHZvaYo9YPFM63lvQpXoH7LjTu46LEo4YSRpe2UtyEMGEaxrvrjkq7P_1mw},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/FZZ8NDYT/Argall et al. - 2009 - A survey of robot learning from demonstration.pdf;/home/mtoussai/Zotero/storage/LIQ85VQW/S0921889008001772.html}
}

@article{2009-daume-SearchbasedStructuredPrediction,
  title = {Search-Based Structured Prediction},
  author = {Daumé, Hal and Langford, John and Marcu, Daniel},
  year = {2009},
  journal = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {75},
  number = {3},
  pages = {297--325},
  issn = {1573-0565},
  delete_delete_delete_doi = {10.1007/s10994-009-5106-x},
  url = {https://delete_delete_delete_doi.org/10.1007/s10994-009-5106-x},
  urlyear = {2024},
  abstract = {We present Searn, an algorithm for integrating search and learning to solve complex structured prediction problems such as those that occur in natural language, speech, computational biology, and vision. Searn is a meta-algorithm that transforms these complex problems into simple classification problems to which any binary classifier may be applied. Unlike current algorithms for structured learning that require decomposition of both the loss function and the feature functions over the predicted structure, Searn is able to learn prediction functions for any loss function and any class of features. Moreover, Searn comes with a strong, natural theoretical guarantee: good performance on the derived classification problems implies good performance on the structured prediction problem.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/EAEU494S/Daumé et al. - 2009 - Search-based structured prediction.pdf}
}

@inproceedings{2010-ross-EfficientReductionsImitation,
  title = {Efficient Reductions for Imitation Learning},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  author = {Ross, Stéphane and Bagnell, Drew},
  year = {2010},
  pages = {661--668},
  delete_delete_delete_publisher = {{JMLR Workshop and Conference Proceedings}},
  url = {https://proceedings.mlr.press/v9/ross10a.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/RKYCCN6B/Ross and Bagnell - 2010 - Efficient reductions for imitation learning.pdf}
}

@online{2011-ross-ReductionImitationLearninga,
  title = {A {{Reduction}} of {{Imitation Learning}} and {{Structured Prediction}} to {{No-Regret Online Learning}}},
  author = {Ross, Stephane and Gordon, Geoffrey J. and Bagnell, J. Andrew},
  year = {2011},
  eprint = {1011.0686},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  delete_delete_delete_doi = {10.48550/arXiv.1011.0686},
  url = {http://arxiv.org/abs/1011.0686},
  urlyear = {2024},
  abstract = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/mtoussai/Zotero/storage/A25LCW59/Ross et al. - 2011 - A Reduction of Imitation Learning and Structured P.pdf;/home/mtoussai/Zotero/storage/KGTCW2HF/1011.html}
}

@article{2013-paraschos-ProbabilisticMovementPrimitives,
  title = {Probabilistic Movement Primitives},
  author = {Paraschos, Alexandros and Daniel, Christian and Peters, Jan R. and Neumann, Gerhard},
  year = {2013},
  journal = {Advances in neural information processing systems},
  volume = {26},
  url = {https://proceedings.neurips.cc/paper/2013/hash/e53a0a2978c28872a4505bdb51db06dc-Abstract.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/JGL5EM3P/Paraschos et al. - 2013 - Probabilistic movement primitives.pdf}
}

@inproceedings{2016-ho-GenerativeAdversarialImitationa,
  title = {Generative {{Adversarial Imitation Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ho, Jonathan and Ermon, Stefano},
  year = {2016},
  volume = {29},
  delete_delete_delete_publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2016/hash/cc7e2b878868cbae992d1fb743995d8f-Abstract.html},
  urlyear = {2024},
  abstract = {Consider learning a policy from example expert behavior, without interaction with the expert or access to a reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free  methods in imitating complex behaviors in large, high-dimensional environments.},
  file = {/home/mtoussai/Zotero/storage/7XTMHMNH/Ho and Ermon - 2016 - Generative Adversarial Imitation Learning.pdf}
}

@online{2017-merel-LearningHumanBehaviors,
  title = {Learning Human Behaviors from Motion Capture by Adversarial Imitation},
  author = {Merel, Josh and Tassa, Yuval and TB, Dhruva and Srinivasan, Sriram and Lemmon, Jay and Wang, Ziyu and Wayne, Greg and Heess, Nicolas},
  year = {2017},
  eprint = {1707.02201},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1707.02201},
  urlyear = {2024},
  abstract = {Rapid progress in deep reinforcement learning has made it increasingly feasible to train controllers for high-dimensional humanoid bodies. However, methods that use pure reinforcement learning with simple reward functions tend to produce non-humanlike and overly stereotyped movement behaviors. In this work, we extend generative adversarial imitation learning to enable training of generic neural network policies to produce humanlike movement patterns from limited demonstrations consisting only of partially observed state features, without access to actions, even when the demonstrations come from a body with different and unknown physical parameters. We leverage this approach to build sub-skill policies from motion capture data and show that they can be reused to solve tasks when controlled by a higher level controller.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  file = {/home/mtoussai/Zotero/storage/U79V692I/Merel et al. - 2017 - Learning human behaviors from motion capture by ad.pdf;/home/mtoussai/Zotero/storage/Z2PLBE4I/1707.html}
}

@online{2017-weng-GANWGAN,
  title = {From {{GAN}} to {{WGAN}}},
  author = {Weng, Lilian},
  year = {2017-08-20T00:00:00+00:00},
  url = {https://lilianweng.github.io/posts/2017-08-20-gan/},
  urlyear = {2024},
  abstract = {[Upyeard on 2018-09-30: thanks to Yoonju, we have this post translated in Korean!] [Upyeard on 2019-04-18: this post is also available on arXiv.] Generative adversarial network (GAN) has shown great results in many generative tasks to replicate the real-world rich content such as images, human language, and music. It is inspired by game theory: two models, a generator and a critic, are competing with each other while making each other stronger at the same time.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/RNZXNYIX/2017-08-20-gan.html}
}

@inproceedings{2018-codevilla-EndtoEndDrivingConditional,
  title = {End-to-{{End Driving Via Conditional Imitation Learning}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Codevilla, Felipe and Muller, Matthias and Lopez, Antonio and Koltun, Vladlen and Dosovitskiy, Alexey},
  year = {2018},
  pages = {4693--4700},
  delete_delete_delete_publisher = {IEEE},
  location = {Brisbane, QLD},
  delete_delete_delete_doi = {10.1109/ICRA.2018.8460487},
  url = {https://ieeexplore.ieee.org/document/8460487/},
  urlyear = {2024},
  abstract = {Deep networks trained on demonstrations of human driving have learned to follow roads and avoid obstacles. However, driving policies trained via imitation learning cannot be controlled at test time. A vehicle trained end-to-end to imitate an expert cannot be guided to take a specific turn at an upcoming intersection. This limits the utility of such systems. We propose to condition imitation learning on high-level command input. At test time, the learned driving policy functions as a chauffeur that handles sensorimotor coordination but continues to respond to navigational commands. We evaluate different architectures for conditional imitation learning in vision-based driving. We conduct experiments in realistic three-dimensional simulations of urban driving and on a 1/5 scale robotic truck that is trained to drive in a residential area. Both systems drive based on visual input yet remain responsive to high-level navigational commands.},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-5386-3081-5},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/TJK6LUM8/Codevilla et al. - 2018 - End-to-End Driving Via Conditional Imitation Learn.pdf}
}

@inproceedings{2018-ichter-LearningSamplingDistributions,
  title = {Learning {{Sampling Distributions}} for {{Robot Motion Planning}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Ichter, Brian and Harrison, James and Pavone, Marco},
  year = {2018},
  pages = {7087--7094},
  issn = {2577-087X},
  delete_delete_delete_doi = {10.1109/ICRA.2018.8460730},
  abstract = {A defining feature of sampling-based motion planning is the reliance on an implicit representation of the state space, which is enabled by a set of probing samples. Traditionally, these samples are drawn either probabilistically or deterministically to uniformly cover the state space. Yet, the motion of many robotic systems is often restricted to “small” regions of the state space, due to e.g. differential constraints or collision-avoidance constraints. To accelerate the planning process, it is thus desirable to devise non-uniform sampling strategies that favor sampling in those regions where an optimal solution might lie. This paper proposes a methodology for nonuniform sampling, whereby a sampling distribution is learned from demonstrations, and then used to bias sampling. The sampling distribution is computed through a conditional variational autoencoder, allowing sample generation from the latent space conditioned on the specific planning problem. This methodology is general, can be used in combination with any sampling-based planner, and can effectively exploit the underlying structure of a planning problem while maintaining the theoretical guarantees of sampling-based approaches. Specifically, on several planning problems, the proposed methodology is shown to effectively learn representations for the relevant regions of the state space, resulting in an order of magnitude improvement in terms of success rate and convergence to the optimal cost.},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Acceleration,bias sampling,collision avoidance,Collision avoidance,collision-avoidance,Feature extraction,Manifolds,mobile robots,Planning,Probabilistic logic,robot motion planning,Robots,sampling methods,sampling-based motion planning,variational autoencoder},
  file = {/home/mtoussai/Zotero/storage/HIRAD8JE/Ichter et al. - 2018 - Learning Sampling Distributions for Robot Motion P.pdf;/home/mtoussai/Zotero/storage/A5FP9BKA/8460730.html}
}

@online{2018-weng-AutoencoderBetaVAE,
  title = {From {{Autoencoder}} to {{Beta-VAE}}},
  author = {Weng, Lilian},
  year = {2018-08-12T00:00:00+00:00},
  url = {https://lilianweng.github.io/posts/2018-08-12-vae/},
  urlyear = {2024},
  abstract = {[Upyeard on 2019-07-18: add a section on VQ-VAE \& VQ-VAE-2.] [Upyeard on 2019-07-26: add a section on TD-VAE.] Autocoder is invented to reconstruct high-dimensional data using a neural network model with a narrow bottleneck layer in the middle (oops, this is probably not true for Variational Autoencoder, and we will investigate it in details in later sections). A nice byproduct is dimension reduction: the bottleneck layer captures a compressed latent encoding.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/YR6VHYYX/2018-08-12-vae.html}
}

@inproceedings{2020-chen-LearningCheating,
  title = {Learning by Cheating},
  booktitle = {Conference on {{Robot Learning}}},
  author = {Chen, Dian and Zhou, Brady and Koltun, Vladlen and Krähenbühl, Philipp},
  year = {2020},
  pages = {66--75},
  delete_delete_delete_publisher = {PMLR},
  url = {http://proceedings.mlr.press/v100/chen20a.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/5YI4TLHU/Chen et al. - 2020 - Learning by cheating.pdf}
}

@inproceedings{2020-chen-LearningCheatinga,
  title = {Learning by {{Cheating}}},
  booktitle = {Proceedings of the {{Conference}} on {{Robot Learning}}},
  author = {Chen, Dian and Zhou, Brady and Koltun, Vladlen and Krähenbühl, Philipp},
  year = {2020},
  pages = {66--75},
  delete_delete_delete_publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v100/chen20a.html},
  urlyear = {2024},
  abstract = {Vision-based urban driving is hard. The autonomous system needs to learn to perceive the world and act in it. We show that this challenging learning problem can be simplified by decomposing it into two stages. We first train an agent that has access to privileged information. This privileged agent cheats by observing the ground-truth layout of the environment and the positions of all traffic participants. In the second stage, the privileged agent acts as a teacher that trains a purely vision-based sensorimotor agent. The resulting sensorimotor agent does not have access to any privileged information and does not cheat. This two-stage training procedure is counter-intuitive at first, but has a number of important advantages that we analyze and empirically demonstrate. We use the presented approach to train a vision-based autonomous driving system that substantially outperforms the state of the art on the CARLA benchmark and the recent NoCrash benchmark. Our approach achieves, for the first time, 100\% success rate on all tasks in the original CARLA benchmark, sets a new record on the NoCrash benchmark, and reduces the frequency of infractions by an order of magnitude compared to the prior state of the art.},
  booktitle = {Conference on {{Robot Learning}}},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/7HBZXKG7/Chen et al. - 2020 - Learning by Cheating.pdf}
}

@inproceedings{2020-ho-DenoisingDiffusionProbabilistic,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  volume = {33},
  pages = {6840--6851},
  delete_delete_delete_publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
  urlyear = {2024},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
  file = {/home/mtoussai/Zotero/storage/U8TUTBGM/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@inproceedings{2020-kaufmann-DeepDroneAcrobatics,
  title = {Deep {{Drone Acrobatics}}},
  booktitle = {Robotics: {{Science}} and {{Systems XVI}}},
  author = {Kaufmann, Elia and Loquercio, Antonio and Ranftl, Rene and Müller, Matthias and Koltun, Vladlen and Scaramuzza, Davide},
  year = {2020},
  delete_delete_delete_publisher = {{Robotics: Science and Systems Foundation}},
  delete_delete_delete_doi = {10.15607/RSS.2020.XVI.040},
  url = {http://www.roboticsproceedings.org/rss16/p040.pdf},
  urlyear = {2024},
  abstract = {Performing acrobatic maneuvers with quadrotors is extremely challenging. Acrobatic flight requires high thrust and extreme angular accelerations that push the platform to its physical limits. Professional drone pilots often measure their level of mastery by flying such maneuvers in competitions. In this paper, we propose to learn a sensorimotor policy that enables an autonomous quadrotor to fly extreme acrobatic maneuvers with only onboard sensing and computation. We train the policy entirely in simulation by leveraging demonstrations from an optimal controller that has access to privileged information. We use appropriate abstractions of the visual input to enable transfer to a real quadrotor. We show that the resulting policy can be directly deployed in the physical world without any fine-tuning on real data. Our methodology has several favorable properties: it does not require a human expert to provide demonstrations, it cannot harm the physical system during training, and it can be used to learn maneuvers that are challenging even for the best human pilots. Our approach enables a physical quadrotor to fly maneuvers such as the Power Loop, the Barrel Roll, and the Matty Flip, during which it incurs accelerations of up to 3g.},
  booktitle = {Robotics: {{Science}} and {{Systems}} 2020},
  isbn = {978-0-9923747-6-1},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/F7KX6798/Kaufmann et al. - 2020 - Deep Drone Acrobatics.pdf}
}

@inproceedings{2020-kim-DomainAdaptiveImitation,
  title = {Domain {{Adaptive Imitation Learning}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Kim, Kuno and Gu, Yihong and Song, Jiaming and Zhao, Shengjia and Ermon, Stefano},
  year = {2020},
  pages = {5286--5295},
  delete_delete_delete_publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/kim20c.html},
  urlyear = {2024},
  abstract = {We study the question of how to imitate tasks across domains with discrepancies such as embodiment, viewpoint, and dynamics mismatch. Many prior works require paired, aligned demonstrations and an additional RL step that requires environment interactions. However, paired, aligned demonstrations are seldom obtainable and RL procedures are expensive. In this work, we formalize the Domain Adaptive Imitation Learning (DAIL) problem - a unified framework for imitation learning in the presence of viewpoint, embodiment, and/or dynamics mismatch. Informally, DAIL is the process of learning how to perform a task optimally, given demonstrations of the task in a distinct domain. We propose a two step approach to DAIL: alignment followed by adaptation. In the alignment step we execute a novel unsupervised MDP alignment algorithm, Generative Adversarial MDP Alignment (GAMA), to learn state and action correspondences from \textbackslash emph\{unpaired, unaligned\} demonstrations. In the adaptation step we leverage the correspondences to zero-shot imitate tasks across domains. To describe when DAIL is feasible via alignment and adaptation, we introduce a theory of MDP alignability. We experimentally evaluate GAMA against baselines in embodiment, viewpoint, and dynamics mismatch scenarios where aligned demonstrations don’t exist and show the effectiveness of our approach},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/IXTZYA94/Kim et al. - 2020 - Domain Adaptive Imitation Learning.pdf;/home/mtoussai/Zotero/storage/Y2J2WZKK/Kim et al. - 2020 - Domain Adaptive Imitation Learning.pdf}
}

@article{2020-lee-LearningQuadrupedalLocomotionb,
  title = {Learning Quadrupedal Locomotion over Challenging Terrain},
  author = {Lee, Joonho and Hwangbo, Jemin and Wellhausen, Lorenz and Koltun, Vladlen and Hutter, Marco},
  year = {2020},
  journal = {Science Robotics},
  volume = {5},
  number = {47},
  pages = {eabc5986},
  delete_delete_delete_publisher = {American Association for the Advancement of Science},
  delete_delete_delete_doi = {10.1126/scirobotics.abc5986},
  url = {https://www.science.org/delete_delete_delete_doi/10.1126/scirobotics.abc5986},
  urlyear = {2024},
  abstract = {Legged locomotion can extend the operational domain of robots to some of the most challenging environments on Earth. However, conventional controllers for legged locomotion are based on elaborate state machines that explicitly trigger the execution of motion primitives and reflexes. These designs have increased in complexity but fallen short of the generality and robustness of animal locomotion. Here, we present a robust controller for blind quadrupedal locomotion in challenging natural environments. Our approach incorporates proprioceptive feedback in locomotion control and demonstrates zero-shot generalization from simulation to natural environments. The controller is trained by reinforcement learning in simulation. The controller is driven by a neural network policy that acts on a stream of proprioceptive signals. The controller retains its robustness under conditions that were never encountered during training: deformable terrains such as mud and snow, dynamic footholds such as rubble, and overground impediments such as thick vegetation and gushing water. The presented work indicates that robust locomotion in natural environments can be achieved by training in simple domains.},
  file = {/home/mtoussai/Zotero/storage/2R62AGRK/Lee et al. - 2020 - Learning quadrupedal locomotion over challenging t.pdf}
}

@online{2020-smith-AVIDLearningMultiStage,
  title = {{{AVID}}: {{Learning Multi-Stage Tasks}} via {{Pixel-Level Translation}} of {{Human Videos}}},
  shorttitle = {{{AVID}}},
  author = {Smith, Laura and Dhawan, Nikita and Zhang, Marvin and Abbeel, Pieter and Levine, Sergey},
  year = {2020},
  eprint = {1912.04443},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1912.04443},
  urlyear = {2024},
  abstract = {Robotic reinforcement learning (RL) holds the promise of enabling robots to learn complex behaviors through experience. However, realizing this promise for long-horizon tasks in the real world requires mechanisms to reduce human burden in terms of defining the task and scaffolding the learning process. In this paper, we study how these challenges can be alleviated with an automated robotic learning framework, in which multi-stage tasks are defined simply by providing videos of a human demonstrator and then learned autonomously by the robot from raw image observations. A central challenge in imitating human videos is the difference in appearance between the human and robot, which typically requires manual correspondence. We instead take an automated approach and perform pixel-level image translation via CycleGAN to convert the human demonstration into a video of a robot, which can then be used to construct a reward function for a model-based RL algorithm. The robot then learns the task one stage at a time, automatically learning how to reset each stage to retry it multiple times without human-provided resets. This makes the learning process largely automatic, from intuitive task specification via a video to automated training with minimal human intervention. We demonstrate that our approach is capable of learning complex tasks, such as operating a coffee machine, directly from raw image observations, requiring only 20 minutes to provide human demonstrations and about 180 minutes of robot interaction.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/mtoussai/Zotero/storage/2TJEHHR4/Smith et al. - 2020 - AVID Learning Multi-Stage Tasks via Pixel-Level T.pdf;/home/mtoussai/Zotero/storage/L3EW7K7Q/1912.html}
}

@online{2021-weng-WhatAreDiffusion,
  title = {What Are {{Diffusion Models}}?},
  author = {Weng, Lilian},
  year = {2021-07-11T00:00:00+00:00},
  url = {https://lilianweng.github.io/posts/2021-07-11-diffusion-models/},
  urlyear = {2024},
  abstract = {[Upyeard on 2021-09-19: Highly recommend this blog post on score-based generative modeling by Yang Song (author of several key papers in the references)]. [Upyeard on 2022-08-27: Added classifier-free guidance, GLIDE, unCLIP and Imagen. [Upyeard on 2022-08-31: Added latent diffusion model. [Updated on 2024-04-13: Added progressive distillation, consistency models, and the Model Architecture section. So far, I’ve written about three types of generative models, GAN, VAE, and Flow-based models. They have shown great success in generating high-quality samples, but each has some limitations of its own.},
  langid = {english}
}

@inproceedings{2022-florence-ImplicitBehavioralCloning,
  title = {Implicit {{Behavioral Cloning}}},
  booktitle = {Proceedings of the 5th {{Conference}} on {{Robot Learning}}},
  author = {Florence, Pete and Lynch, Corey and Zeng, Andy and Ramirez, Oscar A. and Wahid, Ayzaan and Downs, Laura and Wong, Adrian and Lee, Johnny and Mordatch, Igor and Tompson, Jonathan},
  year = {2022},
  pages = {158--168},
  delete_delete_delete_publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v164/florence22a.html},
  urlyear = {2024},
  abstract = {We find that across a wide range of robot policy learning scenarios, treating supervised policy learning with an implicit model generally performs better, on average, than commonly used explicit models.  We present extensive experiments on this finding, and we provide both intuitive insight and theoretical arguments distinguishing the properties of implicit models compared to their explicit counterparts, particularly with respect to approximating complex, potentially discontinuous and multi-valued (set-valued) functions. On robotic policy learning tasks we show that implicit behavior-cloning policies with energy-based models (EBM) often outperform common explicit (Mean Square Error, or Mixture Density) behavior-cloning policies, including on tasks with high-dimensional action spaces and visual image inputs. We find these policies provide competitive results or outperform state-of-the-art offline reinforcement learning methods on the challenging human-expert tasks from the D4RL benchmark suite, despite using no reward information. In the real world, robots with implicit policies can learn complex and remarkably subtle behaviors on contact-rich tasks from human demonstrations, including tasks with high combinatorial complexity and tasks requiring 1mm precision.},
  booktitle = {Conference on {{Robot Learning}}},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/AD8HRZA6/Florence et al. - 2022 - Implicit Behavioral Cloning.pdf}
}

@article{2022-ha-DeepVisualConstraints,
  title = {Deep Visual Constraints: {{Neural}} Implicit Models for Manipulation Planning from Visual Input},
  shorttitle = {Deep Visual Constraints},
  author = {Ha, Jung-Su and Driess, Danny and Toussaint, Marc},
  year = {2022},
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {4},
  pages = {10857--10864},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/9844753/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/PUZWSE5K/Ha et al. - 2022 - Deep visual constraints Neural implicit models fo.pdf}
}

@inproceedings{2022-janner-PlanningDiffusionFlexiblea,
  title = {Planning with {{Diffusion}} for {{Flexible Behavior Synthesis}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Janner, Michael and Du, Yilun and Tenenbaum, Joshua and Levine, Sergey},
  year = {2022},
  pages = {9902--9915},
  delete_delete_delete_publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/janner22a.html},
  urlyear = {2024},
  abstract = {Model-based reinforcement learning methods often use learning only for the purpose of recovering an approximate dynamics model, offloading the rest of the decision-making work to classical trajectory optimizers. While conceptually simple, this combination has a number of empirical shortcomings, suggesting that learned models may not be well-suited to standard trajectory optimization. In this paper, we consider what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical. The core of our technical approach lies in a diffusion probabilistic model that plans by iteratively denoising trajectories. We show how classifier-guided sampling and image inpainting can be reinterpreted as coherent planning strategies, explore the unusual and useful properties of diffusion-based planning methods, and demonstrate the effectiveness of our framework in control settings that emphasize long-horizon decision-making and test-time flexibility.},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/ZMUAHILP/Janner et al. - 2022 - Planning with Diffusion for Flexible Behavior Synt.pdf}
}

@incollection{2022-manuelli-KPAMKeyPointAffordances,
  title = {{{KPAM}}: {{KeyPoint Affordances}} for {{Category-Level Robotic Manipulation}}},
  shorttitle = {{{KPAM}}},
  booktitle = {Robotics {{Research}}},
  author = {Manuelli, Lucas and Gao, Wei and Florence, Peter and Tedrake, Russ},
  editor = {Asfour, Tamim and Yoshida, Eiichi and Park, Jaeheung and Christensen, Henrik and Khatib, Oussama},
  year = {2022},
  volume = {20},
  pages = {132--157},
  delete_delete_delete_publisher = {Springer International Publishing},
  location = {Cham},
  delete_delete_delete_doi = {10.1007/978-3-030-95459-8_9},
  url = {https://link.springer.com/10.1007/978-3-030-95459-8_9},
  urlyear = {2024},
  isbn = {978-3-030-95458-1 978-3-030-95459-8},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/MSL38PAU/Manuelli et al. - 2019 - kPAM KeyPoint Affordances for Category-Level Robo.pdf}
}

@inproceedings{2022-pearce-ImitatingHumanBehaviour,
  title = {Imitating {{Human Behaviour}} with {{Diffusion Models}}},
  author = {Pearce, Tim and Rashid, Tabish and Kanervisto, Anssi and Bignell, Dave and Sun, Mingfei and Georgescu, Raluca and Macua, Sergio Valcarcel and Tan, Shan Zheng and Momennejad, Ida and Hofmann, Katja and Devlin, Sam},
  year = {2022},
  url = {https://openreview.net/forum?id=Pv1GPQzRrC8},
  urlyear = {2024},
  abstract = {Diffusion models have emerged as powerful generative models in the text-to-image domain. This paper studies their application as observation-to-action models for imitating human behaviour in sequential environments. Human behaviour is stochastic and multimodal, with structured correlations between action dimensions. Meanwhile, standard modelling choices in behaviour cloning are limited in their expressiveness and may introduce bias into the cloned policy. We begin by pointing out the limitations of these choices. We then propose that diffusion models are an excellent fit for imitating human behaviour, since they learn an expressive distribution over the joint action space. We introduce several innovations to make diffusion models suitable for sequential environments; designing suitable architectures, investigating the role of guidance, and developing reliable sampling strategies. Experimentally, diffusion models closely match human demonstrations in a simulated robotic control task and a modern 3D gaming environment.},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/V9FH8FPM/Pearce et al. - 2022 - Imitating Human Behaviour with Diffusion Models.pdf}
}

@inproceedings{2022-simeonov-NeuralDescriptorFields,
  title = {Neural Descriptor Fields: {{Se}} (3)-Equivariant Object Representations for Manipulation},
  shorttitle = {Neural Descriptor Fields},
  booktitle = {2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Simeonov, Anthony and Du, Yilun and Tagliasacchi, Andrea and Tenenbaum, Joshua B. and Rodriguez, Alberto and Agrawal, Pulkit and Sitzmann, Vincent},
  year = {2022},
  pages = {6394--6400},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/9812146/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/GBWCG3SH/Simeonov et al. - 2022 - Neural descriptor fields Se (3)-equivariant objec.pdf}
}

@inproceedings{2022-tagliabue-DemonstrationEfficientGuidedPolicy,
  title = {Demonstration-{{Efficient Guided Policy Search}} via {{Imitation}} of {{Robust Tube MPC}}},
  booktitle = {2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Tagliabue, Andrea and Kim, Dong-Ki and Everett, Michael and How, Jonathan P.},
  year = {2022},
  pages = {462--468},
  delete_delete_delete_doi = {10.1109/ICRA46639.2022.9812122},
  url = {https://ieeexplore.ieee.org/document/9812122},
  urlyear = {2024},
  abstract = {We propose a demonstration-efficient strategy to compress a computationally expensive Model Predictive Controller (MPC) into a more computationally efficient representation based on a deep neural network and Imitation Learning (IL). By generating a Robust Tube variant (RTMPC) of the MPC and leveraging properties from the tube, we introduce a data augmentation method that enables high demonstration-efficiency, capable of compensating the distribution shifts typically encountered in IL. Our approach opens the possibility of zero-shot transfer from a single demonstration collected in a nominal domain, such as a simulation or a robot in a lab/controlled environment, to a domain with bounded model errors/perturbations. Numerical and experimental evaluations performed on a trajectory tracking MPC for a multirotor show that our method outperforms strategies commonly employed in IL, such as DAgger and Domain Randomization, in terms of demonstration-efficiency and robustness to perturbations unseen during training.},
  booktitle = {2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  file = {/home/mtoussai/Zotero/storage/L4ZXD8SJ/Tagliabue et al. - 2022 - Demonstration-Efficient Guided Policy Search via I.pdf;/home/mtoussai/Zotero/storage/3Y3DCKAE/9812122.html}
}

@inproceedings{2023-belkhale-HydraHybridRobot,
  title = {Hydra: {{Hybrid}} Robot Actions for Imitation Learning},
  shorttitle = {Hydra},
  booktitle = {Conference on {{Robot Learning}}},
  author = {Belkhale, Suneel and Cui, Yuchen and Sadigh, Dorsa},
  year = {2023},
  pages = {2113--2133},
  delete_delete_delete_publisher = {PMLR},
  url = {https://proceedings.mlr.press/v229/belkhale23a.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/U5RTLI2B/Belkhale et al. - 2023 - Hydra Hybrid robot actions for imitation learning.pdf}
}

@inproceedings{2023-chi-DiffusionPolicyVisuomotora,
  title = {Diffusion {{Policy}}: {{Visuomotor Policy Learning}} via {{Action Diffusion}}},
  shorttitle = {Diffusion {{Policy}}},
  booktitle = {Robotics: {{Science}} and {{Systems XIX}}},
  author = {Chi, Cheng and Feng, Siyuan and Du, Yilun and Xu, Zhenjia and Cousineau, Eric and Burchfiel, Benjamin and Song, Shuran},
  year = {2023},
  delete_delete_delete_publisher = {{Robotics: Science and Systems Foundation}},
  delete_delete_delete_doi = {10.15607/RSS.2023.XIX.026},
  url = {http://www.roboticsproceedings.org/rss19/p026.pdf},
  urlyear = {2024},
  abstract = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot’s visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details will be publicly available.},
  booktitle = {Robotics: {{Science}} and {{Systems}} 2023},
  isbn = {978-0-9923747-9-2},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/J4YZRG6N/Chi et al. - 2023 - Diffusion Policy Visuomotor Policy Learning via A.pdf}
}

@online{2023-zhao-LearningFineGrainedBimanualc,
  title = {Learning {{Fine-Grained Bimanual Manipulation}} with {{Low-Cost Hardware}}},
  author = {Zhao, Tony Z. and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
  year = {2023},
  eprint = {2304.13705},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2304.13705},
  urlyear = {2024},
  abstract = {Fine manipulation tasks, such as threading cable ties or slotting a battery, are notoriously difficult for robots because they require precision, careful coordination of contact forces, and closed-loop visual feedback. Performing these tasks typically requires high-end robots, accurate sensors, or careful calibration, which can be expensive and difficult to set up. Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks? We present a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Imitation learning, however, presents its own challenges, particularly in high-precision domains: errors in the policy can compound over time, and human demonstrations can be non-stationary. To address these challenges, we develop a simple yet novel algorithm, Action Chunking with Transformers (ACT), which learns a generative model over action sequences. ACT allows the robot to learn 6 difficult tasks in the real world, such as opening a translucent condiment cup and slotting a battery with 80-90\% success, with only 10 minutes worth of demonstrations. Project website: https://tonyzhaozh.github.io/aloha/},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/mtoussai/Zotero/storage/HK5W964P/Zhao et al. - 2023 - Learning Fine-Grained Bimanual Manipulation with L.pdf;/home/mtoussai/Zotero/storage/V9S34NMR/Zhao et al. - 2023 - Learning Fine-Grained Bimanual Manipulation with L.pdf;/home/mtoussai/Zotero/storage/PU3VWUBA/2304.html}
}

@book{2024-bishop-DeepLearningFoundations,
  title = {Deep {{Learning}}: {{Foundations}} and {{Concepts}}},
  shorttitle = {Deep {{Learning}}},
  author = {Bishop, Christopher M. and Bishop, Hugh},
  year = {2024},
  delete_delete_delete_publisher = {Springer International Publishing},
  location = {Cham},
  delete_delete_delete_doi = {10.1007/978-3-031-45468-4},
  url = {https://link.springer.com/10.1007/978-3-031-45468-4},
  urlyear = {2024},
  isbn = {978-3-031-45467-7 978-3-031-45468-4},
  langid = {english},
  keywords = {Convolutional networks,Decision theory,Deep learning,Directed graphical models,machine learning,Neural networks}
}

@online{2024-chan-TutorialDiffusionModels,
  title = {Tutorial on {{Diffusion Models}} for {{Imaging}} and {{Vision}}},
  author = {Chan, Stanley H.},
  year = {2024},
  eprint = {2403.18103},
  eprinttype = {arXiv},
  eprintclass = {cs},
  delete_delete_delete_doi = {10.48550/arXiv.2403.18103},
  url = {http://arxiv.org/abs/2403.18103},
  urlyear = {2024},
  abstract = {The astonishing growth of generative tools in recent years has empowered many exciting applications in text-to-image generation and text-to-video generation. The underlying principle behind these generative tools is the concept of diffusion, a particular sampling mechanism that has overcome some shortcomings that were deemed difficult in the previous approaches. The goal of this tutorial is to discuss the essential ideas underlying the diffusion models. The target audience of this tutorial includes undergraduate and graduate students who are interested in delete_delete_delete_doing research on diffusion models or applying these models to solve other problems.},
  pubstate = {prepublished},
  file = {/home/mtoussai/Zotero/storage/867UN8UR/Chan - 2024 - Tutorial on Diffusion Models for Imaging and Visio.pdf;/home/mtoussai/Zotero/storage/7UMDUQX2/2403.html}
}

@article{2024-sontakke-RoboclipOneDemonstration,
  title = {Roboclip: {{One}} Demonstration Is Enough to Learn Robot Policies},
  shorttitle = {Roboclip},
  author = {Sontakke, Sumedh and Zhang, Jesse and Arnold, Séb and Pertsch, Karl and Bıyık, Erdem and Sadigh, Dorsa and Finn, Chelsea and Itti, Laurent},
  year = {2024},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/ae54ce310476218f26dd48c1626d5187-Abstract-Conference.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/FPNT9IG4/Sontakke et al. - 2024 - Roboclip One demonstration is enough to learn rob.pdf}
}
