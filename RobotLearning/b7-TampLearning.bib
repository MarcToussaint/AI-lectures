@inproceedings{2010-kollar-UnderstandingNaturalLanguage,
  title = {Toward Understanding Natural Language Directions},
  booktitle = {2010 5th {{ACM}}/{{IEEE International Conference}} on {{Human-Robot Interaction}} ({{HRI}})},
  author = {Kollar, Thomas and Tellex, Stefanie and Roy, Deb and Roy, Nicholas},
  year = {2010},
  pages = {259--266},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/5453186/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/NCAYER9I/Kollar et al. - 2010 - Toward understanding natural language directions.pdf}
}

@incollection{2013-matuszek-LearningParseNatural,
  title = {Learning to {{Parse Natural Language Commands}} to a {{Robot Control System}}},
  booktitle = {Experimental {{Robotics}}},
  author = {Matuszek, Cynthia and Herbst, Evan and Zettlemoyer, Luke and Fox, Dieter},
  editor = {Desai, Jaydev P. and Dudek, Gregory and Khatib, Oussama and Kumar, Vijay},
  year = {2013},
  volume = {88},
  pages = {403--415},
  delete_delete_delete_publisher = {Springer International Publishing},
  location = {Heidelberg},
  delete_delete_delete_doi = {10.1007/978-3-319-00065-7_28},
  url = {https://link.springer.com/10.1007/978-3-319-00065-7_28},
  urlyear = {2024},
  isbn = {978-3-319-00064-0 978-3-319-00065-7},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/3RTSY9CX/Matuszek et al. - 2013 - Learning to Parse Natural Language Commands to a R.pdf}
}

@inproceedings{2014-howard-NaturalLanguagePlanner,
  title = {A Natural Language Planner Interface for Mobile Manipulators},
  booktitle = {2014 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Howard, Thomas M. and Tellex, Stefanie and Roy, Nicholas},
  year = {2014},
  pages = {6652--6659},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/6907841/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/HLT6TV3U/Howard et al. - 2014 - A natural language planner interface for mobile ma.pdf}
}

@inproceedings{2015-toussaint-LogicGeometricProgrammingOptimizationBased,
  title = {Logic-{{Geometric Programming}}: {{An Optimization-Based Approach}} to {{Combined Task}} and {{Motion Planning}}.},
  shorttitle = {Logic-{{Geometric Programming}}},
  booktitle = {{{IJCAI}}},
  author = {Toussaint, Marc},
  year = {2015},
  pages = {1930--1936},
  url = {https://argmin.lis.tu-berlin.de/papers/15-toussaint-IJCAI.pdf},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/T95BLUKJ/Toussaint - 2015 - Logic-Geometric Programming An Optimization-Based.pdf}
}

@article{2016-paul-EfficientGroundingAbstract,
  title = {Efficient Grounding of Abstract Spatial Concepts for Natural Language Interaction with Robot Manipulators},
  author = {Paul, Rohan and Arkin, Jacob and Roy, Nicholas and M Howard, Thomas},
  year = {2016},
  delete_delete_delete_publisher = {{Robotics: Science and Systems Foundation}},
  url = {https://dspace.mit.edu/handle/1721.1/116438},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/MSHMIAHB/Paul et al. - 2016 - Efficient grounding of abstract spatial concepts f.pdf}
}

@article{2016-tamar-ValueIterationNetworks,
  title = {Value Iteration Networks},
  author = {Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
  year = {2016},
  journal = {Advances in neural information processing systems},
  volume = {29},
  url = {https://proceedings.neurips.cc/paper_files/paper/2016/hash/c21002f464c5fc5bee3b98ced83963b8-Abstract.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/TQCKBXMV/Tamar et al. - 2016 - Value iteration networks.pdf}
}

@inproceedings{2018-gopalan-SequencetoSequenceLanguageGrounding,
  title = {Sequence-to-{{Sequence Language Grounding}} of {{Non-Markovian Task Specifications}}.},
  booktitle = {Robotics: {{Science}} and {{Systems}}},
  author = {Gopalan, Nakul and Arumugam, Dilip and Wong, Lawson LS and Tellex, Stefanie},
  year = {2018},
  volume = {2018},
  url = {https://dilipa.github.io/papers/rss18_seq2seq.pdf},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/CZ6RK2HR/Gopalan et al. - 2018 - Sequence-to-Sequence Language Grounding of Non-Mar.pdf}
}

@article{2018-toussaint-DifferentiablePhysicsStable,
  title = {Differentiable Physics and Stable Modes for Tool-Use and Manipulation Planning},
  author = {Toussaint, Marc A. and Allen, Kelsey Rebecca and Smith, Kevin A. and Tenenbaum, Joshua B.},
  year = {2018},
  delete_delete_delete_publisher = {{Robotics: Science and systems foundation}},
  url = {https://dspace.mit.edu/handle/1721.1/126626},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/XZKL2C69/Toussaint et al. - 2018 - Differentiable physics and stable modes for tool-u.pdf}
}

@online{2020-driess-DeepVisualReasoning,
  title = {Deep {{Visual Reasoning}}: {{Learning}} to {{Predict Action Sequences}} for {{Task}} and {{Motion Planning}} from an {{Initial Scene Image}}},
  shorttitle = {Deep {{Visual Reasoning}}},
  author = {Driess, Danny and Ha, Jung-Su and Toussaint, Marc},
  year = {2020},
  eprint = {2006.05398},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.05398},
  urlyear = {2024},
  abstract = {In this paper, we propose a deep convolutional recurrent neural network that predicts action sequences for task and motion planning (TAMP) from an initial scene image. Typical TAMP problems are formalized by combining reasoning on a symbolic, discrete level (e.g. first-order logic) with continuous motion planning such as nonlinear trajectory optimization. Due to the great combinatorial complexity of possible discrete action sequences, a large number of optimization/motion planning problems have to be solved to find a solution, which limits the scalability of these approaches. To circumvent this combinatorial complexity, we develop a neural network which, based on an initial image of the scene, directly predicts promising discrete action sequences such that ideally only one motion planning problem has to be solved to find a solution to the overall TAMP problem. A key aspect is that our method generalizes to scenes with many and varying number of objects, although being trained on only two objects at a time. This is possible by encoding the objects of the scene in images as input to the neural network, instead of a fixed feature vector. Results show runtime improvements of several magnitudes. Video: https://youtu.be/i8yyEbbvoEk},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/mtoussai/Zotero/storage/SLVRQ7UB/Driess et al. - 2020 - Deep Visual Reasoning Learning to Predict Action .pdf;/home/mtoussai/Zotero/storage/UASRLN8F/2006.html}
}

@online{2020-nguyen-RobotObjectRetrieval,
  title = {Robot {{Object Retrieval}} with {{Contextual Natural Language Queries}}},
  author = {Nguyen, Thao and Gopalan, Nakul and Patel, Roma and Corsaro, Matt and Pavlick, Ellie and Tellex, Stefanie},
  year = {2020},
  eprint = {2006.13253},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2006.13253},
  urlyear = {2024},
  abstract = {Natural language object retrieval is a highly useful yet challenging task for robots in human-centric environments. Previous work has primarily focused on commands specifying the desired object's type such as "scissors" and/or visual attributes such as "red," thus limiting the robot to only known object classes. We develop a model to retrieve objects based on descriptions of their usage. The model takes in a language command containing a verb, for example "Hand me something to cut," and RGB images of candiyear objects and selects the object that best satisfies the task specified by the verb. Our model directly predicts an object's appearance from the object's use specified by a verb phrase. We do not need to explicitly specify an object's class label. Our approach allows us to predict high level concepts like an object's utility based on the language query. Based on contextual information present in the language commands, our model can generalize to unseen object classes and unknown nouns in the commands. Our model correctly selects objects out of sets of five candiyears to fulfill natural language commands, and achieves an average accuracy of 62.3\% on a held-out test set of unseen ImageNet object classes and 53.0\% on unseen object classes and unknown nouns. Our model also achieves an average accuracy of 54.7\% on unseen YCB object classes, which have a different image distribution from ImageNet objects. We demonstrate our model on a KUKA LBR iiwa robot arm, enabling the robot to retrieve objects based on natural language descriptions of their usage. We also present a new dataset of 655 verb-object pairs denoting object usage over 50 verbs and 216 object classes.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/home/mtoussai/Zotero/storage/7IWPHBL6/Nguyen et al. - 2020 - Robot Object Retrieval with Contextual Natural Lan.pdf;/home/mtoussai/Zotero/storage/7X4YAK26/2006.html}
}

@article{2020-tellex-RobotsThatUse,
  title = {Robots {{That Use Language}}},
  author = {Tellex, Stefanie and Gopalan, Nakul and Kress-Gazit, Hadas and Matuszek, Cynthia},
  year = {2020},
  journal = {Annual Review of Control, Robotics, and Autonomous Systems},
  shortjournal = {Annu. Rev. Control Robot. Auton. Syst.},
  volume = {3},
  number = {1},
  pages = {25--55},
  issn = {2573-5144, 2573-5144},
  delete_delete_delete_doi = {10.1146/annurev-control-101119-071628},
  url = {https://www.annualreviews.org/delete_delete_delete_doi/10.1146/annurev-control-101119-071628},
  urlyear = {2024},
  abstract = {This article surveys the use of natural language in robotics from a robotics point of view. To use human language, robots must map words to aspects of the physical world, mediated by the robot's sensors and actuators. This problem differs from other natural language processing domains due to the need to ground the language to noisy percepts and physical actions. Here, we describe central aspects of language use by robots, including understanding natural language requests, using language to drive learning about the physical world, and engaging in collaborative dialogue with a human partner. We describe common approaches, roughly divided into learning methods, logic-based methods, and methods that focus on questions of human–robot interaction. Finally, we describe several application domains for language-using robots.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/DKPP7IXR/Tellex et al. - Robots That Use Language A Survey.pdf}
}

@inproceedings{2021-driess-LearningGeometricReasoning,
  title = {Learning Geometric Reasoning and Control for Long-Horizon Tasks from Visual Input},
  booktitle = {2021 {{IEEE}} International Conference on Robotics and Automation ({{ICRA}})},
  author = {Driess, Danny and Ha, Jung-Su and Tedrake, Russ and Toussaint, Marc},
  year = {2021},
  pages = {14298--14305},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/9560934/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/YAXKAM95/Driess et al. - 2021 - Learning geometric reasoning and control for long-.pdf}
}

@article{2021-garrett-IntegratedTaskMotion,
  title = {Integrated {{Task}} and {{Motion Planning}}},
  author = {Garrett, Caelan Reed and Chitnis, Rohan and Holladay, Rachel and Kim, Beomjoon and Silver, Tom and Kaelbling, Leslie Pack and Lozano-Pérez, Tomás},
  year = {2021},
  journal = {Annual Review of Control, Robotics, and Autonomous Systems},
  shortjournal = {Annu. Rev. Control Robot. Auton. Syst.},
  volume = {4},
  number = {1},
  pages = {265--293},
  issn = {2573-5144, 2573-5144},
  delete_delete_delete_doi = {10.1146/annurev-control-091420-084139},
  url = {https://www.annualreviews.org/delete_delete_delete_doi/10.1146/annurev-control-091420-084139},
  urlyear = {2024},
  abstract = {The problem of planning for a robot that operates in environments containing a large number of objects, taking actions to move itself through the world as well as to change the state of the objects, is known as task and motion planning (TAMP). TAMP problems contain elements of discrete task planning, discrete–continuous mathematical programming, and continuous motion planning and thus cannot be effectively addressed by any of these fields directly. In this article, we define a class of TAMP problems and survey algorithms for solving them, characterizing the solution methods in terms of their strategies for solving the continuous-space subproblems and their techniques for integrating the discrete and continuous components of the search.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/VEYZWPUX/Garrett et al. - 2021 - Integrated Task and Motion Planning.pdf}
}

@inproceedings{2021-radford-LearningTransferableVisual,
  title = {Learning Transferable Visual Models from Natural Language Supervision},
  booktitle = {International Conference on Machine Learning},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack},
  year = {2021},
  pages = {8748--8763},
  delete_delete_delete_publisher = {PMLR},
  url = {http://proceedings.mlr.press/v139/radford21a},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/4R9QN9ET/Radford et al. - 2021 - Learning transferable visual models from natural l.pdf}
}

@article{2022-ha-DeepVisualConstraintsa,
  title = {Deep Visual Constraints: {{Neural}} Implicit Models for Manipulation Planning from Visual Input},
  shorttitle = {Deep Visual Constraints},
  author = {Ha, Jung-Su and Driess, Danny and Toussaint, Marc},
  year = {2022},
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {4},
  pages = {10857--10864},
  delete_delete_delete_publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/9844753/},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/UQ9FPNAW/Ha et al. - 2022 - Deep visual constraints Neural implicit models fo.pdf}
}

@inproceedings{2022-shridhar-CliportWhatWhere,
  title = {Cliport: {{What}} and Where Pathways for Robotic Manipulation},
  shorttitle = {Cliport},
  booktitle = {Conference on Robot Learning},
  author = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
  year = {2022},
  pages = {894--906},
  delete_delete_delete_publisher = {PMLR},
  url = {https://proceedings.mlr.press/v164/shridhar22a.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/LDUN9ZZ7/Shridhar et al. - 2022 - Cliport What and where pathways for robotic manip.pdf}
}

@inproceedings{2023-brohan-CanNotSay,
  title = {Do as i Can, Not as i Say: {{Grounding}} Language in Robotic Affordances},
  shorttitle = {Do as i Can, Not as i Say},
  booktitle = {Conference on Robot Learning},
  author = {Brohan, Anthony and Chebotar, Yevgen and Finn, Chelsea and Hausman, Karol and Herzog, Alexander and Ho, Daniel and Ibarz, Julian and Irpan, Alex and Jang, Eric and Julian, Ryan},
  year = {2023},
  pages = {287--318},
  delete_delete_delete_publisher = {PMLR},
  url = {https://proceedings.mlr.press/v205/ichter23a.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/FC4PP9P9/Ahn et al. - Do As I Can, Not As I Say Grounding Language in R.pdf}
}

@inproceedings{2023-driess-LearningMultiobjectDynamicsa,
  title = {Learning Multi-Object Dynamics with Compositional Neural Radiance Fields},
  booktitle = {Conference on Robot Learning},
  author = {Driess, Danny and Huang, Zhiao and Li, Yunzhu and Tedrake, Russ and Toussaint, Marc},
  year = {2023},
  pages = {1755--1768},
  delete_delete_delete_publisher = {PMLR},
  url = {https://proceedings.mlr.press/v205/driess23a.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/JMYXEAKE/Driess et al. - 2023 - Learning multi-object dynamics with compositional .pdf}
}

@online{2023-driess-PaLMEEmbodiedMultimodala,
  title = {{{PaLM-E}}: {{An Embodied Multimodal Language Model}}},
  shorttitle = {{{PaLM-E}}},
  author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
  year = {2023},
  eprint = {2303.03378},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.03378},
  urlyear = {2024},
  abstract = {Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/mtoussai/Zotero/storage/8D9I88PE/Driess et al. - 2023 - PaLM-E An Embodied Multimodal Language Model.pdf;/home/mtoussai/Zotero/storage/73CXFSVF/2303.html}
}

@inproceedings{2023-zitkovich-Rt2VisionlanguageactionModels,
  title = {Rt-2: {{Vision-language-action}} Models Transfer Web Knowledge to Robotic Control},
  shorttitle = {Rt-2},
  booktitle = {Conference on {{Robot Learning}}},
  author = {Zitkovich, Brianna and Yu, Tianhe and Xu, Sichun and Xu, Peng and Xiao, Ted and Xia, Fei and Wu, Jialin and Wohlhart, Paul and Welker, Stefan and Wahid, Ayzaan},
  year = {2023},
  pages = {2165--2183},
  delete_delete_delete_publisher = {PMLR},
  url = {https://proceedings.mlr.press/v229/zitkovich23a.html},
  urlyear = {2024},
  file = {/home/mtoussai/Zotero/storage/PJLASJ5D/Brohan et al. - RT-2 Vision-Language-Action Models Transfer Web K.pdf;/home/mtoussai/Zotero/storage/M4WJHT67/zitkovich23a.html}
}
