@inproceedings{2017-lowe-MultiAgentActorCriticMixed,
  title = {Multi-{{Agent Actor-Critic}} for {{Mixed Cooperative-Competitive Environments}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lowe, Ryan and family=WU, given=YI, given-i=YI and Tamar, Aviv and Harb, Jean and Pieter Abbeel, OpenAI and Mordatch, Igor},
  year = {2017},
  volume = {30},
  delete_delete_delete_publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/68a9750337a418a86fe06c1991a1d64c-Abstract.html},
  urlyear = {2024},
  abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
  file = {/home/mtoussai/Zotero/storage/FHWHRDC2/Lowe et al. - 2017 - Multi-Agent Actor-Critic for Mixed Cooperative-Com.pdf}
}

@inproceedings{2017-zaheer-DeepSets,
  title = {Deep {{Sets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  year = {2017},
  volume = {30},
  delete_delete_delete_publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2017/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html},
  urlyear = {2024},
  abstract = {We study the problem of designing models for machine learning tasks defined on sets. In contrast to the traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets and are invariant to permutations. Such problems are widespread, ranging from the estimation of population statistics, to anomaly detection in piezometer data of embankment dams, to cosmology. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
  file = {/home/mtoussai/Zotero/storage/CP2BILA5/Zaheer et al. - 2017 - Deep Sets.pdf}
}

@inproceedings{2018-everett-MotionPlanningDynamic,
  title = {Motion {{Planning Among Dynamic}}, {{Decision-Making Agents}} with {{Deep Reinforcement Learning}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Everett, Michael and Chen, Yu Fan and How, Jonathan P.},
  year = {2018},
  pages = {3052--3059},
  delete_delete_delete_publisher = {IEEE},
  location = {Madrid},
  delete_delete_delete_doi = {10.1109/IROS.2018.8593871},
  url = {https://ieeexplore.ieee.org/document/8593871/},
  urlyear = {2024},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  isbn = {978-1-5386-8094-0},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/KXMBPUDG/Everett et al. - 2018 - Motion Planning Among Dynamic, Decision-Making Age.pdf}
}

@article{2018-sunehag-ValueDecompositionNetworksCooperative,
  title = {Value-{{Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward}}},
  author = {Sunehag, Peter and Lever, Guy and Gruslys, Audrunas and Czarnecki, Wojciech Marian and Zambaldi, Vinicius and Jaderberg, Max and Lanctot, Marc and Sonnerat, Nicolas and Leibo, Joel Z and Tuyls, Karl and Graepel, Thore},
  year = {2018},
  abstract = {We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the “lazy agent” problem, which arises due to partial observability. We address these problems by training individual agents with a novel value-decomposition network architecture, which learns to decompose the team value function into agent-wise value functions.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/AFV8ZGZH/Sunehag et al. - 2018 - Value-Decomposition Networks For Cooperative Multi.pdf}
}

@article{2019-sartoretti-PRIMALPathfindingReinforcement,
  title = {{{PRIMAL}}: {{Pathfinding}} via {{Reinforcement}} and {{Imitation Multi-Agent Learning}}},
  shorttitle = {{{PRIMAL}}},
  author = {Sartoretti, Guillaume and Kerr, Justin and Shi, Yunfei and Wagner, Glenn and Kumar, T. K. Satish and Koenig, Sven and Choset, Howie},
  year = {2019},
  journal = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {4},
  number = {3},
  pages = {2378--2385},
  issn = {2377-3766, 2377-3774},
  delete_delete_delete_doi = {10.1109/LRA.2019.2903261},
  url = {https://ieeexplore.ieee.org/document/8661608/},
  urlyear = {2024},
  abstract = {Multi-agent path finding (MAPF) is an essential component of many large-scale, real-world robot deployments, from aerial swarms to warehouse automation. However, despite the community’s continued efforts, most state-of-the-art MAPF planners still rely on centralized planning and scale poorly past a few hundred agents. Such planning approaches are maladapted to realworld deployments, where noise and uncertainty often require paths be recomputed online, which is impossible when planning times are in seconds to minutes. We present PRIMAL, a novel framework for MAPF that combines reinforcement and imitation learning to teach fully decentralized policies, where agents reactively plan paths online in a partially observable world while exhibiting implicit coordination. This framework extends our previous work on distributed learning of collaborative policies by introducing demonstrations of an expert MAPF planner during training, as well as careful reward shaping and environment sampling. Once learned, the resulting policy can be copied onto any number of agents and naturally scales to different team sizes and world dimensions. We present results on randomized worlds with up to 1024 agents and compare success rates against state-of-the-art MAPF planners. Finally, we experimentally valiyear the learned policies in a hybrid simulation of a factory mockup, involving both real world and simulated robots.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/BFYHZGKY/Sartoretti et al. - 2019 - PRIMAL Pathfinding via Reinforcement and Imitatio.pdf}
}

@article{2020-fan-DistributedMultirobotCollision,
  title = {Distributed Multi-Robot Collision Avoidance via Deep Reinforcement Learning for Navigation in Complex Scenarios},
  author = {Fan, Tingxiang and Long, Pinxin and Liu, Wenxi and Pan, Jia},
  year = {2020},
  journal = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {39},
  number = {7},
  pages = {856--892},
  issn = {0278-3649, 1741-3176},
  delete_delete_delete_doi = {10.1177/0278364920916531},
  url = {http://journals.sagepub.com/delete_delete_delete_doi/10.1177/0278364920916531},
  urlyear = {2024},
  abstract = {Developing a safe and efficient collision-avoidance policy for multiple robots is challenging in the decentralized scenarios where each robot generates its paths with limited observation of other robots’ states and intentions. Prior distributed multi-robot collision-avoidance systems often require frequent inter-robot communication or agent-level features to plan a local collision-free action, which is not robust and computationally prohibitive. In addition, the performance of these methods is not comparable with their centralized counterparts in practice. In this article, we present a decentralized sensor-level collision-avoidance policy for multi-robot systems, which shows promising results in practical applications. In particular, our policy directly maps raw sensor measurements to an agent’s steering commands in terms of the movement velocity. As a first step toward reducing the performance gap between decentralized and centralized methods, we present a multi-scenario multi-stage training framework to learn an optimal policy. The policy is trained over a large number of robots in rich, complex environments simultaneously using a policy-gradient-based reinforcement-learning algorithm. The learning algorithm is also integrated into a hybrid control framework to further improve the policy’s robustness and effectiveness. We valiyear the learned sensor-level collision-3avoidance policy in a variety of simulated and real-world scenarios with thorough performance evaluations for large-scale multi-robot systems. The generalization of the learned policy is verified in a set of unseen scenarios including the navigation of a group of heterogeneous robots and a large-scale scenario with 100 robots. Although the policy is trained using simulation data only, we have successfully deployed it on physical robots with shapes and dynamics characteristics that are different from the simulated agents, in order to demonstrate the controller’s robustness against the simulation-to-real modeling error. Finally, we show that the collision-avoidance policy learned from multi-robot navigation tasks provides an excellent solution for safe and effective autonomous navigation for a single robot working in a dense real human crowd. Our learned policy enables a robot to make effective progress in a crowd without getting stuck. More importantly, the policy has been successfully deployed on different types of physical robot platforms without tedious parameter tuning. Videos are available at https://sites.google. com/view/hybridmrca.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/4GPWDTU4/Fan et al. - 2020 - Distributed multi-robot collision avoidance via de.pdf}
}

@article{2020-hu-VoronoiBasedMultiRobotAutonomous,
  title = {Voronoi-{{Based Multi-Robot Autonomous Exploration}} in {{Unknown Environments}} via {{Deep Reinforcement Learning}}},
  author = {Hu, Junyan and Niu, Hanlin and Carrasco, Joaquin and Lennox, Barry and Arvin, Farshad},
  year = {2020},
  journal = {IEEE Transactions on Vehicular Technology},
  shortjournal = {IEEE Trans. Veh. Technol.},
  volume = {69},
  number = {12},
  pages = {14413--14423},
  issn = {0018-9545, 1939-9359},
  delete_delete_delete_doi = {10.1109/TVT.2020.3034800},
  url = {https://ieeexplore.ieee.org/document/9244647/},
  urlyear = {2024},
  abstract = {Autonomous exploration is an important application of multi-vehicle systems, where a team of networked robots are coordinated to explore an unknown environment collaboratively. This technique has earned significant research interest due to its usefulness in search and rescue, fault detection and monitoring, localization and mapping, etc. In this paper, a novel cooperative exploration strategy is proposed for multiple mobile robots, which reduces the overall task completion time and energy costs compared to conventional methods. To efficiently navigate the networked robots during the collaborative tasks, a hierarchical control architecture is designed which contains a high-level decision making layer and a low-level target tracking layer. The proposed cooperative exploration approach is developed using dynamic Voronoi partitions, which minimizes duplicated exploration areas by assigning different target locations to individual robots. To deal with sudden obstacles in the unknown environment, an integrated deep reinforcement learning based collision avoidance algorithm is then proposed, which enables the control policy to learn from human demonstration data and thus improve the learning speed and performance. Finally, simulation and experimental results are provided to demonstrate the effectiveness of the proposed scheme.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/3X8D3IAJ/Hu et al. - 2020 - Voronoi-Based Multi-Robot Autonomous Exploration i.pdf}
}

@inproceedings{2020-li-GraphNeuralNetworks,
  title = {Graph {{Neural Networks}} for {{Decentralized Multi-Robot Path Planning}}},
  booktitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Li, Qingbiao and Gama, Fernando and Ribeiro, Alejandro and Prorok, Amanda},
  year = {2020},
  pages = {11785--11792},
  delete_delete_delete_publisher = {IEEE},
  location = {Las Vegas, NV, USA},
  delete_delete_delete_doi = {10.1109/IROS45743.2020.9341668},
  url = {https://ieeexplore.ieee.org/document/9341668/},
  urlyear = {2024},
  abstract = {Effective communication is key to successful, decentralized, multi-robot path planning. Yet, it is far from obvious what information is crucial to the task at hand, and how and when it must be shared among robots. To side-step these issues and move beyond hand-crafted heuristics, we propose a combined model that automatically synthesizes local communication and decision-making policies for robots navigating in constrained workspaces. Our architecture is composed of a convolutional neural network (CNN) that extracts adequate features from local observations, and a graph neural network (GNN) that communicates these features among robots. We train the model to imitate an expert algorithm, and use the resulting model online in decentralized planning involving only local communication and local observations. We evaluate our method in simulations by navigating teams of robots to their destinations in 2D cluttered workspaces. We measure the success rates and sum of costs over the planned paths. The results show a performance close to that of our expert algorithm, demonstrating the validity of our approach. In particular, we show our model’s capability to generalize to previously unseen cases (involving larger environments and larger robot teams).},
  booktitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  isbn = {978-1-72816-212-6},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/2PTD57QB/Li et al. - 2020 - Graph Neural Networks for Decentralized Multi-Robo.pdf}
}

@article{2020-riviere-GLASGlobaltoLocalSafe,
  title = {{{GLAS}}: {{Global-to-Local Safe Autonomy Synthesis}} for {{Multi-Robot Motion Planning With End-to-End Learning}}},
  shorttitle = {{{GLAS}}},
  author = {Riviere, Benjamin and Honig, Wolfgang and Yue, Yisong and Chung, Soon-Jo},
  year = {2020},
  journal = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {5},
  number = {3},
  pages = {4249--4256},
  issn = {2377-3766, 2377-3774},
  delete_delete_delete_doi = {10.1109/LRA.2020.2994035},
  url = {https://ieeexplore.ieee.org/document/9091314/},
  urlyear = {2024},
  abstract = {We present GLAS: Global-to-Local Autonomy Synthesis, a provably-safe, automated distributed policy generation for multi-robot motion planning. Our approach combines the advantage of centralized planning of avoiding local minima with the advantage of decentralized controllers of scalability and distributed computation. In particular, our synthesized policies only require relative state information of nearby neighbors and obstacles, and compute a provably-safe action. Our approach has three major components: i) we generate demonstration trajectories using a global planner and extract local observations from them, ii) we use deep imitation learning to learn a decentralized policy that can run efficiently online, and iii) we introduce a novel differentiable safety module to ensure collision-free operation, thereby allowing for end-to-end policy training. Our numerical experiments demonstrate that our policies have a 20\% higher success rate than optimal reciprocal collision avoidance, ORCA, across a wide range of robot and obstacle densities. We demonstrate our method on an aerial swarm, executing the policy on low-end microcontrollers in real-time.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/53RL3PQW/Riviere et al. - 2020 - GLAS Global-to-Local Safe Autonomy Synthesis for .pdf}
}

@inproceedings{2020-shi-NeuralSwarmDecentralizedCloseProximity,
  title = {Neural-{{Swarm}}: {{Decentralized Close-Proximity Multirotor Control Using Learned Interactions}}},
  shorttitle = {Neural-{{Swarm}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Shi, Guanya and Honig, Wolfgang and Yue, Yisong and Chung, Soon-Jo},
  year = {2020},
  pages = {3241--3247},
  delete_delete_delete_publisher = {IEEE},
  location = {Paris, France},
  delete_delete_delete_doi = {10.1109/ICRA40945.2020.9196800},
  url = {https://ieeexplore.ieee.org/document/9196800/},
  urlyear = {2024},
  abstract = {In this paper, we present Neural-Swarm, a nonlinear decentralized stable controller for close-proximity flight of multirotor swarms. Close-proximity control is challenging due to the complex aerodynamic interaction effects between multirotors, such as downwash from higher vehicles to lower ones. Conventional methods often fail to properly capture these interaction effects, resulting in controllers that must maintain large safety distances between vehicles, and thus are not capable of close-proximity flight. Our approach combines a nominal dynamics model with a regularized permutation-invariant Deep Neural Network (DNN) that accurately learns the high-order multi-vehicle interactions. We design a stable nonlinear tracking controller using the learned model. Experimental results demonstrate that the proposed controller significantly outperforms a baseline nonlinear tracking controller with up to four times smaller worst-case height tracking errors. We also empirically demonstrate the ability of our learned model to generalize to larger swarm sizes.},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-72817-395-5},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/VEMEJII2/Shi et al. - 2020 - Neural-Swarm Decentralized Close-Proximity Multir.pdf}
}

@inproceedings{2020-tolstaya-LearningDecentralizedControllers,
  title = {Learning {{Decentralized Controllers}} for {{Robot Swarms}} with {{Graph Neural Networks}}},
  booktitle = {Proceedings of the {{Conference}} on {{Robot Learning}}},
  author = {Tolstaya, Ekaterina and Gama, Fernando and Paulos, James and Pappas, George and Kumar, Vijay and Ribeiro, Alejandro},
  year = {2020},
  pages = {671--682},
  delete_delete_delete_publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v100/tolstaya20a.html},
  urlyear = {2024},
  abstract = {We consider the problem of finding distributed controllers for large networks of mobile robots with interacting dynamics and sparsely available communications. Our approach is to learn local controllers that require only local information and communications at test time by imitating the policy of centralized controllers using global information at training time. By extending aggregation graph neural networks to time varying signals and time varying network support, we learn a single common local controller which exploits information from distant teammates using only local communication interchanges. We apply this approach to the problem of flocking to demonstrate performance on communication graphs that change as the robots move. We examine how a decreasing communication radius and faster velocities increase the value of multi-hop information.},
  booktitle = {Conference on {{Robot Learning}}},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/6ZXZYICM/Tolstaya et al. - 2020 - Learning Decentralized Controllers for Robot Swarm.pdf}
}

@article{2021-damani-PRIMAL_2Pathfinding,
  title = {{{PRIMAL}}\$\_2\$: {{Pathfinding Via Reinforcement}} and {{Imitation Multi-Agent Learning}} - {{Lifelong}}},
  shorttitle = {{{PRIMAL}}\$\_2\$},
  author = {Damani, Mehul and Luo, Zhiyao and Wenzel, Emerson and Sartoretti, Guillaume},
  year = {2021},
  journal = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {6},
  number = {2},
  pages = {2666--2673},
  issn = {2377-3766, 2377-3774},
  delete_delete_delete_doi = {10.1109/LRA.2021.3062803},
  url = {https://ieeexplore.ieee.org/document/9366340/},
  urlyear = {2024},
  abstract = {Multi-agent path finding (MAPF) is an indispensable component of large-scale robot deployments in numerous domains ranging from airport management to warehouse automation. In particular, this work addresses lifelong MAPF (LMAPF) – an online variant of the problem where agents are immediately assigned a new goal upon reaching their current one – in dense and highly structured environments, typical of real-world warehouse operations. Effectively solving LMAPF in such environments requires expensive coordination between agents as well as frequent replanning abilities, a daunting task for existing coupled and decoupled approaches alike. With the purpose of achieving considerable agent coordination without any compromise on reactivity and scalability, we introduce PRIMAL2, a distributed reinforcement learning framework for LMAPF where agents learn fully decentralized policies to reactively plan paths online in a partially observable world. We extend our previous work, which was effective in low-density sparsely occupied worlds, to highly structured and constrained worlds by identifying behaviors and conventions which improve implicit agent coordination, and enable their learning through the construction of a novel local agent observation and various training aids. We present extensive results of PRIMAL2 in both MAPF and LMAPF environments and compare its performance to state-of-the-art planners in terms of makespan and throughput. We show that PRIMAL2 significantly surpasses our previous work and performs comparably to these baselines, while allowing real-time re-planning and scaling up to 2048 agents.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/5CDDPWN5/Damani et al. - 2021 - PRIMAL$_2$ Pathfinding Via Reinforcement and Imit.pdf}
}

@inproceedings{2021-kortvelesy-ModGNNExpertPolicy,
  title = {{{ModGNN}}: {{Expert Policy Approximation}} in {{Multi-Agent Systems}} with a {{Modular Graph Neural Network Architecture}}},
  shorttitle = {{{ModGNN}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Kortvelesy, Ryan and Prorok, Amanda},
  year = {2021},
  pages = {9161--9167},
  delete_delete_delete_publisher = {IEEE},
  location = {Xi'an, China},
  delete_delete_delete_doi = {10.1109/ICRA48506.2021.9561386},
  url = {https://ieeexplore.ieee.org/document/9561386/},
  urlyear = {2024},
  abstract = {Recent work in the multi-agent domain has shown the promise of Graph Neural Networks (GNNs) to learn complex coordination strategies. However, most current approaches use minor variants of a Graph Convolutional Network (GCN), which applies a convolution to the communication graph formed by the multi-agent system. In this paper, we investigate whether the performance and generalization of GCNs can be improved upon. We introduce ModGNN, a decentralized framework which serves as a generalization of GCNs, providing more flexibility. To test our hypothesis, we evaluate an implementation of ModGNN against several baselines in the multi-agent flocking problem. We perform an ablation analysis to show that the most important component of our framework is one that does not exist in a GCN. By varying the number of agents, we also demonstrate that an application-agnostic implementation of ModGNN possesses an improved ability to generalize to new environments.},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-72819-077-8},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/EV2ESCQ3/Kortvelesy and Prorok - 2021 - ModGNN Expert Policy Approximation in Multi-Agent.pdf}
}

@article{2021-li-MessageAwareGraphAttention,
  title = {Message-{{Aware Graph Attention Networks}} for {{Large-Scale Multi-Robot Path Planning}}},
  author = {Li, Qingbiao and Lin, Weizhe and Liu, Zhe and Prorok, Amanda},
  year = {2021},
  journal = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {6},
  number = {3},
  pages = {5533--5540},
  issn = {2377-3766, 2377-3774},
  delete_delete_delete_doi = {10.1109/LRA.2021.3077863},
  url = {https://ieeexplore.ieee.org/document/9424371/},
  urlyear = {2024},
  abstract = {The domains of transport and logistics are increasingly relying on autonomous mobile robots for the handling and distribution of passengers or resources. At large system scales, finding decentralized path planning and coordination solutions is key to efficient system performance. Recently, Graph Neural Networks (GNNs) have become popular due to their ability to learn communication policies in decentralized multi-agent systems. Yet, vanilla GNNs rely on simplistic message aggregation mechanisms that prevent agents from prioritizing important information. To tackle this challenge, in this letter, we extend our previous work that utilizes GNNs in multi-agent path planning by incorporating a novel mechanism to allow for message-dependent attention. Our Message-Aware Graph Attention neTwork (MAGAT) is based on a key-query-like mechanism that determines the relative importance of features in the messages received from various neighboring robots. We show that MAGAT is able to achieve a performance close to that of a coupled centralized expert algorithm. Further, ablation studies and comparisons to several benchmark models show that our attention mechanism is very effective across different robot densities and performs stably in different constraints in communication bandwidth. Experiments demonstrate that our model is able to generalize well in previously unseen problem instances, and that it achieves a 47\% improvement over the benchmark success rate, even in very large-scale instances that are ×100 larger than the training instances.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/KH27ASU3/Li et al. - 2021 - Message-Aware Graph Attention Networks for Large-S.pdf}
}

@article{2021-riviere-NeuralTreeExpansion,
  title = {Neural {{Tree Expansion}} for {{Multi-Robot Planning}} in {{Non-Cooperative Environments}}},
  author = {Riviere, Benjamin and Honig, Wolfgang and Anderson, Matthew and Chung, Soon-Jo},
  year = {2021},
  journal = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {6},
  number = {4},
  pages = {6868--6875},
  issn = {2377-3766, 2377-3774},
  delete_delete_delete_doi = {10.1109/LRA.2021.3096758},
  url = {https://ieeexplore.ieee.org/document/9484771/},
  urlyear = {2024},
  abstract = {We present a self-improving, Neural Tree Expansion (NTE) method for multi-robot online planning in non-cooperative environments, where each robot attempts to maximize its cumulative reward while interacting with other self-interested robots. Our algorithm adapts the centralized, perfect information, discreteaction space method from AlphaZero to a decentralized, partial information, continuous action space setting for multi-robot applications. Our method has three interacting components: (i) a centralized, perfect-information “expert” Monte Carlo Tree Search (MCTS) with large computation resources that provides expert demonstrations, (ii) a decentralized, partial-information “learner” MCTS with small computation resources that runs in real-time and provides self-play examples, and (iii) policy \& value neural networks that are trained with the expert demonstrations and bias both the expert and the learner tree growth. Our numerical experiments demonstrate Neural Tree Expansion’s computational advantage by finding better solutions than a MCTS with 20 times more resources. The resulting policies are dynamically sophisticated, demonstrate coordination between robots, and play the Reach-Target-Avoid differential game significantly better than the state-of-the-art control-theoretic baseline for multi-robot, doubleintegrator systems. Our hardware experiments on an aerial swarm demonstrate the computational advantage of Neural Tree Expansion, enabling online planning at 20 Hz with effective policies in complex scenarios.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/74S2QLG6/Riviere et al. - 2021 - Neural Tree Expansion for Multi-Robot Planning in .pdf}
}

@article{2022-gama-SynthesizingDecentralizedControllers,
  title = {Synthesizing {{Decentralized Controllers With Graph Neural Networks}} and {{Imitation Learning}}},
  author = {Gama, Fernando and Li, Qingbiao and Tolstaya, Ekaterina and Prorok, Amanda and Ribeiro, Alejandro},
  year = {2022},
  journal = {IEEE Transactions on Signal Processing},
  shortjournal = {IEEE Trans. Signal Process.},
  volume = {70},
  pages = {1932--1946},
  issn = {1053-587X, 1941-0476},
  delete_delete_delete_doi = {10.1109/TSP.2022.3166401},
  url = {https://ieeexplore.ieee.org/document/9755021/},
  urlyear = {2024},
  abstract = {Dynamical systems consisting of a set of autonomous agents face the challenge of having to accomplish a global task, relying only on local information. While centralized controllers are readily available, they face limitations in terms of scalability and implementation, as they do not respect the distributed information structure imposed by the network system of agents. Given the difficulties in finding optimal decentralized controllers, we propose a novel framework using graph neural networks (GNNs) to learn these controllers. GNNs are well-suited for the task since they are naturally distributed architectures and exhibit good scalability and transferability properties. We show that GNNs learn appropriate decentralized controllers by means of imitation learning, leverage their permutation invariance properties to successfully scale to larger teams and transfer to unseen scenarios at deployment time. The problems of flocking and multi-agent path planning are explored to illustrate the potential of GNNs in learning decentralized controllers.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/B33QIANH/Gama et al. - 2022 - Synthesizing Decentralized Controllers With Graph .pdf}
}

@article{2022-shi-NeuralSwarm2PlanningControl,
  title = {Neural-{{Swarm2}}: {{Planning}} and {{Control}} of {{Heterogeneous Multirotor Swarms Using Learned Interactions}}},
  shorttitle = {Neural-{{Swarm2}}},
  author = {Shi, Guanya and Honig, Wolfgang and Shi, Xichen and Yue, Yisong and Chung, Soon-Jo},
  year = {2022},
  journal = {IEEE Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {38},
  number = {2},
  pages = {1063--1079},
  issn = {1552-3098, 1941-0468},
  delete_delete_delete_doi = {10.1109/TRO.2021.3098436},
  url = {https://ieeexplore.ieee.org/document/9508420/},
  urlyear = {2024},
  abstract = {We present Neural-Swarm2, a learning-based method for motion planning and control that allows heterogeneous multirotors in a swarm to safely fly in close proximity. Such operation for drones is challenging due to complex aerodynamic interaction forces, such as downwash generated by nearby drones and ground effect. Conventional planning and control methods neglect capturing these interaction forces, resulting in sparse swarm configuration during flight. Our approach combines a physics-based nominal dynamics model with learned deep neural networks with strong Lipschitz properties. We make use of two techniques to accurately predict the aerodynamic interactions between heterogeneous multirotors: 1) Spectral normalization for stability and generalization guarantees of unseen data and 2) heterogeneous deep sets for supporting any number of heterogeneous neighbors in a permutationinvariant manner without reducing expressiveness. The learned residual dynamics benefit both the proposed interaction-aware multirobot motion planning and the nonlinear tracking control design because the learned interaction forces reduce the modelling errors. Experimental results demonstrate that Neural-Swarm2 is able to generalize to larger swarms beyond training cases and significantly outperforms a baseline nonlinear tracking controller with up to three times reduction in worst-case tracking errors.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/K5RC9APD/Shi et al. - 2022 - Neural-Swarm2 Planning and Control of Heterogeneo.pdf}
}

@article{2022-wang-DistributedReinforcementLearning,
  title = {Distributed {{Reinforcement Learning}} for {{Robot Teams}}: A {{Review}}},
  shorttitle = {Distributed {{Reinforcement Learning}} for {{Robot Teams}}},
  author = {Wang, Yutong and Damani, Mehul and Wang, Pamela and Cao, Yuhong and Sartoretti, Guillaume},
  year = {2022},
  journal = {Current Robotics Reports},
  shortjournal = {Curr Robot Rep},
  volume = {3},
  number = {4},
  pages = {239--257},
  issn = {2662-4087},
  delete_delete_delete_doi = {10.1007/s43154-022-00091-8},
  url = {https://delete_delete_delete_doi.org/10.1007/s43154-022-00091-8},
  urlyear = {2024},
  abstract = {Recent advances in sensing, actuation, and computation have opened the door to multi-robot systems consisting of hundreds/thousands of robots, with promising applications to automated manufacturing, disaster relief, harvesting, last-mile delivery, port/airport operations, or search and rescue. The community has leveraged model-free multi-agent reinforcement learning (MARL) to devise efficient, scalable controllers for multi-robot systems (MRS). This review aims to provide an analysis of the state-of-the-art in distributed MARL for multi-robot cooperation.},
  langid = {english},
  keywords = {Communication learning,Cooperation,Mixed cooperative-competitive settings,Motion planning,Multi-robot systems,Reinforcement learning},
  file = {/home/mtoussai/Zotero/storage/VRUMZLA4/Wang et al. - 2022 - Distributed Reinforcement Learning for Robot Teams.pdf}
}

@article{2022-yu-DiNNODistributedNeural,
  title = {{{DiNNO}}: {{Distributed Neural Network Optimization}} for {{Multi-Robot Collaborative Learning}}},
  shorttitle = {{{DiNNO}}},
  author = {Yu, Javier and Vincent, Joseph A. and Schwager, Mac},
  year = {2022},
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {2},
  pages = {1896--1903},
  issn = {2377-3766},
  delete_delete_delete_doi = {10.1109/LRA.2022.3142402},
  url = {https://ieeexplore.ieee.org/abstract/document/9681319},
  urlyear = {2024},
  abstract = {We present DiNNO, a distributed algorithm that enables a group of robots to collaboratively optimize a deep neural network model while communicating over a mesh network. Each robot only has access to its own data and maintains its own version of the neural network, but eventually learns a model that is as good as if it had been trained on all the data centrally. No robot sends raw data over the wireless network, preserving data privacy and ensuring efficient use of wireless bandwidth. At each iteration, each robot approximately optimizes an augmented Lagrangian function, then communicates the resulting weights to its neighbors, upyears dual variables, and repeats. Eventually, all robots’ local model weights reach a consensus. For convex objective functions, this consensus is a global optimum. Unlike many existing methods we test our algorithm on robotics-related, deep learning tasks with nontrivial model architectures. We compare DiNNO to two benchmark distributed deep learning algorithms in (i) an MNIST image classification task, (ii) a multi-robot implicit mapping task, and (iii) a multi-robot reinforcement learning task. In these experiments we show that DiNNO performs well when faced with nonconvex deep learning objectives, time-varying communication graphs, and streaming data. In all experiments our method outperforms baselines, and was able to achieve validation loss equivalent to centrally trained models. See msl.stanford.edu/projects/dist\_nn\_train for videos and code.},
  booktitle = {{{IEEE Robotics}} and {{Automation Letters}}},
  keywords = {Data models,Deep learning,Deep learning methods,distributed robot systems,multi-robot systems,Neural networks,Optimization,Robots,Task analysis,Training},
  file = {/home/mtoussai/Zotero/storage/GXC7KSJJ/Yu et al. - 2022 - DiNNO Distributed Neural Network Optimization for.pdf;/home/mtoussai/Zotero/storage/753RABDB/9681319.html}
}

@inproceedings{2022-yu-LearningControlAdmissibility,
  title = {Learning {{Control Admissibility Models}} with {{Graph Neural Networks}} for {{Multi-Agent Navigation}}},
  author = {Yu, Chenning and Yu, Hongzhan and Gao, Sicun},
  year = {2022},
  url = {https://openreview.net/forum?id=xC-68ANJeK_},
  urlyear = {2024},
  abstract = {Deep reinforcement learning in continuous domains focuses on learning control policies that map states to distributions over actions that ideally concentrate on the optimal choices in each step. In multi-agent navigation problems, the optimal actions depend heavily on the agents' density. Their interaction patterns grow exponentially with respect to such density, making it hard for learning-based methods to generalize. We propose to switch the learning objectives from predicting the optimal actions to predicting sets of admissible actions, which we call control admissibility models (CAMs), such that they can be easily composed and used for online inference for an arbitrary number of agents. We design CAMs using graph neural networks and develop training methods that optimize the CAMs in the standard model-free setting, with the additional benefit of eliminating the need for reward engineering typically required to balance collision avoidance and goal-reaching requirements. We evaluate the proposed approach in multi-agent navigation environments. We show that the CAM models can be trained in environments with only a few agents and be easily composed for deployment in dense environments with hundreds of agents, achieving better performance than state-of-the-art methods.},
  booktitle = {6th {{Annual Conference}} on {{Robot Learning}}},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/YAJBDF2B/Yu et al. - 2022 - Learning Control Admissibility Models with Graph N.pdf}
}

@inproceedings{2022-zabounidis-ConceptLearningInterpretable,
  title = {Concept {{Learning}} for {{Interpretable Multi-Agent Reinforcement Learning}}},
  author = {Zabounidis, Renos and Campbell, Joseph and Stepputtis, Simon and Hughes, Dana and Sycara, Katia P.},
  year = {2022},
  url = {https://openreview.net/forum?id=TAgVKiF2O8p},
  urlyear = {2024},
  abstract = {Multi-agent robotic systems are increasingly operating in real-world environments in close proximity to humans, yet are largely controlled by policy models with inscrutable deep neural network representations. We introduce a method for incorporating interpretable concepts from a domain expert into models trained through multi-agent reinforcement learning, by requiring the model to first predict such concepts then utilize them for decision making. This allows an expert to both reason about the resulting concept policy models in terms of these high-level concepts at run-time, as well as intervene and correct mispredictions to improve performance. We show that this yields improved interpretability and training stability, with benefits to policy performance and sample efficiency in a simulated and real-world cooperative-competitive multi-agent game.},
  booktitle = {6th {{Annual Conference}} on {{Robot Learning}}},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/G5LR9HU5/Zabounidis et al. - 2022 - Concept Learning for Interpretable Multi-Agent Rei.pdf}
}

@article{2022-zhou-MultiRobotCollaborativePerception,
  title = {Multi-{{Robot Collaborative Perception With Graph Neural Networks}}},
  author = {Zhou, Yang and Xiao, Jiuhong and Zhou, Yue and Loianno, Giuseppe},
  year = {2022},
  journal = {IEEE Robotics and Automation Letters},
  shortjournal = {IEEE Robot. Autom. Lett.},
  volume = {7},
  number = {2},
  pages = {2289--2296},
  issn = {2377-3766, 2377-3774},
  delete_delete_delete_doi = {10.1109/LRA.2022.3141661},
  url = {https://ieeexplore.ieee.org/document/9676458/},
  urlyear = {2024},
  abstract = {Multi-robot systems such as swarms of aerial robots are naturally suited to offer additional flexibility, resilience, and robustness in several tasks compared to a single robot by enabling cooperation among the agents. To enhance the autonomous robot decision-making process and situational awareness, multi-robot systems have to coordinate their perception capabilities to collect, share, and fuse environment information among the agents efficiently to obtain context-appropriate information or gain resilience to sensor noise or failures. In this letter, we propose a general-purpose Graph Neural Network (GNN) with the main goal to increase, in multi-robot perception tasks, single robots’ inference perception accuracy as well as resilience to sensor failures and disturbances. We show that the proposed framework can address multi-view visual perception problems such as monocular depth estimation and semantic segmentation. Several experiments both using photo-realistic and real data gathered from multiple aerial robots’ viewpoints show the effectiveness of the proposed approach in challenging inference conditions including images corrupted by heavy noise and camera occlusions or failures.},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/FVI2HZBJ/Zhou et al. - 2022 - Multi-Robot Collaborative Perception With Graph Ne.pdf}
}

@inproceedings{2023-howell-GeneralizationHeterogeneousMultiRobot,
  title = {Generalization of {{Heterogeneous Multi-Robot Policies}} via {{Awareness}} and {{Communication}} of {{Capabilities}}},
  author = {Howell, Pierce and Rudolph, Max and Torbati, Reza Joseph and Fu, Kevin and Ravichandar, Harish},
  year = {2023},
  url = {https://openreview.net/forum?id=N3VbFUpwaa},
  urlyear = {2024},
  abstract = {Recent advances in multi-agent reinforcement learning (MARL) are enabling impressive coordination in heterogeneous multi-robot teams. However, existing approaches often overlook the challenge of generalizing learned policies to teams of new compositions, sizes, and robots. While such generalization might not be important in teams of virtual agents that can retrain policies on-demand, it is pivotal in multi-robot systems that are deployed in the real-world and must readily adapt to inevitable changes. As such, multi-robot policies must remain robust to team changes -- an ability we call adaptive teaming. In this work, we investigate if awareness and communication of robot capabilities can provide such generalization by conducting detailed experiments involving an established multi-robot test bed. We demonstrate that shared decentralized policies, that enable robots to be both aware of and communicate their capabilities, can achieve adaptive teaming by implicitly capturing the fundamental relationship between collective capabilities and effective coordination. Videos of trained policies can be viewed at https://sites.google.com/view/cap-comm .},
  booktitle = {7th {{Annual Conference}} on {{Robot Learning}}},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/7A2NXQM5/Howell et al. - 2023 - Generalization of Heterogeneous Multi-Robot Polici.pdf}
}

@inproceedings{2023-liu-TraCoLearningVirtual,
  title = {{{TraCo}}: {{Learning Virtual Traffic Coordinator}} for {{Cooperation}} with {{Multi-Agent Reinforcement Learning}}},
  shorttitle = {{{TraCo}}},
  author = {Liu, Weiwei and Jing, Wei and Gao, Lingping and Guo, Ke and Xu, Gang and Liu, Yong},
  year = {2023},
  url = {https://openreview.net/forum?id=TgJ8vJUVUBR},
  urlyear = {2024},
  abstract = {Multi-agent reinforcement learning (MARL) has emerged as a popular technique in diverse domains due to its ability to automate system controller design and facilitate continuous intelligence learning. For instance, traffic flow is often trained with MARL to enable intelligent simulations for autonomous driving. However, The existing MARL algorithm only characterizes the relative degree of each agent's contribution to the team, and cannot express the contribution that the team needs from the agent. Especially in the field of autonomous driving, the team changes over time, and the agent needs to act directly according to the needs of the team. To address these limitations, we propose an innovative method inspired by realistic traffic coordinators called the Traffic Coordinator Network (TraCo). Our approach leverages a combination of cross-attention and counterfactual advantage function, allowing us to extract distinctive characteristics of domain agents and accurately quantify the contribution that a team needs from an agent. Through experiments conducted on four traffic tasks, we demonstrate that our method outperforms existing approaches, yielding superior performance. Furthermore, our approach enables the emergence of rich and diverse social behaviors among vehicles within the traffic flow.},
  booktitle = {7th {{Annual Conference}} on {{Robot Learning}}},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/YAIL6YUZ/Liu et al. - 2023 - TraCo Learning Virtual Traffic Coordinator for Co.pdf}
}

@article{2023-orr-MultiAgentDeepReinforcement,
  title = {Multi-{{Agent Deep Reinforcement Learning}} for {{Multi-Robot Applications}}: {{A Survey}}},
  shorttitle = {Multi-{{Agent Deep Reinforcement Learning}} for {{Multi-Robot Applications}}},
  author = {Orr, James and Dutta, Ayan},
  year = {2023},
  journal = {Sensors},
  volume = {23},
  number = {7},
  pages = {3625},
  delete_delete_delete_publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  delete_delete_delete_doi = {10.3390/s23073625},
  url = {https://www.mdpi.com/1424-8220/23/7/3625},
  urlyear = {2024},
  abstract = {Deep reinforcement learning has produced many success stories in recent years. Some example fields in which these successes have taken place include mathematics, games, health care, and robotics. In this paper, we are especially interested in multi-agent deep reinforcement learning, where multiple agents present in the environment not only learn from their own experiences but also from each other and its applications in multi-robot systems. In many real-world scenarios, one robot might not be enough to complete the given task on its own, and, therefore, we might need to deploy multiple robots who work together towards a common global objective of finishing the task. Although multi-agent deep reinforcement learning and its applications in multi-robot systems are of tremendous significance from theoretical and applied standpoints, the latest survey in this domain years to 2004 albeit for traditional learning applications as deep reinforcement learning was not invented. We classify the reviewed papers in our survey primarily based on their multi-robot applications. Our survey also discusses a few challenges that the current research in this domain faces and provides a potential list of future applications involving multi-robot systems that can benefit from advances in multi-agent deep reinforcement learning.},
  issue = {7},
  langid = {english},
  keywords = {deep reinforcement learning,multi-agent learning,multi-robot systems,survey},
  file = {/home/mtoussai/Zotero/storage/CWSG83PE/Orr and Dutta - 2023 - Multi-Agent Deep Reinforcement Learning for Multi-.pdf}
}

@inproceedings{2023-wu-HijackingRobotTeams,
  title = {Hijacking {{Robot Teams Through Adversarial Communication}}},
  author = {Wu, Zixuan and Ye, Sean Charles and Han, Byeolyi and Gombolay, Matthew},
  year = {2023},
  url = {https://openreview.net/forum?id=bIvIUNH9VQ},
  urlyear = {2024},
  abstract = {Communication is often necessary for robot teams to collaborate and complete a decentralized task. Multi-agent reinforcement learning (MARL) systems allow agents to learn how to collaborate and communicate to complete a task. These domains are ubiquitous and include safety-critical domains such as wildfire fighting, traffic control, or search and rescue missions. However, critical vulnerabilities may arise in communication systems as jamming the signals can interrupt the robot team. This work presents a framework for applying black-box adversarial attacks to learned MARL policies by manipulating only the communication signals between agents. Our system only requires observations of MARL policies after training is complete, as this is more realistic than attacking the training process. To this end, we imitate a learned policy of the targeted agents without direct interaction with the environment or ground truth rewards. Instead, we infer the rewards by only observing the behavior of the targeted agents. Our framework reduces reward by 201\% compared to an equivalent baseline method and also shows favorable results when deployed in real swarm robots. Our novel attack methodology within MARL systems contributes to the field by enhancing our understanding on the reliability of multi-agent systems.},
  booktitle = {7th {{Annual Conference}} on {{Robot Learning}}},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/WMADPMG6/Wu et al. - 2023 - Hijacking Robot Teams Through Adversarial Communic.pdf}
}

@inproceedings{2023-wu-IntentAwarePlanningHeterogeneous,
  title = {Intent-{{Aware Planning}} in {{Heterogeneous Traffic}} via {{Distributed Multi-Agent Reinforcement Learning}}},
  author = {Wu, Xiyang and Chandra, Rohan and Guan, Tianrui and Bedi, Amrit and Manocha, Dinesh},
  year = {2023},
  url = {https://openreview.net/forum?id=EvuAJ0wD98},
  urlyear = {2024},
  abstract = {Navigating safely and efficiently in dense and heterogeneous traffic scenarios is challenging for autonomous vehicles (AVs) due to their inability to infer the behaviors or intentions of nearby drivers. In this work, we introduce a distributed multi-agent reinforcement learning (MARL) algorithm for joint trajectory and intent prediction for autonomous vehicles in dense and heterogeneous environments. Our approach for intent-aware planning, iPLAN, allows agents to infer nearby drivers' intents solely from their local observations. We model an explicit representation of agents' private incentives: Behavioral Incentive for high-level decision-making strategy that sets planning sub-goals and Instant Incentive for low-level motion planning to execute sub-goals. Our approach enables agents to infer their opponents' behavior incentives and integrate this inferred information into their decision-making and motion-planning processes. We perform experiments on two simulation environments, Non-Cooperative Navigation and Heterogeneous Highway. In Heterogeneous Highway, results show that, compared with centralized training decentralized execution (CTDE) MARL baselines such as QMIX and MAPPO, our method yields a \$4.3\textbackslash\%\$ and \$38.4\textbackslash\%\$ higher episodic reward in mild and chaotic traffic, with \$48.1\textbackslash\%\$ higher success rate and \$80.6\textbackslash\%\$ longer survival time in chaotic traffic. We also compare with a decentralized training decentralized execution (DTDE) baseline IPPO and demonstrate a higher episodic reward of \$12.7\textbackslash\%\$ and \$6.3\textbackslash\%\$ in mild traffic and chaotic traffic, \$25.3\textbackslash\%\$ higher success rate, and \$13.7\textbackslash\%\$ longer survival time.},
  booktitle = {7th {{Annual Conference}} on {{Robot Learning}}},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/PEZ3FR3R/Wu et al. - 2023 - Intent-Aware Planning in Heterogeneous Traffic via.pdf}
}

@inproceedings{2023-zhang-NeuralGraphControl,
  title = {Neural {{Graph Control Barrier Functions Guided Distributed Collision-avoidance Multi-agent Control}}},
  author = {Zhang, Songyuan and Garg, Kunal and Fan, Chuchu},
  year = {2023},
  url = {https://openreview.net/forum?id=VscdYkKgwdH},
  urlyear = {2024},
  abstract = {We consider the problem of designing distributed collision-avoidance multi-agent control in large-scale environments with potentially moving obstacles, where a large number of agents are required to maintain safety using only local information and reach their goals. This paper addresses the problem of collision avoidance, scalability, and generalizability by introducing graph control barrier functions (GCBFs) for distributed control. The newly introduced GCBF is based on the well-established CBF theory for safety guarantees but utilizes a graph structure for scalable and generalizable decentralized control. We use graph neural networks to learn both neural a GCBF certificate and distributed control. We also extend the framework from handling state-based models to directly taking point clouds from LiDAR for more practical robotics settings. We demonstrated the efficacy of GCBF in a variety of numerical experiments, where the number, density, and traveling distance of agents, as well as the number of unseen and uncontrolled obstacles increase. Empirical results show that GCBF outperforms leading methods such as MAPPO and multi-agent distributed CBF (MDCBF). Trained with only \$16\$ agents, GCBF can achieve up to \$3\$ times improvement of success rate (agents reach goals and never encountered in any collisions) on \${$<$}500\$ agents, and still maintain more than \$50\textbackslash\%\$ success rates for \${$>\backslash$}!1000\$ agents when other methods completely fail.},
  booktitle = {7th {{Annual Conference}} on {{Robot Learning}}},
  langid = {english},
  file = {/home/mtoussai/Zotero/storage/W4VWX7DS/Zhang et al. - 2023 - Neural Graph Control Barrier Functions Guided Dist.pdf}
}

@book{2024-bishop-DeepLearningFoundations,
  title = {Deep {{Learning}}: {{Foundations}} and {{Concepts}}},
  shorttitle = {Deep {{Learning}}},
  author = {Bishop, Christopher M. and Bishop, Hugh},
  year = {2024},
  delete_delete_delete_publisher = {Springer International Publishing},
  location = {Cham},
  delete_delete_delete_doi = {10.1007/978-3-031-45468-4},
  url = {https://link.springer.com/10.1007/978-3-031-45468-4},
  urlyear = {2024},
  isbn = {978-3-031-45467-7 978-3-031-45468-4},
  langid = {english},
  keywords = {Convolutional networks,Decision theory,Deep learning,Directed graphical models,machine learning,Neural networks}
}

@online{2024-chen-WhySolvingMultiagent,
  title = {Why {{Solving Multi-agent Path Finding}} with {{Large Language Model}} Has Not {{Succeeded Yet}}},
  author = {Chen, Weizhe and Koenig, Sven and Dilkina, Bistra},
  year = {2024},
  eprint = {2401.03630},
  eprinttype = {arXiv},
  eprintclass = {cs},
  delete_delete_delete_doi = {10.48550/arXiv.2401.03630},
  url = {http://arxiv.org/abs/2401.03630},
  urlyear = {2024},
  abstract = {With the explosive influence caused by the success of large language models (LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work showing that foundation models can be used to solve a large variety of tasks. However, there is very limited work that shares insights on multi-agent planning. Multi-agent planning is different from other domains by combining the difficulty of multi-agent coordination and planning, and making it hard to leverage external tools to facilitate the reasoning needed. In this paper, we focus on the problem of multi-agent path finding (MAPF), which is also known as multi-robot route planning, and study the performance of solving MAPF with LLMs. We first show the motivating success on an empty room map without obstacles, then the failure to plan on the harder room map and maze map of the standard MAPF benchmark. We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis. Based on our results, we discussed how researchers with different backgrounds could help with this problem from different perspectives.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Multiagent Systems},
  file = {/home/mtoussai/Zotero/storage/BR9BTFS6/Chen et al. - 2024 - Why Solving Multi-agent Path Finding with Large La.pdf;/home/mtoussai/Zotero/storage/QCYFELAD/2401.html}
}

@article{2024-garg-LearningSafeControl,
  title = {Learning Safe Control for Multi-Robot Systems: {{Methods}}, Verification, and Open Challenges},
  shorttitle = {Learning Safe Control for Multi-Robot Systems},
  author = {Garg, Kunal and Zhang, Songyuan and So, Oswin and Dawson, Charles and Fan, Chuchu},
  year = {2024},
  journal = {Annual Reviews in Control},
  shortjournal = {Annual Reviews in Control},
  volume = {57},
  pages = {100948},
  issn = {1367-5788},
  delete_delete_delete_doi = {10.1016/j.arcontrol.2024.100948},
  url = {https://www.sciencedirect.com/science/article/pii/S1367578824000178},
  urlyear = {2024},
  abstract = {In this survey, we review the recent advances in control design methods for robotic multi-agent systems (MAS), focusing on learning-based methods with safety considerations. We start by reviewing various notions of safety and liveness properties, and modeling frameworks used for problem formulation of MAS. Then we provide a comprehensive review of learning-based methods for safe control design for multi-robot systems. We start with various shielding-based methods, such as safety certificates, predictive filters, and reachability tools. Then, we review the current state of control barrier certificate learning in both a centralized and distributed manner, followed by a comprehensive review of multi-agent reinforcement learning with a particular focus on safety. Next, we discuss the state-of-the-art verification tools for the correctness of learning-based methods. Based on the capabilities and the limitations of the state-of-the-art methods in learning and verification for MAS, we identify various broad themes for open challenges: how to design methods that can achieve good performance along with safety guarantees; how to decompose single-agent-based centralized methods for MAS; how to account for communication-related practical issues; and how to assess transfer of theoretical guarantees to practice.},
  keywords = {Certificate-based multi-agent control,Safe multi-agent reinforcement learning,Verification for multi-agent systems},
  file = {/home/mtoussai/Zotero/storage/J746HGIG/Garg et al. - 2024 - Learning safe control for multi-robot systems Met.pdf}
}

@online{2024-huang-CollisionAvoidanceNavigation,
  title = {Collision {{Avoidance}} and {{Navigation}} for a {{Quadrotor Swarm Using End-to-end Deep Reinforcement Learning}}},
  author = {Huang, Zhehui and Yang, Zhaojing and Krupani, Rahul and Şenbaşlar, Baskın and Batra, Sumeet and Sukhatme, Gaurav S.},
  year = {2024},
  eprint = {2309.13285},
  eprinttype = {arXiv},
  eprintclass = {cs},
  delete_delete_delete_doi = {10.48550/arXiv.2309.13285},
  url = {http://arxiv.org/abs/2309.13285},
  urlyear = {2024},
  abstract = {End-to-end deep reinforcement learning (DRL) for quadrotor control promises many benefits -- easy deployment, task generalization and real-time execution capability. Prior end-to-end DRL-based methods have showcased the ability to deploy learned controllers onto single quadrotors or quadrotor teams maneuvering in simple, obstacle-free environments. However, the addition of obstacles increases the number of possible interactions exponentially, thereby increasing the difficulty of training RL policies. In this work, we propose an end-to-end DRL approach to control quadrotor swarms in environments with obstacles. We provide our agents a curriculum and a replay buffer of the clipped collision episodes to improve performance in obstacle-rich environments. We implement an attention mechanism to attend to the neighbor robots and obstacle interactions - the first successful demonstration of this mechanism on policies for swarm behavior deployed on severely compute-constrained hardware. Our work is the first work that demonstrates the possibility of learning neighbor-avoiding and obstacle-avoiding control policies trained with end-to-end DRL that transfers zero-shot to real quadrotors. Our approach scales to 32 robots with 80\% obstacle density in simulation and 8 robots with 20\% obstacle density in physical deployment. Video demonstrations are available on the project website at: https://sites.google.com/view/obst-avoid-swarm-rl.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems,Computer Science - Robotics},
  file = {/home/mtoussai/Zotero/storage/XDSZUNSX/Huang et al. - 2024 - Collision Avoidance and Navigation for a Quadrotor.pdf;/home/mtoussai/Zotero/storage/YNMX92TA/2309.html}
}
