\input{../latex/shared}

\renewcommand{\course}{Machine Learning}
\renewcommand{\exnum}{7}

\exercises
\excludecomment{solution}
\exercisestitle

(DS BSc students can skip the exercise 3.)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exsection{Kernels and Features (4 Points)}

Reconsider the equations for Kernel Ridge regression given on slide
05:3, and -- if features are given -- the definition of the kernel
function and $\k(x)$ in terms of the features as given on slide 05:2.

a) Prove that Kernel Ridge regression with the linear kernel function
$k(x,x') = 1 + x^\T x'$ is equivalent to Ridge regression with linear
features $\phi(x) = \mat{c}{1 \\ x}$. (1 P)

b) In Kernel Ridge regression, the optimal function is of the form
$f(x) = \k(x)^\T \a$ and therefore linear in $\k(x)$. In plain ridge
regression, the optimal function is of the form $f(x) = \phi(x)^\T \b$
and linear in $\phi(x)$. Prove that choosing $k(x,x') = (1+ x^\T x')^2$
implies that $f(x) = \k(x)^\T \a$ is a second order polynomial over
$x$.  (2 P)

c) Equally, note that choosing the squared exponential kernel $k(x,x')
= \exp(-\gamma \|x-x'\|^2)$ implies that the optimal $f(x)$ is linear
in radial basis function (RBF) features. Does this necessarily impliy
that Kernel Ridge regression with squared exponential kernel, and
plain Ridge regression with RBF features are exactly equivalent?
(Equivalent means, have the same optimal function.) Distinguish the
cases $\l=0$ (no regularization) and $\l>0$.  (1 P)

(Voluntary: Practically test yourself on the regression problem from Exercise 2, whether Kernel Ridge Regression and RBF features are exactly equivalent.)

%% In exercise 2 we implemented Ridge regression. Modify the code to
%% implement Kernel ridge regression. Try to program it in a way that you
%% only need to change one line to have a different kernel.  Note that this
%% computes optimal ``parameters'' $\a= (K + \l I)^\1 y$ such that
%% $f(x) = \k(x)^\T \a$.

%% a) Using a linear kernel, does this reproduce the
%% linear regression we looked at in exercise 2? Test this on the
%% data. If not, how can you make it equivalent?

%% %b) the general polynomial kernel $k(x,x') = (x^\T x' + a)^d$, $a\ge0$, $d\in \NNN$. Relation to the polynomial features?

%% b) Is using the squared exponential kernel $k(x,x')
%% = \exp(-\gamma \|x-x'\|^2)$ exactly equivalent to using the radial basis
%% function features we introduced?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exsection{Kernel Construction (4 Points)}

For a non-empty set $X$, a kernel is a symmetric function
$k: X \times X \to \RRR$. Note that the set
$X$ can be arbitrarily structured (real vector space, graphs,
images, strings and so on). A very important class of useful kernels
for machine learning are positive definite kernels. A kernel is
called \emph{positive definite}, if for all arbitrary finite subsets
$\{x_i\}_{i=1}^n \subseteq X$ the corresponding
\emph{kernel matrix} $K$ with elements $K_{ij} = k(x_i,x_j)$ is
positive \emph{semi}-definite,
\begin{align}
\a \in\RRR^n ~ \To ~ \a^\T K\a \ge 0 ~.
\end{align}

Let $k_1,k_2: X\times X \to \RRR$ be two positive definite
kernels. Often, one wants to construct more complicated kernels out of
existing ones. Proof that
\begin{enumerate}
\item $k(x,x') = k_1(x,x') + k_2(x,x')$
\item $k(x,x') = c\cdot k_1(x,x')$ ~ for ~ $c \ge 0$
\item $k(x,x') = k_1(x,x') \cdot k_2(x,x')$
\item $k(x,x') = k_1(f(x),f(x'))$ ~ for ~ $f:X\to X$
\end{enumerate}
are positive definite kernels.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exsection{Kernel logistic regression (2 Points)}

The ``kernel trick'' is generally applicable whenever the ``solution''
(which may be the predictive function $f^\text{ridge}(x)$, or the
discriminative function, or principal components...) can be written in
a form that only uses the kernel function $k(x,x')$, but never features
$\phi(x)$ or parameters $\b$ explicitly.

Derive a kernelization of Logistic Regression. That is,
think about how you could perform the Newton iterations based only on
the kernel function $k(x,x')$.

Tips: Reformulate the Newton iterations
\begin{align}
\b
&\gets \b
 - (\vec X^\T W \vec X + 2 \l I)^\1~
[\vec X^\T (\vec p - \vec y) + 2\l I \b] 
\end{align}
using the two Woodbury identities
\begin{align}
(X^\T W X + A)^\1 X^\T W
&= A^\1 X^\T (X A^\1 X^\T + W^\1)^\1 \\
(X^\T WX+A)^\1
&= A^\1 - A^\1 X^\T (XA^\1X^\T+W^\1)^\1 X A^\1
\end{align}
Note that you'll need to handle the $\vec X^\T (\vec p - \vec y)$ and
$2\l I \b$ differently.

Then think about what is actually been iterated in the kernalized
case: surely we cannot iteratively update the optimal parameters,
because we want to rewrite equations to never touch $\b$ or $\phi(x)$
explicitly.

\exerfoot
